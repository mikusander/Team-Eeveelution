{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc2c558d",
   "metadata": {},
   "source": [
    "# PokÃ©mon Battles â€” Ensemble XGBoost + LightGBM (Voting)\n",
    "Notebook che implementa ensemble tra XGBoost e LightGBM con:\n",
    "- **STESSE FEATURE** del notebook LightGBM ottimizzato\n",
    "- **STESSI IPERPARAMETRI** dal notebook LightGBM\n",
    "- 10-fold CV per entrambi i modelli\n",
    "- Voting Classifier (hard voting + soft voting)\n",
    "- Weighted voting con ottimizzazione pesi\n",
    "- Submission finale"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a6375e",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "297dafd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caricamento dati...\n",
      "Train records: 10000, Test records: 5000\n",
      "Train records: 10000, Test records: 5000\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# --- Percorsi ---\n",
    "train_file_path = 'train.jsonl'\n",
    "test_file_path = 'test.jsonl'\n",
    "\n",
    "def load_jsonl(path):\n",
    "    data = []\n",
    "    with open(path, 'r') as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line))\n",
    "    return data\n",
    "\n",
    "print('Caricamento dati...')\n",
    "train_raw = load_jsonl(train_file_path)\n",
    "test_raw = load_jsonl(test_file_path)\n",
    "print(f'Train records: {len(train_raw)}, Test records: {len(test_raw)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651a397d",
   "metadata": {},
   "source": [
    "## Feature Engineering Functions\n",
    "NOTA: Copia TUTTE le funzioni dal notebook LightGBM per garantire identiche feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7102841",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Feature engineering functions loaded\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from collections import Counter\n",
    "\n",
    "# === TYPE CHART (Gen 1) ===\n",
    "TYPE_CHART = {\n",
    "    'normal': {'rock': 0.5, 'ghost': 0},\n",
    "    'fire': {'fire': 0.5, 'water': 0.5, 'grass': 2, 'ice': 2, 'bug': 2, 'rock': 0.5, 'dragon': 0.5},\n",
    "    'water': {'fire': 2, 'water': 0.5, 'grass': 0.5, 'ground': 2, 'rock': 2, 'dragon': 0.5},\n",
    "    'grass': {'fire': 0.5, 'water': 2, 'grass': 0.5, 'poison': 0.5, 'ground': 2, 'flying': 0.5, 'bug': 0.5, 'rock': 2, 'dragon': 0.5},\n",
    "    'electric': {'water': 2, 'grass': 0.5, 'electric': 0.5, 'ground': 0, 'flying': 2, 'dragon': 0.5},\n",
    "    'ice': {'fire': 0.5, 'water': 0.5, 'grass': 2, 'ground': 2, 'flying': 2, 'dragon': 2},\n",
    "    'fighting': {'normal': 2, 'ice': 2, 'poison': 0.5, 'flying': 0.5, 'psychic': 0.5, 'bug': 0.5, 'rock': 2, 'ghost': 0},\n",
    "    'poison': {'grass': 2, 'poison': 0.5, 'ground': 0.5, 'bug': 2, 'rock': 0.5, 'ghost': 0.5},\n",
    "    'ground': {'fire': 2, 'grass': 0.5, 'electric': 2, 'poison': 2, 'flying': 0, 'bug': 0.5, 'rock': 2},\n",
    "    'flying': {'grass': 2, 'electric': 0.5, 'fighting': 2, 'bug': 2, 'rock': 0.5},\n",
    "    'psychic': {'fighting': 2, 'poison': 2, 'psychic': 0.5, 'ghost': 0},\n",
    "    'bug': {'fire': 0.5, 'grass': 2, 'fighting': 0.5, 'poison': 2, 'flying': 0.5, 'psychic': 2, 'ghost': 0.5},\n",
    "    'rock': {'fire': 2, 'ice': 2, 'fighting': 0.5, 'ground': 0.5, 'flying': 2, 'bug': 2},\n",
    "    'ghost': {'normal': 0, 'psychic': 0, 'ghost': 2},\n",
    "    'dragon': {'dragon': 2}\n",
    "}\n",
    "\n",
    "def get_effectiveness(attack_type: str, defense_types: list) -> float:\n",
    "    if not attack_type or not defense_types:\n",
    "        return 1.0\n",
    "    eff = 1.0\n",
    "    for d in defense_types:\n",
    "        eff *= TYPE_CHART.get(attack_type, {}).get(d, 1.0)\n",
    "    return eff\n",
    "\n",
    "def calculate_type_advantage(team1: list, team2_lead: dict) -> dict:\n",
    "    out = {'p1_vs_lead_avg_effectiveness': 0.0, 'p1_vs_lead_max_effectiveness': 0.0, 'p1_super_effective_options': 0}\n",
    "    if not team1 or not team2_lead:\n",
    "        return out\n",
    "    lead_types = [t.lower() for t in team2_lead.get('types', [])]\n",
    "    if not lead_types:\n",
    "        return out\n",
    "    effs = []\n",
    "    for p in team1:\n",
    "        p_types = [t.lower() for t in p.get('types', [])]\n",
    "        max_eff = 0.0\n",
    "        for pt in p_types:\n",
    "            max_eff = max(max_eff, get_effectiveness(pt, lead_types))\n",
    "        effs.append(max_eff)\n",
    "    if not effs:\n",
    "        return out\n",
    "    out['p1_vs_lead_avg_effectiveness'] = float(np.mean(effs))\n",
    "    out['p1_vs_lead_max_effectiveness'] = float(np.max(effs))\n",
    "    out['p1_super_effective_options'] = int(sum(1 for e in effs if e >= 2))\n",
    "    return out\n",
    "\n",
    "def _entropy(counter: Counter) -> float:\n",
    "    total = sum(counter.values())\n",
    "    if total == 0:\n",
    "        return 0.0\n",
    "    ent = 0.0\n",
    "    for v in counter.values():\n",
    "        p = v / total\n",
    "        if p > 0:\n",
    "            ent -= p * math.log(p, 2)\n",
    "    return ent\n",
    "\n",
    "def team_aggregate_features(team: list, prefix: str = 'p1_') -> dict:\n",
    "    stats = ['base_hp','base_atk','base_def','base_spa','base_spd','base_spe']\n",
    "    out = {}\n",
    "    vals = {s: [] for s in stats}\n",
    "    levels = []\n",
    "    types_counter = Counter()\n",
    "    names = []\n",
    "    for p in team:\n",
    "        names.append(p.get('name',''))\n",
    "        for s in stats:\n",
    "            vals[s].append(p.get(s, 0))\n",
    "        levels.append(p.get('level', 0))\n",
    "        for t in p.get('types', []):\n",
    "            types_counter[t.lower()] += 1\n",
    "    for s in stats:\n",
    "        arr = np.array(vals[s], dtype=float)\n",
    "        out[f'{prefix}{s}_sum'] = float(arr.sum())\n",
    "        out[f'{prefix}{s}_mean'] = float(arr.mean())\n",
    "        out[f'{prefix}{s}_max'] = float(arr.max())\n",
    "        out[f'{prefix}{s}_min'] = float(arr.min())\n",
    "        out[f'{prefix}{s}_std'] = float(arr.std())\n",
    "    level_arr = np.array(levels, dtype=float)\n",
    "    out[f'{prefix}level_mean'] = float(level_arr.mean()) if level_arr.size else 0.0\n",
    "    out[f'{prefix}level_sum'] = float(level_arr.sum()) if level_arr.size else 0.0\n",
    "    out[f'{prefix}n_unique_types'] = int(len(types_counter))\n",
    "    common_types = ['normal','fire','water','electric','grass','psychic','ice','dragon','rock','ground','flying']\n",
    "    for t in common_types:\n",
    "        out[f'{prefix}type_{t}_count'] = int(types_counter.get(t, 0))\n",
    "    out[f'{prefix}lead_name'] = names[0] if names else ''\n",
    "    out[f'{prefix}n_unique_names'] = int(len(set(names)))\n",
    "    out[f'{prefix}type_entropy'] = float(_entropy(types_counter))\n",
    "    spe_arr = np.array(vals['base_spe'], dtype=float)\n",
    "    out[f'{prefix}spe_p25'] = float(np.percentile(spe_arr, 25)) if spe_arr.size else 0.0\n",
    "    out[f'{prefix}spe_p50'] = float(np.percentile(spe_arr, 50)) if spe_arr.size else 0.0\n",
    "    out[f'{prefix}spe_p75'] = float(np.percentile(spe_arr, 75)) if spe_arr.size else 0.0\n",
    "    return out\n",
    "\n",
    "def lead_vs_lead_features(p1_lead: dict, p2_lead: dict) -> dict:\n",
    "    out = {}\n",
    "    stats = ['base_hp','base_atk','base_def','base_spa','base_spd','base_spe']\n",
    "    for s in stats:\n",
    "        out[f'lead_diff_{s}'] = float(p1_lead.get(s,0) - p2_lead.get(s,0))\n",
    "    out['lead_speed_advantage'] = float(p1_lead.get('base_spe',0) - p2_lead.get('base_spe',0))\n",
    "    p1_types = [t.lower() for t in p1_lead.get('types', [])]\n",
    "    p2_types = [t.lower() for t in p2_lead.get('types', [])]\n",
    "    max_eff = 0.0\n",
    "    for pt in p1_types:\n",
    "        max_eff = max(max_eff, get_effectiveness(pt, p2_types))\n",
    "    out['lead_p1_vs_p2_effectiveness'] = float(max_eff)\n",
    "    return out\n",
    "\n",
    "def lead_aggregate_features(pokemon: dict, prefix: str = 'p2_lead_') -> dict:\n",
    "    out = {}\n",
    "    stats = ['base_hp','base_atk','base_def','base_spa','base_spd','base_spe']\n",
    "    for s in stats:\n",
    "        out[f'{prefix}{s}'] = float(pokemon.get(s,0))\n",
    "    out[f'{prefix}level'] = int(pokemon.get('level',0))\n",
    "    types = [x.lower() for x in pokemon.get('types', [])]\n",
    "    common_types = ['normal','fire','water','electric','grass','psychic','ice','dragon','rock','ground','flying']\n",
    "    for t in common_types:\n",
    "        out[f'{prefix}type_{t}'] = int(t in types)\n",
    "    out[f'{prefix}name'] = pokemon.get('name','')\n",
    "    out[f'{prefix}n_unique_types'] = int(len(set(types)))\n",
    "    return out\n",
    "\n",
    "def quick_boost_features_v2(record: dict) -> dict:\n",
    "    \"\"\"Feature ad alto impatto senza bisogno team completo P2\"\"\"\n",
    "    out = {}\n",
    "    p1_team = record.get('p1_team_details', [])\n",
    "    p2_lead = record.get('p2_lead_details', {})\n",
    "    timeline = record.get('battle_timeline', [])\n",
    "    \n",
    "    if not p1_team:\n",
    "        return out\n",
    "    \n",
    "    # 1. Speed Control\n",
    "    p2_lead_spe = p2_lead.get('base_spe', 0)\n",
    "    faster_count = sum(1 for p in p1_team if p.get('base_spe', 0) > p2_lead_spe)\n",
    "    slower_count = sum(1 for p in p1_team if p.get('base_spe', 0) <= p2_lead_spe)\n",
    "    \n",
    "    out['p1_faster_than_lead_count'] = faster_count\n",
    "    out['p1_slower_than_lead_count'] = slower_count\n",
    "    out['p1_speed_control_ratio'] = faster_count / max(1, len(p1_team))\n",
    "    \n",
    "    # 2. Bulk vs Lead\n",
    "    p1_avg_bulk = np.mean([\n",
    "        p.get('base_hp', 0) * (p.get('base_def', 0) + p.get('base_spd', 0)) \n",
    "        for p in p1_team\n",
    "    ])\n",
    "    p2_lead_bulk = p2_lead.get('base_hp', 1) * (\n",
    "        p2_lead.get('base_def', 1) + p2_lead.get('base_spd', 1)\n",
    "    )\n",
    "    out['p1_avg_bulk_vs_lead'] = p1_avg_bulk / max(p2_lead_bulk, 1)\n",
    "    \n",
    "    # 3. Offensive Power\n",
    "    p1_total_atk = sum(p.get('base_atk', 0) + p.get('base_spa', 0) for p in p1_team)\n",
    "    p2_lead_offense = p2_lead.get('base_atk', 0) + p2_lead.get('base_spa', 0)\n",
    "    out['p1_total_offense'] = p1_total_atk\n",
    "    out['p1_offense_advantage'] = p1_total_atk / max(p2_lead_offense, 1)\n",
    "    \n",
    "    # 4. Type Coverage Score\n",
    "    p2_lead_types = [t.lower() for t in p2_lead.get('types', [])]\n",
    "    if p2_lead_types:\n",
    "        coverage_scores = []\n",
    "        for p in p1_team:\n",
    "            p_types = [t.lower() for t in p.get('types', [])]\n",
    "            max_eff = max([get_effectiveness(pt, p2_lead_types) for pt in p_types] or [1.0])\n",
    "            coverage_scores.append(max_eff)\n",
    "        \n",
    "        out['p1_avg_effectiveness_vs_lead'] = float(np.mean(coverage_scores))\n",
    "        out['p1_max_effectiveness_vs_lead'] = float(np.max(coverage_scores))\n",
    "        out['p1_se_count_vs_lead'] = sum(1 for s in coverage_scores if s >= 2.0)\n",
    "        out['p1_weak_count_vs_lead'] = sum(1 for s in coverage_scores if s <= 0.5)\n",
    "    \n",
    "    # 5. First blood\n",
    "    if timeline:\n",
    "        first_p1_ko = False\n",
    "        first_p2_ko = False\n",
    "        \n",
    "        for turn in timeline[:30]:\n",
    "            p1_state = turn.get('p1_pokemon_state', {})\n",
    "            p2_state = turn.get('p2_pokemon_state', {})\n",
    "            \n",
    "            if not first_p2_ko and p2_state.get('fainted'):\n",
    "                first_p1_ko = True\n",
    "                out['p1_first_blood'] = 1\n",
    "                out['p1_first_blood_turn'] = turn.get('turn', 0)\n",
    "                break\n",
    "            \n",
    "            if not first_p1_ko and p1_state.get('fainted'):\n",
    "                first_p2_ko = True\n",
    "                out['p1_first_blood'] = 0\n",
    "                out['p1_first_blood_turn'] = turn.get('turn', 0)\n",
    "                break\n",
    "        \n",
    "        if not first_p1_ko and not first_p2_ko:\n",
    "            out['p1_first_blood'] = -1\n",
    "            out['p1_first_blood_turn'] = 0\n",
    "    \n",
    "    # 6. Team Level Advantage\n",
    "    p1_avg_level = np.mean([p.get('level', 50) for p in p1_team])\n",
    "    p2_lead_level = p2_lead.get('level', 50)\n",
    "    out['p1_avg_level_advantage'] = p1_avg_level - p2_lead_level\n",
    "    \n",
    "    # 7. Stat Product\n",
    "    p1_stat_products = []\n",
    "    for p in p1_team:\n",
    "        stat_product = (\n",
    "            p.get('base_hp', 1) * \n",
    "            p.get('base_atk', 1) * \n",
    "            p.get('base_def', 1) * \n",
    "            p.get('base_spa', 1) * \n",
    "            p.get('base_spd', 1) * \n",
    "            p.get('base_spe', 1)\n",
    "        )\n",
    "        p1_stat_products.append(stat_product)\n",
    "    \n",
    "    out['p1_avg_stat_product'] = float(np.mean(p1_stat_products))\n",
    "    out['p1_max_stat_product'] = float(np.max(p1_stat_products))\n",
    "    \n",
    "    p2_lead_stat_product = (\n",
    "        p2_lead.get('base_hp', 1) * \n",
    "        p2_lead.get('base_atk', 1) * \n",
    "        p2_lead.get('base_def', 1) * \n",
    "        p2_lead.get('base_spa', 1) * \n",
    "        p2_lead.get('base_spd', 1) * \n",
    "        p2_lead.get('base_spe', 1)\n",
    "    )\n",
    "    out['p1_stat_product_advantage'] = out['p1_avg_stat_product'] / max(p2_lead_stat_product, 1)\n",
    "    \n",
    "    return out\n",
    "\n",
    "def summary_from_timeline(timeline: list, p1_team: list) -> dict:\n",
    "    out = {}\n",
    "    if not timeline:\n",
    "        return {'tl_p1_moves':0,'tl_p2_moves':0,'tl_p1_est_damage':0.0,'tl_p2_est_damage':0.0,'damage_diff':0.0}\n",
    "    \n",
    "    p1_moves = p2_moves = 0\n",
    "    p1_damage = p2_damage = 0.0\n",
    "    p1_last_active = p2_last_active = ''\n",
    "    p1_last_hp = p2_last_hp = np.nan\n",
    "    p1_fainted = p2_fainted = 0\n",
    "    p1_fainted_names = set()\n",
    "    p2_fainted_names = set()\n",
    "    last_p1_hp = {}\n",
    "    last_p2_hp = {}\n",
    "    \n",
    "    for turn in timeline[:30]:\n",
    "        p1_state = turn.get('p1_pokemon_state',{}) or {}\n",
    "        p2_state = turn.get('p2_pokemon_state',{}) or {}\n",
    "        \n",
    "        if p1_state.get('name'):\n",
    "            p1_last_active = p1_state.get('name')\n",
    "        if p2_state.get('name'):\n",
    "            p2_last_active = p2_state.get('name')\n",
    "        \n",
    "        if p1_state.get('fainted') and p1_state.get('name') not in p1_fainted_names:\n",
    "            p1_fainted += 1\n",
    "            p1_fainted_names.add(p1_state.get('name'))\n",
    "        if p2_state.get('fainted') and p2_state.get('name') not in p2_fainted_names:\n",
    "            p2_fainted += 1\n",
    "            p2_fainted_names.add(p2_state.get('name'))\n",
    "        \n",
    "        p2_name, p2_hp = p2_state.get('name'), p2_state.get('hp_pct')\n",
    "        if p2_name and p2_hp is not None:\n",
    "            prev_hp = last_p2_hp.get(p2_name)\n",
    "            if prev_hp is not None:\n",
    "                delta = max(0.0, prev_hp - p2_hp)\n",
    "                p1_damage += delta\n",
    "            last_p2_hp[p2_name] = p2_hp\n",
    "        \n",
    "        p1_name, p1_hp = p1_state.get('name'), p1_state.get('hp_pct')\n",
    "        if p1_name and p1_hp is not None:\n",
    "            prev_hp = last_p1_hp.get(p1_name)\n",
    "            if prev_hp is not None:\n",
    "                delta = max(0.0, prev_hp - p1_hp)\n",
    "                p2_damage += delta\n",
    "            last_p1_hp[p1_name] = p1_hp\n",
    "        \n",
    "        if turn.get('p1_move_details'):\n",
    "            p1_moves += 1\n",
    "        if turn.get('p2_move_details'):\n",
    "            p2_moves += 1\n",
    "        \n",
    "        p1_last_hp = p1_state.get('hp_pct', np.nan)\n",
    "        p2_last_hp = p2_state.get('hp_pct', np.nan)\n",
    "    \n",
    "    out['tl_p1_moves'] = int(p1_moves)\n",
    "    out['tl_p2_moves'] = int(p2_moves)\n",
    "    out['tl_p1_est_damage'] = float(p1_damage)\n",
    "    out['tl_p2_est_damage'] = float(p2_damage)\n",
    "    out['tl_p1_fainted'] = int(p1_fainted)\n",
    "    out['tl_p2_fainted'] = int(p2_fainted)\n",
    "    out['damage_diff'] = float(p1_damage - p2_damage)\n",
    "    out['fainted_diff'] = int(p1_fainted - p2_fainted)\n",
    "    out['tl_p1_last_hp'] = float(p1_last_hp) if not np.isnan(p1_last_hp) else 0.0\n",
    "    out['tl_p2_last_hp'] = float(p2_last_hp) if not np.isnan(p2_last_hp) else 0.0\n",
    "    out['tl_p1_last_active'] = p1_last_active\n",
    "    out['tl_p2_last_active'] = p2_last_active\n",
    "    \n",
    "    return out\n",
    "\n",
    "def ability_features(team: list, prefix: str) -> dict:\n",
    "    immunity_abilities = {'levitate':0,'volt_absorb':0,'water_absorb':0,'flash_fire':0}\n",
    "    stat_drop_abilities = {'intimidate':0}\n",
    "    weather_abilities = {'drought':0,'drizzle':0,'sand_stream':0}\n",
    "    out = {}\n",
    "    for pokemon in team:\n",
    "        ability = (pokemon.get('ability','') or '').lower().replace(' ','_')\n",
    "        if ability in immunity_abilities:\n",
    "            immunity_abilities[ability] += 1\n",
    "        if ability in stat_drop_abilities:\n",
    "            stat_drop_abilities[ability] += 1\n",
    "        if ability in weather_abilities:\n",
    "            weather_abilities[ability] += 1\n",
    "    for ability,count in immunity_abilities.items():\n",
    "        out[f'{prefix}ability_{ability}_count'] = int(count)\n",
    "    for ability,count in stat_drop_abilities.items():\n",
    "        out[f'{prefix}ability_{ability}_count'] = int(count)\n",
    "    for ability,count in weather_abilities.items():\n",
    "        out[f'{prefix}ability_{ability}_count'] = int(count)\n",
    "    out[f'{prefix}total_immunity_abilities'] = int(sum(immunity_abilities.values()))\n",
    "    out[f'{prefix}total_stat_drop_abilities'] = int(sum(stat_drop_abilities.values()))\n",
    "    return out\n",
    "\n",
    "def prepare_record_features(record: dict, max_turns: int = 30) -> dict:\n",
    "    out = {}\n",
    "    out['battle_id'] = record.get('battle_id')\n",
    "    if 'player_won' in record:\n",
    "        out['player_won'] = int(bool(record.get('player_won')))\n",
    "    \n",
    "    p1_team = record.get('p1_team_details', [])\n",
    "    out.update(team_aggregate_features(p1_team, prefix='p1_'))\n",
    "    \n",
    "    p2_lead = record.get('p2_lead_details', {})\n",
    "    out.update(lead_aggregate_features(p2_lead, prefix='p2_lead_'))\n",
    "    out.update(ability_features(p1_team, prefix='p1_'))\n",
    "    \n",
    "    p1_lead = p1_team[0] if p1_team else {}\n",
    "    out.update(lead_vs_lead_features(p1_lead, p2_lead))\n",
    "    out.update(ability_features([p2_lead], prefix='p2_lead_'))\n",
    "    \n",
    "    tl = record.get('battle_timeline', [])\n",
    "    out.update(summary_from_timeline(tl[:max_turns], p1_team))\n",
    "    out.update(quick_boost_features_v2(record))\n",
    "    out.update(calculate_type_advantage(p1_team, p2_lead))\n",
    "    \n",
    "    # Derived features\n",
    "    out['team_hp_sum_minus_p2lead_hp'] = out.get('p1_base_hp_sum', 0) - out.get('p2_lead_base_hp', 0)\n",
    "    out['speed_advantage'] = out.get('p1_base_spe_sum', 0) - out.get('p2_lead_base_spe', 0)\n",
    "    \n",
    "    p2_team = record.get('p2_team_details', [])\n",
    "    if p2_team:\n",
    "        out.update(team_aggregate_features(p2_team, prefix='p2_'))\n",
    "        out['team_hp_sum_diff'] = out.get('p1_base_hp_sum',0) - out.get('p2_base_hp_sum',0)\n",
    "        out['team_spe_mean_diff'] = out.get('p1_base_spe_mean',0) - out.get('p2_base_spe_mean',0)\n",
    "    \n",
    "    return out\n",
    "\n",
    "def create_features_from_raw(data: list) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    for b in tqdm(data, desc='Feature Engineering'):\n",
    "        try:\n",
    "            feat = prepare_record_features(b, max_turns=30)\n",
    "            if 'battle_id' not in feat:\n",
    "                feat['battle_id'] = b.get('battle_id')\n",
    "            rows.append(feat)\n",
    "        except Exception as e:\n",
    "            rows.append({'battle_id': b.get('battle_id'), 'error': 1})\n",
    "    df = pd.DataFrame(rows)\n",
    "    if 'player_won' in df.columns:\n",
    "        df['player_won'] = df['player_won'].astype(int)\n",
    "    return df.fillna(0)\n",
    "\n",
    "print(\"âœ… Feature engineering functions loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84498f6b",
   "metadata": {},
   "source": [
    "## Create Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc96c587",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature Engineering: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10000/10000 [00:06<00:00, 1467.20it/s]\n",
      "\n",
      "Feature Engineering: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [00:03<00:00, 1584.34it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape train/test: (10000, 133) (5000, 132)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>battle_id</th>\n",
       "      <th>player_won</th>\n",
       "      <th>p1_base_hp_sum</th>\n",
       "      <th>p1_base_hp_mean</th>\n",
       "      <th>p1_base_hp_max</th>\n",
       "      <th>p1_base_hp_min</th>\n",
       "      <th>p1_base_hp_std</th>\n",
       "      <th>p1_base_atk_sum</th>\n",
       "      <th>p1_base_atk_mean</th>\n",
       "      <th>p1_base_atk_max</th>\n",
       "      <th>...</th>\n",
       "      <th>p1_first_blood_turn</th>\n",
       "      <th>p1_avg_level_advantage</th>\n",
       "      <th>p1_avg_stat_product</th>\n",
       "      <th>p1_max_stat_product</th>\n",
       "      <th>p1_stat_product_advantage</th>\n",
       "      <th>p1_vs_lead_avg_effectiveness</th>\n",
       "      <th>p1_vs_lead_max_effectiveness</th>\n",
       "      <th>p1_super_effective_options</th>\n",
       "      <th>team_hp_sum_minus_p2lead_hp</th>\n",
       "      <th>speed_advantage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>695.0</td>\n",
       "      <td>115.833333</td>\n",
       "      <td>250.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>69.367179</td>\n",
       "      <td>435.0</td>\n",
       "      <td>72.500000</td>\n",
       "      <td>110.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.170415e+11</td>\n",
       "      <td>6.592480e+11</td>\n",
       "      <td>0.720754</td>\n",
       "      <td>1.083333</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>635.0</td>\n",
       "      <td>365.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>740.0</td>\n",
       "      <td>123.333333</td>\n",
       "      <td>250.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>64.204534</td>\n",
       "      <td>435.0</td>\n",
       "      <td>72.500000</td>\n",
       "      <td>110.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.399565e+11</td>\n",
       "      <td>6.592480e+11</td>\n",
       "      <td>0.886622</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>685.0</td>\n",
       "      <td>250.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>745.0</td>\n",
       "      <td>124.166667</td>\n",
       "      <td>250.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>64.382753</td>\n",
       "      <td>505.0</td>\n",
       "      <td>84.166667</td>\n",
       "      <td>130.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.932361e+11</td>\n",
       "      <td>6.592480e+11</td>\n",
       "      <td>85.111624</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>495.0</td>\n",
       "      <td>345.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>730.0</td>\n",
       "      <td>121.666667</td>\n",
       "      <td>250.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>65.362239</td>\n",
       "      <td>465.0</td>\n",
       "      <td>77.500000</td>\n",
       "      <td>110.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.504200e+11</td>\n",
       "      <td>1.075781e+12</td>\n",
       "      <td>1.172854</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>655.0</td>\n",
       "      <td>345.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>685.0</td>\n",
       "      <td>114.166667</td>\n",
       "      <td>250.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>70.794107</td>\n",
       "      <td>455.0</td>\n",
       "      <td>75.833333</td>\n",
       "      <td>110.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.157984e+11</td>\n",
       "      <td>6.592480e+11</td>\n",
       "      <td>0.717928</td>\n",
       "      <td>1.083333</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>625.0</td>\n",
       "      <td>320.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 133 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   battle_id  player_won  p1_base_hp_sum  p1_base_hp_mean  p1_base_hp_max  \\\n",
       "0          0           1           695.0       115.833333           250.0   \n",
       "1          1           1           740.0       123.333333           250.0   \n",
       "2          2           1           745.0       124.166667           250.0   \n",
       "3          3           1           730.0       121.666667           250.0   \n",
       "4          4           1           685.0       114.166667           250.0   \n",
       "\n",
       "   p1_base_hp_min  p1_base_hp_std  p1_base_atk_sum  p1_base_atk_mean  \\\n",
       "0            55.0       69.367179            435.0         72.500000   \n",
       "1            65.0       64.204534            435.0         72.500000   \n",
       "2            60.0       64.382753            505.0         84.166667   \n",
       "3            60.0       65.362239            465.0         77.500000   \n",
       "4            50.0       70.794107            455.0         75.833333   \n",
       "\n",
       "   p1_base_atk_max  ...  p1_first_blood_turn  p1_avg_level_advantage  \\\n",
       "0            110.0  ...                    0                     0.0   \n",
       "1            110.0  ...                    0                     0.0   \n",
       "2            130.0  ...                    0                     0.0   \n",
       "3            110.0  ...                    0                     0.0   \n",
       "4            110.0  ...                    0                     0.0   \n",
       "\n",
       "   p1_avg_stat_product  p1_max_stat_product  p1_stat_product_advantage  \\\n",
       "0         3.170415e+11         6.592480e+11                   0.720754   \n",
       "1         2.399565e+11         6.592480e+11                   0.886622   \n",
       "2         2.932361e+11         6.592480e+11                  85.111624   \n",
       "3         4.504200e+11         1.075781e+12                   1.172854   \n",
       "4         3.157984e+11         6.592480e+11                   0.717928   \n",
       "\n",
       "   p1_vs_lead_avg_effectiveness  p1_vs_lead_max_effectiveness  \\\n",
       "0                      1.083333                           2.0   \n",
       "1                      1.000000                           1.0   \n",
       "2                      1.000000                           1.0   \n",
       "3                      1.000000                           1.0   \n",
       "4                      1.083333                           2.0   \n",
       "\n",
       "   p1_super_effective_options  team_hp_sum_minus_p2lead_hp  speed_advantage  \n",
       "0                           1                        635.0            365.0  \n",
       "1                           0                        685.0            250.0  \n",
       "2                           0                        495.0            345.0  \n",
       "3                           0                        655.0            345.0  \n",
       "4                           1                        625.0            320.0  \n",
       "\n",
       "[5 rows x 133 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_df = create_features_from_raw(train_raw)\n",
    "test_df = create_features_from_raw(test_raw)\n",
    "print('Feature shape train/test:', train_df.shape, test_df.shape)\n",
    "display(train_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd79a6c",
   "metadata": {},
   "source": [
    "## Preprocessing - STESSE FEATURE del notebook LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def4f4d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num FEATURES numeriche: 127\n",
      "Preprocessing completato.\n",
      "Dataset completo size: 10000\n",
      "Features: 127\n"
     ]
    }
   ],
   "source": [
    "# âœ… USA LE STESSE FEATURE del notebook LightGBM ottimizzato\n",
    "# Copia esatta dal notebook pokemon_lightgbm_cv.ipynb\n",
    "\n",
    "exclude_cols = ['battle_id', 'player_won']\n",
    "string_cols = train_df.select_dtypes(include=['object']).columns.tolist()\n",
    "exclude_cols.extend(string_cols)\n",
    "\n",
    "ALL_NUMERIC_FEATURES = [c for c in train_df.columns if c not in exclude_cols]\n",
    "\n",
    "# âš ï¸ IMPORTANTE: Usa FEATURES_FINAL se hai fatto feature selection nel notebook LightGBM\n",
    "# Altrimenti usa ALL_NUMERIC_FEATURES\n",
    "# Per ora uso tutte le feature (come nel notebook LightGBM originale)\n",
    "FEATURES = ALL_NUMERIC_FEATURES\n",
    "\n",
    "print(f'âœ… Usando {len(FEATURES)} feature (identiche al notebook LightGBM)')\n",
    "\n",
    "# Preprocessing identico\n",
    "num_df = train_df[FEATURES].astype(float).replace([np.inf, -np.inf], np.nan)\n",
    "medians = num_df.median()\n",
    "train_imputed = num_df.fillna(medians)\n",
    "train_preproc_df = train_imputed.copy()\n",
    "\n",
    "y = train_df['player_won'].astype(int).values\n",
    "X = train_preproc_df.values\n",
    "\n",
    "print('Preprocessing completato.')\n",
    "print('Dataset size:', X.shape)\n",
    "\n",
    "# Allinea test\n",
    "test_aligned = test_df.reindex(columns=FEATURES, fill_value=np.nan).astype(float).replace([np.inf, -np.inf], np.nan)\n",
    "test_imputed = test_aligned.fillna(medians)\n",
    "test_preproc_df = pd.DataFrame(test_imputed.values, columns=FEATURES, index=test_df.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f399bd",
   "metadata": {},
   "source": [
    "## Model Hyperparameters - IDENTICI al notebook LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604cf948",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… IPERPARAMETRI IDENTICI dal notebook pokemon_lightgbm_cv.ipynb\n",
    "# Questi sono i best params trovati con Optuna nel notebook LightGBM\n",
    "\n",
    "lgb_params = {\n",
    "    'objective': 'binary',\n",
    "    'metric': 'binary_logloss',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'verbosity': -1,\n",
    "    'seed': 42,\n",
    "    'learning_rate': 0.048286,\n",
    "    'n_estimators': 600,\n",
    "    'num_leaves': 74,\n",
    "    'max_depth': 7,\n",
    "    'min_child_samples': 95,\n",
    "    'min_child_weight': 0.046044,\n",
    "    'reg_alpha': 0.001554,\n",
    "    'reg_lambda': 0.000040,\n",
    "    'subsample': 0.578947,\n",
    "    'colsample_bytree': 0.575072,\n",
    "    'subsample_freq': 2,\n",
    "    'min_split_gain': 0.741598,\n",
    "    'max_bin': 320\n",
    "}\n",
    "\n",
    "# âœ… XGBoost params - equivalenti configurati per matching\n",
    "# Mappo i parametri LightGBM a XGBoost per coerenza\n",
    "xgb_params = {\n",
    "    'objective': 'binary:logistic',\n",
    "    'eval_metric': 'logloss',\n",
    "    'seed': 42,\n",
    "    'learning_rate': 0.048286,  # STESSO di LightGBM\n",
    "    'n_estimators': 600,         # STESSO di LightGBM\n",
    "    'max_depth': 7,              # STESSO di LightGBM\n",
    "    'min_child_weight': 0.046,   # Equivalente a min_child_weight LightGBM\n",
    "    'gamma': 0.741,              # Equivalente a min_split_gain\n",
    "    'subsample': 0.579,          # STESSO di LightGBM\n",
    "    'colsample_bytree': 0.575,   # STESSO di LightGBM\n",
    "    'reg_alpha': 0.001554,       # STESSO di LightGBM\n",
    "    'reg_lambda': 0.000040,      # STESSO di LightGBM\n",
    "    'tree_method': 'hist',\n",
    "    'max_bin': 320,              # STESSO di LightGBM\n",
    "    'random_state': 42\n",
    "}\n",
    "\n",
    "print(\"âœ… Hyperparameters configurati (IDENTICI al notebook LightGBM)\")\n",
    "print(f\"\\nLightGBM params: {lgb_params}\")\n",
    "print(f\"\\nXGBoost params: {xgb_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9257fb14",
   "metadata": {},
   "source": [
    "## 10-Fold CV - LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01a8960b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "10-FOLD CV - LIGHTGBM\n",
      "======================================================================\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'lgb_params' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     11\u001b[39m X_tr, X_val = X[train_idx], X[val_idx]\n\u001b[32m     12\u001b[39m y_tr, y_val = y[train_idx], y[val_idx]\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m clf = lgb.LGBMClassifier(**\u001b[43mlgb_params\u001b[49m)\n\u001b[32m     15\u001b[39m clf.fit(\n\u001b[32m     16\u001b[39m     X_tr, y_tr,\n\u001b[32m     17\u001b[39m     eval_set=[(X_val, y_val)],\n\u001b[32m     18\u001b[39m     callbacks=[lgb.early_stopping(stopping_rounds=\u001b[32m30\u001b[39m, verbose=\u001b[38;5;28;01mFalse\u001b[39;00m)]\n\u001b[32m     19\u001b[39m )\n\u001b[32m     21\u001b[39m y_pred_val = clf.predict(X_val)\n",
      "\u001b[31mNameError\u001b[39m: name 'lgb_params' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"10-FOLD CV - LIGHTGBM\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "lgb_accuracies = []\n",
    "lgb_oof_preds = np.zeros(len(X))  # Out-of-fold predictions\n",
    "\n",
    "for fold_idx, (train_idx, val_idx) in enumerate(skf.split(X, y), 1):\n",
    "    X_tr, X_val = X[train_idx], X[val_idx]\n",
    "    y_tr, y_val = y[train_idx], y[val_idx]\n",
    "    \n",
    "    clf = lgb.LGBMClassifier(**lgb_params)\n",
    "    clf.fit(\n",
    "        X_tr, y_tr,\n",
    "        eval_set=[(X_val, y_val)],\n",
    "        callbacks=[lgb.early_stopping(stopping_rounds=30, verbose=False)]\n",
    "    )\n",
    "    \n",
    "    y_pred_val = clf.predict(X_val)\n",
    "    val_acc = accuracy_score(y_val, y_pred_val)\n",
    "    lgb_accuracies.append(val_acc)\n",
    "    \n",
    "    # Salva OOF predictions per ensemble\n",
    "    lgb_oof_preds[val_idx] = clf.predict_proba(X_val)[:, 1]\n",
    "    \n",
    "    print(f'LightGBM Fold {fold_idx}: val_acc={val_acc*100:.2f}%')\n",
    "\n",
    "lgb_mean_acc = np.mean(lgb_accuracies)\n",
    "print(f'\\nâœ… LightGBM Mean CV Accuracy: {lgb_mean_acc*100:.2f}% Â± {np.std(lgb_accuracies)*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd105b1c",
   "metadata": {},
   "source": [
    "## 10-Fold CV - XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead4e7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"10-FOLD CV - XGBOOST\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "xgb_accuracies = []\n",
    "xgb_oof_preds = np.zeros(len(X))\n",
    "\n",
    "for fold_idx, (train_idx, val_idx) in enumerate(skf.split(X, y), 1):\n",
    "    X_tr, X_val = X[train_idx], X[val_idx]\n",
    "    y_tr, y_val = y[train_idx], y[val_idx]\n",
    "    \n",
    "    clf = xgb.XGBClassifier(**xgb_params)\n",
    "    clf.fit(\n",
    "        X_tr, y_tr,\n",
    "        eval_set=[(X_val, y_val)],\n",
    "        early_stopping_rounds=30,\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    y_pred_val = clf.predict(X_val)\n",
    "    val_acc = accuracy_score(y_val, y_pred_val)\n",
    "    xgb_accuracies.append(val_acc)\n",
    "    \n",
    "    # Salva OOF predictions\n",
    "    xgb_oof_preds[val_idx] = clf.predict_proba(X_val)[:, 1]\n",
    "    \n",
    "    print(f'XGBoost Fold {fold_idx}: val_acc={val_acc*100:.2f}%')\n",
    "\n",
    "xgb_mean_acc = np.mean(xgb_accuracies)\n",
    "print(f'\\nâœ… XGBoost Mean CV Accuracy: {xgb_mean_acc*100:.2f}% Â± {np.std(xgb_accuracies)*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3475f08f",
   "metadata": {},
   "source": [
    "## Ensemble - Hard Voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718e5390",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"ENSEMBLE - HARD VOTING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Hard voting: predizione basata su maggioranza\n",
    "lgb_hard = (lgb_oof_preds >= 0.5).astype(int)\n",
    "xgb_hard = (xgb_oof_preds >= 0.5).astype(int)\n",
    "\n",
    "# Majority vote\n",
    "hard_vote_preds = ((lgb_hard + xgb_hard) >= 1).astype(int)\n",
    "hard_vote_acc = accuracy_score(y, hard_vote_preds)\n",
    "\n",
    "print(f\"Hard Voting Accuracy: {hard_vote_acc*100:.2f}%\")\n",
    "print(f\"LightGBM solo: {lgb_mean_acc*100:.2f}%\")\n",
    "print(f\"XGBoost solo:  {xgb_mean_acc*100:.2f}%\")\n",
    "print(f\"Improvement: {(hard_vote_acc - max(lgb_mean_acc, xgb_mean_acc))*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728c17b5",
   "metadata": {},
   "source": [
    "## Ensemble - Soft Voting (Simple Average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3805c544",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"ENSEMBLE - SOFT VOTING (SIMPLE AVERAGE)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Soft voting: media delle probabilitÃ \n",
    "soft_vote_probs = (lgb_oof_preds + xgb_oof_preds) / 2\n",
    "soft_vote_preds = (soft_vote_probs >= 0.5).astype(int)\n",
    "soft_vote_acc = accuracy_score(y, soft_vote_preds)\n",
    "\n",
    "print(f\"Soft Voting (0.5/0.5) Accuracy: {soft_vote_acc*100:.2f}%\")\n",
    "print(f\"Improvement over best single: {(soft_vote_acc - max(lgb_mean_acc, xgb_mean_acc))*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07074ca6",
   "metadata": {},
   "source": [
    "## Ensemble - Weighted Soft Voting (Optimization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78bdfd1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"ENSEMBLE - WEIGHTED SOFT VOTING (GRID SEARCH)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Trova pesi ottimali con grid search\n",
    "best_weight = 0.5\n",
    "best_weighted_acc = 0.0\n",
    "\n",
    "weights_to_try = np.arange(0.0, 1.01, 0.05)\n",
    "results = []\n",
    "\n",
    "for w_lgb in weights_to_try:\n",
    "    w_xgb = 1.0 - w_lgb\n",
    "    weighted_probs = w_lgb * lgb_oof_preds + w_xgb * xgb_oof_preds\n",
    "    weighted_preds = (weighted_probs >= 0.5).astype(int)\n",
    "    acc = accuracy_score(y, weighted_preds)\n",
    "    results.append({'w_lgb': w_lgb, 'w_xgb': w_xgb, 'accuracy': acc})\n",
    "    \n",
    "    if acc > best_weighted_acc:\n",
    "        best_weighted_acc = acc\n",
    "        best_weight = w_lgb\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(f\"\\nâœ… Best Weights Found:\")\n",
    "print(f\"   LightGBM weight: {best_weight:.2f}\")\n",
    "print(f\"   XGBoost weight:  {1-best_weight:.2f}\")\n",
    "print(f\"   Accuracy: {best_weighted_acc*100:.2f}%\")\n",
    "\n",
    "print(f\"\\nTop 10 weight combinations:\")\n",
    "display(results_df.sort_values('accuracy', ascending=False).head(10))\n",
    "\n",
    "# Salva risultati\n",
    "results_df.to_csv('ensemble_weight_search.csv', index=False)\n",
    "print(f\"\\nâœ… Weight search results saved to 'ensemble_weight_search.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1e06f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot weight vs accuracy\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(results_df['w_lgb'], results_df['accuracy'] * 100, marker='o', linewidth=2)\n",
    "    plt.axvline(best_weight, color='red', linestyle='--', label=f'Best weight={best_weight:.2f}')\n",
    "    plt.axhline(lgb_mean_acc * 100, color='blue', linestyle='--', alpha=0.5, label=f'LGB only={lgb_mean_acc*100:.2f}%')\n",
    "    plt.axhline(xgb_mean_acc * 100, color='green', linestyle='--', alpha=0.5, label=f'XGB only={xgb_mean_acc*100:.2f}%')\n",
    "    plt.xlabel('LightGBM Weight')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.title('Ensemble Accuracy vs Weight (LightGBM)')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('ensemble_weight_curve.png', dpi=150)\n",
    "    print(\"âœ… Plot saved to 'ensemble_weight_curve.png'\")\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ Plot failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf60cc9f",
   "metadata": {},
   "source": [
    "## Final 10-Fold CV - Voting Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb598a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"FINAL 10-FOLD CV - VOTING ENSEMBLE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "ensemble_accuracies = []\n",
    "ensemble_oof_preds = np.zeros(len(X))\n",
    "\n",
    "for fold_idx, (train_idx, val_idx) in enumerate(skf.split(X, y), 1):\n",
    "    X_tr, X_val = X[train_idx], X[val_idx]\n",
    "    y_tr, y_val = y[train_idx], y[val_idx]\n",
    "    \n",
    "    # Train both models\n",
    "    lgb_clf = lgb.LGBMClassifier(**lgb_params)\n",
    "    xgb_clf = xgb.XGBClassifier(**xgb_params)\n",
    "    \n",
    "    lgb_clf.fit(X_tr, y_tr, eval_set=[(X_val, y_val)], \n",
    "                callbacks=[lgb.early_stopping(stopping_rounds=30, verbose=False)])\n",
    "    xgb_clf.fit(X_tr, y_tr, eval_set=[(X_val, y_val)], \n",
    "                early_stopping_rounds=30, verbose=False)\n",
    "    \n",
    "    # Weighted soft voting con best weights\n",
    "    lgb_probs = lgb_clf.predict_proba(X_val)[:, 1]\n",
    "    xgb_probs = xgb_clf.predict_proba(X_val)[:, 1]\n",
    "    \n",
    "    ensemble_probs = best_weight * lgb_probs + (1 - best_weight) * xgb_probs\n",
    "    ensemble_preds = (ensemble_probs >= 0.5).astype(int)\n",
    "    \n",
    "    val_acc = accuracy_score(y_val, ensemble_preds)\n",
    "    ensemble_accuracies.append(val_acc)\n",
    "    ensemble_oof_preds[val_idx] = ensemble_probs\n",
    "    \n",
    "    print(f'Ensemble Fold {fold_idx}: val_acc={val_acc*100:.2f}%')\n",
    "\n",
    "ensemble_mean_acc = np.mean(ensemble_accuracies)\n",
    "print(f'\\n' + '='*70)\n",
    "print(f'FINAL RESULTS (usando STESSI params del notebook LightGBM):')\n",
    "print(f'='*70)\n",
    "print(f'LightGBM:     {lgb_mean_acc*100:.2f}% Â± {np.std(lgb_accuracies)*100:.2f}%')\n",
    "print(f'XGBoost:      {xgb_mean_acc*100:.2f}% Â± {np.std(xgb_accuracies)*100:.2f}%')\n",
    "print(f'Ensemble:     {ensemble_mean_acc*100:.2f}% Â± {np.std(ensemble_accuracies)*100:.2f}%')\n",
    "print(f'\\nImprovement:  +{(ensemble_mean_acc - max(lgb_mean_acc, xgb_mean_acc))*100:.2f}%')\n",
    "print(f'\\nðŸ“Š Config: {len(FEATURES)} features, LGB weight={best_weight:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12265917",
   "metadata": {},
   "source": [
    "## Make Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d579fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"SUBMISSION - ENSEMBLE VOTING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Train final models su tutto il dataset\n",
    "print(\"Training LightGBM on full dataset...\")\n",
    "final_lgb = lgb.LGBMClassifier(**lgb_params)\n",
    "final_lgb.fit(X, y)\n",
    "\n",
    "print(\"Training XGBoost on full dataset...\")\n",
    "final_xgb = xgb.XGBClassifier(**xgb_params)\n",
    "final_xgb.fit(X, y)\n",
    "\n",
    "# Predict su test con weighted voting\n",
    "print(\"Predicting on test set...\")\n",
    "X_test_matrix = test_preproc_df.values\n",
    "\n",
    "lgb_test_probs = final_lgb.predict_proba(X_test_matrix)[:, 1]\n",
    "xgb_test_probs = final_xgb.predict_proba(X_test_matrix)[:, 1]\n",
    "\n",
    "# Ensemble con best weights\n",
    "ensemble_test_probs = best_weight * lgb_test_probs + (1 - best_weight) * xgb_test_probs\n",
    "ensemble_test_preds = (ensemble_test_probs >= 0.5).astype(int)\n",
    "\n",
    "# Crea submission\n",
    "submission_df = pd.DataFrame({\n",
    "    'battle_id': test_df['battle_id'].astype(np.int64),\n",
    "    'player_won': ensemble_test_preds.astype(np.int64)\n",
    "})\n",
    "\n",
    "submission_path = 'submission_ensemble_voting.csv'\n",
    "submission_df.to_csv(submission_path, index=False)\n",
    "\n",
    "print(f\"\\nâœ… Submission saved to '{submission_path}'\")\n",
    "print(f\"Models: LightGBM ({best_weight:.2f}) + XGBoost ({1-best_weight:.2f})\")\n",
    "print(f\"Features: {len(FEATURES)} (identiche al notebook LightGBM)\")\n",
    "print(f\"Expected CV Accuracy: {ensemble_mean_acc*100:.2f}% Â± {np.std(ensemble_accuracies)*100:.2f}%\")\n",
    "print(f\"\\nPrediction distribution:\")\n",
    "print(submission_df['player_won'].value_counts())\n",
    "print(f\"\\nPreview:\")\n",
    "display(submission_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45713b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bonus: Salva anche predizioni individuali per confronto\n",
    "lgb_submission = pd.DataFrame({\n",
    "    'battle_id': test_df['battle_id'].astype(np.int64),\n",
    "    'player_won': final_lgb.predict(X_test_matrix).astype(np.int64)\n",
    "})\n",
    "lgb_submission.to_csv('submission_lightgbm_only.csv', index=False)\n",
    "\n",
    "xgb_submission = pd.DataFrame({\n",
    "    'battle_id': test_df['battle_id'].astype(np.int64),\n",
    "    'player_won': final_xgb.predict(X_test_matrix).astype(np.int64)\n",
    "})\n",
    "xgb_submission.to_csv('submission_xgboost_only.csv', index=False)\n",
    "\n",
    "print(\"âœ… Individual model submissions also saved:\")\n",
    "print(\"   - submission_lightgbm_only.csv\")\n",
    "print(\"   - submission_xgboost_only.csv\")\n",
    "print(\"   - submission_ensemble_voting.csv (RECOMMENDED)\")\n",
    "print(\"\\nðŸ“‹ Summary:\")\n",
    "print(f\"   â€¢ {len(FEATURES)} features (same as LightGBM notebook)\")\n",
    "print(f\"   â€¢ LGB params: Optuna-optimized from LightGBM notebook\")\n",
    "print(f\"   â€¢ XGB params: Mapped from LGB params for consistency\")\n",
    "print(f\"   â€¢ Ensemble weight: {best_weight:.2f} LGB + {1-best_weight:.2f} XGB\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
