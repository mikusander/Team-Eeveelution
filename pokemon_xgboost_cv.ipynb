{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9b7eeca",
   "metadata": {},
   "source": [
    "# Pokémon battles — XGBoost with 10-fold outer CV\n",
    "Notebook breve che esegue: feature engineering, split train/val/test, 10-fold outer CV con GridSearchCV interno, valutazione per fold, valutazione su holdout e generazione submission.csv."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae11f50",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3aebef2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caricamento dati...\n",
      "Train records: 10000, Test records: 5000\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from xgboost import XGBClassifier\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# --- Percorsi (modificare se necessario) ---\n",
    "COMPETITION_NAME = 'fds-pokemon-battles-prediction-2025'\n",
    "train_file_path = 'train.jsonl'\n",
    "test_file_path = 'test.jsonl'\n",
    "\n",
    "def load_jsonl(path):\n",
    "    data = []\n",
    "    with open(path, 'r') as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line))\n",
    "    return data\n",
    "\n",
    "print('Caricamento dati...')\n",
    "train_raw = load_jsonl(train_file_path)\n",
    "test_raw = load_jsonl(test_file_path)\n",
    "print(f'Train records: {len(train_raw)}, Test records: {len(test_raw)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea38e58a",
   "metadata": {},
   "source": [
    "# Features engeneering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e15c30c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FE: 100%|██████████| 10000/10000 [00:07<00:00, 1403.93it/s]\n",
      "FE: 100%|██████████| 5000/5000 [00:03<00:00, 1441.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape train/test: (10000, 236) (5000, 235)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>battle_id</th>\n",
       "      <th>player_won</th>\n",
       "      <th>p1_base_hp_sum</th>\n",
       "      <th>p1_base_hp_mean</th>\n",
       "      <th>p1_base_hp_max</th>\n",
       "      <th>p1_base_hp_min</th>\n",
       "      <th>p1_base_hp_std</th>\n",
       "      <th>p1_base_atk_sum</th>\n",
       "      <th>p1_base_atk_mean</th>\n",
       "      <th>p1_base_atk_max</th>\n",
       "      <th>...</th>\n",
       "      <th>team_hp_sum_minus_p2lead_hp</th>\n",
       "      <th>team_spa_mean_minus_p2spa</th>\n",
       "      <th>speed_advantage</th>\n",
       "      <th>n_unique_types_diff</th>\n",
       "      <th>damage_per_turn_diff</th>\n",
       "      <th>last_pair</th>\n",
       "      <th>p1_vs_lead_avg_effectiveness</th>\n",
       "      <th>p1_vs_lead_max_effectiveness</th>\n",
       "      <th>p1_super_effective_options</th>\n",
       "      <th>p1_se_options_vs_lead_bulk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>695.0</td>\n",
       "      <td>115.833333</td>\n",
       "      <td>250.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>69.367179</td>\n",
       "      <td>435.0</td>\n",
       "      <td>72.500000</td>\n",
       "      <td>110.0</td>\n",
       "      <td>...</td>\n",
       "      <td>635.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>365.0</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.070393</td>\n",
       "      <td>starmie_VS_snorlax</td>\n",
       "      <td>1.083333</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.005405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>740.0</td>\n",
       "      <td>123.333333</td>\n",
       "      <td>250.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>64.204534</td>\n",
       "      <td>435.0</td>\n",
       "      <td>72.500000</td>\n",
       "      <td>110.0</td>\n",
       "      <td>...</td>\n",
       "      <td>685.0</td>\n",
       "      <td>-45.000000</td>\n",
       "      <td>250.0</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.012174</td>\n",
       "      <td>tauros_VS_alakazam</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>745.0</td>\n",
       "      <td>124.166667</td>\n",
       "      <td>250.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>64.382753</td>\n",
       "      <td>505.0</td>\n",
       "      <td>84.166667</td>\n",
       "      <td>130.0</td>\n",
       "      <td>...</td>\n",
       "      <td>495.0</td>\n",
       "      <td>-15.000000</td>\n",
       "      <td>345.0</td>\n",
       "      <td>6</td>\n",
       "      <td>-0.000690</td>\n",
       "      <td>snorlax_VS_gengar</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>730.0</td>\n",
       "      <td>121.666667</td>\n",
       "      <td>250.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>65.362239</td>\n",
       "      <td>465.0</td>\n",
       "      <td>77.500000</td>\n",
       "      <td>110.0</td>\n",
       "      <td>...</td>\n",
       "      <td>655.0</td>\n",
       "      <td>33.333333</td>\n",
       "      <td>345.0</td>\n",
       "      <td>6</td>\n",
       "      <td>-0.014574</td>\n",
       "      <td>snorlax_VS_zapdos</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>685.0</td>\n",
       "      <td>114.166667</td>\n",
       "      <td>250.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>70.794107</td>\n",
       "      <td>455.0</td>\n",
       "      <td>75.833333</td>\n",
       "      <td>110.0</td>\n",
       "      <td>...</td>\n",
       "      <td>625.0</td>\n",
       "      <td>-2.500000</td>\n",
       "      <td>320.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.006923</td>\n",
       "      <td>tauros_VS_chansey</td>\n",
       "      <td>1.083333</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.005405</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 236 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   battle_id  player_won  p1_base_hp_sum  p1_base_hp_mean  p1_base_hp_max  \\\n",
       "0          0           1           695.0       115.833333           250.0   \n",
       "1          1           1           740.0       123.333333           250.0   \n",
       "2          2           1           745.0       124.166667           250.0   \n",
       "3          3           1           730.0       121.666667           250.0   \n",
       "4          4           1           685.0       114.166667           250.0   \n",
       "\n",
       "   p1_base_hp_min  p1_base_hp_std  p1_base_atk_sum  p1_base_atk_mean  \\\n",
       "0            55.0       69.367179            435.0         72.500000   \n",
       "1            65.0       64.204534            435.0         72.500000   \n",
       "2            60.0       64.382753            505.0         84.166667   \n",
       "3            60.0       65.362239            465.0         77.500000   \n",
       "4            50.0       70.794107            455.0         75.833333   \n",
       "\n",
       "   p1_base_atk_max  ...  team_hp_sum_minus_p2lead_hp  \\\n",
       "0            110.0  ...                        635.0   \n",
       "1            110.0  ...                        685.0   \n",
       "2            130.0  ...                        495.0   \n",
       "3            110.0  ...                        655.0   \n",
       "4            110.0  ...                        625.0   \n",
       "\n",
       "   team_spa_mean_minus_p2spa  speed_advantage  n_unique_types_diff  \\\n",
       "0                   0.000000            365.0                    3   \n",
       "1                 -45.000000            250.0                    4   \n",
       "2                 -15.000000            345.0                    6   \n",
       "3                  33.333333            345.0                    6   \n",
       "4                  -2.500000            320.0                    4   \n",
       "\n",
       "   damage_per_turn_diff           last_pair  p1_vs_lead_avg_effectiveness  \\\n",
       "0             -0.070393  starmie_VS_snorlax                      1.083333   \n",
       "1             -0.012174  tauros_VS_alakazam                      1.000000   \n",
       "2             -0.000690   snorlax_VS_gengar                      1.000000   \n",
       "3             -0.014574   snorlax_VS_zapdos                      1.000000   \n",
       "4              0.006923   tauros_VS_chansey                      1.083333   \n",
       "\n",
       "   p1_vs_lead_max_effectiveness  p1_super_effective_options  \\\n",
       "0                           2.0                           1   \n",
       "1                           1.0                           0   \n",
       "2                           1.0                           0   \n",
       "3                           1.0                           0   \n",
       "4                           2.0                           1   \n",
       "\n",
       "   p1_se_options_vs_lead_bulk  \n",
       "0                    0.005405  \n",
       "1                    0.000000  \n",
       "2                    0.000000  \n",
       "3                    0.000000  \n",
       "4                    0.005405  \n",
       "\n",
       "[5 rows x 236 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import math\n",
    "from collections import Counter\n",
    "\n",
    "# === TYPE CHART (Gen 1) ===\n",
    "TYPE_CHART = {\n",
    "    'normal': {'rock': 0.5, 'ghost': 0},\n",
    "    'fire': {'fire': 0.5, 'water': 0.5, 'grass': 2, 'ice': 2, 'bug': 2, 'rock': 0.5, 'dragon': 0.5},\n",
    "    'water': {'fire': 2, 'water': 0.5, 'grass': 0.5, 'ground': 2, 'rock': 2, 'dragon': 0.5},\n",
    "    'grass': {'fire': 0.5, 'water': 2, 'grass': 0.5, 'poison': 0.5, 'ground': 2, 'flying': 0.5, 'bug': 0.5, 'rock': 2, 'dragon': 0.5},\n",
    "    'electric': {'water': 2, 'grass': 0.5, 'electric': 0.5, 'ground': 0, 'flying': 2, 'dragon': 0.5},\n",
    "    'ice': {'fire': 0.5, 'water': 0.5, 'grass': 2, 'ground': 2, 'flying': 2, 'dragon': 2},\n",
    "    'fighting': {'normal': 2, 'ice': 2, 'poison': 0.5, 'flying': 0.5, 'psychic': 0.5, 'bug': 0.5, 'rock': 2, 'ghost': 0},\n",
    "    'poison': {'grass': 2, 'poison': 0.5, 'ground': 0.5, 'bug': 2, 'rock': 0.5, 'ghost': 0.5},\n",
    "    'ground': {'fire': 2, 'grass': 0.5, 'electric': 2, 'poison': 2, 'flying': 0, 'bug': 0.5, 'rock': 2},\n",
    "    'flying': {'grass': 2, 'electric': 0.5, 'fighting': 2, 'bug': 2, 'rock': 0.5},\n",
    "    'psychic': {'fighting': 2, 'poison': 2, 'psychic': 0.5, 'ghost': 0},\n",
    "    'bug': {'fire': 0.5, 'grass': 2, 'fighting': 0.5, 'poison': 2, 'flying': 0.5, 'psychic': 2, 'ghost': 0.5},\n",
    "    'rock': {'fire': 2, 'ice': 2, 'fighting': 0.5, 'ground': 0.5, 'flying': 2, 'bug': 2},\n",
    "    'ghost': {'normal': 0, 'psychic': 0, 'ghost': 2},\n",
    "    'dragon': {'dragon': 2}\n",
    "}\n",
    "\n",
    "def get_effectiveness(attack_type: str, defense_types: list) -> float:\n",
    "    if not attack_type or not defense_types:\n",
    "        return 1.0\n",
    "    eff = 1.0\n",
    "    for d in defense_types:\n",
    "        eff *= TYPE_CHART.get(attack_type, {}).get(d, 1.0)\n",
    "    return eff\n",
    "\n",
    "def calculate_type_advantage(team1: list, team2_lead: dict) -> dict:\n",
    "    out = {'p1_vs_lead_avg_effectiveness': 0.0, 'p1_vs_lead_max_effectiveness': 0.0, 'p1_super_effective_options': 0}\n",
    "    if not team1 or not team2_lead:\n",
    "        return out\n",
    "    lead_types = [t.lower() for t in team2_lead.get('types', [])]\n",
    "    if not lead_types:\n",
    "        return out\n",
    "    effs = []\n",
    "    for p in team1:\n",
    "        p_types = [t.lower() for t in p.get('types', [])]\n",
    "        max_eff = 0.0\n",
    "        for pt in p_types:\n",
    "            max_eff = max(max_eff, get_effectiveness(pt, lead_types))\n",
    "        effs.append(max_eff)\n",
    "    if not effs:\n",
    "        return out\n",
    "    out['p1_vs_lead_avg_effectiveness'] = float(np.mean(effs))\n",
    "    out['p1_vs_lead_max_effectiveness'] = float(np.max(effs))\n",
    "    out['p1_super_effective_options'] = int(sum(1 for e in effs if e >= 2))\n",
    "    return out\n",
    "\n",
    "def _entropy(counter: Counter) -> float:\n",
    "    total = sum(counter.values())\n",
    "    if total == 0:\n",
    "        return 0.0\n",
    "    ent = 0.0\n",
    "    for v in counter.values():\n",
    "        p = v / total\n",
    "        if p > 0:\n",
    "            ent -= p * math.log(p, 2)\n",
    "    return ent\n",
    "\n",
    "def team_aggregate_features(team: list, prefix: str = 'p1_') -> dict:\n",
    "    stats = ['base_hp','base_atk','base_def','base_spa','base_spd','base_spe']\n",
    "    out = {}\n",
    "    vals = {s: [] for s in stats}\n",
    "    levels = []\n",
    "    types_counter = Counter()\n",
    "    names = []\n",
    "    for p in team:\n",
    "        names.append(p.get('name',''))\n",
    "        for s in stats:\n",
    "            vals[s].append(p.get(s, 0))\n",
    "        levels.append(p.get('level', 0))\n",
    "        for t in p.get('types', []):\n",
    "            types_counter[t.lower()] += 1\n",
    "    for s in stats:\n",
    "        arr = np.array(vals[s], dtype=float)\n",
    "        out[f'{prefix}{s}_sum'] = float(arr.sum())\n",
    "        out[f'{prefix}{s}_mean'] = float(arr.mean())\n",
    "        out[f'{prefix}{s}_max'] = float(arr.max())\n",
    "        out[f'{prefix}{s}_min'] = float(arr.min())\n",
    "        out[f'{prefix}{s}_std'] = float(arr.std())\n",
    "    level_arr = np.array(levels, dtype=float)\n",
    "    out[f'{prefix}level_mean'] = float(level_arr.mean()) if level_arr.size else 0.0\n",
    "    out[f'{prefix}level_sum'] = float(level_arr.sum()) if level_arr.size else 0.0\n",
    "    out[f'{prefix}n_unique_types'] = int(len(types_counter))\n",
    "    common_types = ['normal','fire','water','electric','grass','psychic','ice','dragon','rock','ground','flying']\n",
    "    for t in common_types:\n",
    "        out[f'{prefix}type_{t}_count'] = int(types_counter.get(t, 0))\n",
    "    out[f'{prefix}lead_name'] = names[0] if names else ''\n",
    "    out[f'{prefix}n_unique_names'] = int(len(set(names)))\n",
    "    out[f'{prefix}type_entropy'] = float(_entropy(types_counter))\n",
    "    spe_arr = np.array(vals['base_spe'], dtype=float)\n",
    "    out[f'{prefix}spe_p25'] = float(np.percentile(spe_arr, 25)) if spe_arr.size else 0.0\n",
    "    out[f'{prefix}spe_p50'] = float(np.percentile(spe_arr, 50)) if spe_arr.size else 0.0\n",
    "    out[f'{prefix}spe_p75'] = float(np.percentile(spe_arr, 75)) if spe_arr.size else 0.0\n",
    "    return out\n",
    "\n",
    "def lead_vs_lead_features(p1_lead: dict, p2_lead: dict) -> dict:\n",
    "    out = {}\n",
    "    stats = ['base_hp','base_atk','base_def','base_spa','base_spd','base_spe']\n",
    "    for s in stats:\n",
    "        out[f'lead_diff_{s}'] = float(p1_lead.get(s,0) - p2_lead.get(s,0))\n",
    "    out['lead_speed_advantage'] = float(p1_lead.get('base_spe',0) - p2_lead.get('base_spe',0))\n",
    "    p1_types = [t.lower() for t in p1_lead.get('types', [])]\n",
    "    p2_types = [t.lower() for t in p2_lead.get('types', [])]\n",
    "    max_eff = 0.0\n",
    "    for pt in p1_types:\n",
    "        max_eff = max(max_eff, get_effectiveness(pt, p2_types))\n",
    "    out['lead_p1_vs_p2_effectiveness'] = float(max_eff)\n",
    "    return out\n",
    "\n",
    "def lead_aggregate_features(pokemon: dict, prefix: str = 'p2_lead_') -> dict:\n",
    "    out = {}\n",
    "    stats = ['base_hp','base_atk','base_def','base_spa','base_spd','base_spe']\n",
    "    for s in stats:\n",
    "        out[f'{prefix}{s}'] = float(pokemon.get(s,0))\n",
    "    out[f'{prefix}level'] = int(pokemon.get('level',0))\n",
    "    types = [x.lower() for x in pokemon.get('types', [])]\n",
    "    common_types = ['normal','fire','water','electric','grass','psychic','ice','dragon','rock','ground','flying']\n",
    "    for t in common_types:\n",
    "        out[f'{prefix}type_{t}'] = int(t in types)\n",
    "    out[f'{prefix}name'] = pokemon.get('name','')\n",
    "    out[f'{prefix}n_unique_types'] = int(len(set(types)))\n",
    "    return out\n",
    "\n",
    "def summary_from_timeline(timeline: list, p1_team: list) -> dict:\n",
    "    out = {}\n",
    "    if not timeline:\n",
    "        return {'tl_p1_moves':0,'tl_p2_moves':0,'tl_p1_est_damage':0.0,'tl_p2_est_damage':0.0,'damage_diff':0.0}\n",
    "    p1_moves = p2_moves = 0\n",
    "    p1_damage = p2_damage = 0.0\n",
    "    p1_last_active = p2_last_active = ''\n",
    "    p1_last_hp = p2_last_hp = np.nan\n",
    "    p1_fainted = p2_fainted = 0\n",
    "    p1_fainted_names = set()\n",
    "    p2_fainted_names = set()\n",
    "    last_p1_hp = {}\n",
    "    last_p2_hp = {}\n",
    "    p1_comeback_kos = 0\n",
    "    p2_comeback_kos = 0\n",
    "    p1_inflicted_statuses = Counter()\n",
    "    p2_inflicted_statuses = Counter()\n",
    "    p1_pokemon_statuses = {}\n",
    "    p2_pokemon_statuses = {}\n",
    "    p1_move_type_counts = Counter()\n",
    "    p2_move_type_counts = Counter()\n",
    "    p1_damage_first2 = 0.0\n",
    "    p2_damage_first2 = 0.0\n",
    "\n",
    "    # NEW: per-turn damage accumulation, KO timing and early/late KO counters\n",
    "    p1_dmg_by_turn = {}  # damage inflitto da p1 (contro p2) per turno\n",
    "    p2_dmg_by_turn = {}  # damage inflitto da p2 (contro p1) per turno\n",
    "    seen_turns = set()\n",
    "    first_ko_turn_p1_taken = None   # primo KO subìto da p1 (p1_fainted++)\n",
    "    first_ko_turn_p1_inflicted = None  # primo KO inflitto da p1 (p2_fainted++)\n",
    "    early_threshold = 10\n",
    "    p1_kos_early = p1_kos_late = 0\n",
    "    p2_kos_early = p2_kos_late = 0\n",
    "\n",
    "    for turn in timeline[:30]:\n",
    "        prev_p1_fainted, prev_p2_fainted = p1_fainted, p2_fainted\n",
    "        p1_state = turn.get('p1_pokemon_state',{}) or {}\n",
    "        p2_state = turn.get('p2_pokemon_state',{}) or {}\n",
    "        tnum = turn.get('turn', None)\n",
    "        if tnum is None:\n",
    "            # fallback: usa lunghezza dei turni visti + 1\n",
    "            tnum = (len(seen_turns) + 1)\n",
    "        seen_turns.add(tnum)\n",
    "\n",
    "        if p1_state.get('name'):\n",
    "            p1_last_active = p1_state.get('name')\n",
    "        if p2_state.get('name'):\n",
    "            p2_last_active = p2_state.get('name')\n",
    "\n",
    "        if p1_state.get('fainted') and p1_state.get('name') not in p1_fainted_names:\n",
    "            p1_fainted += 1\n",
    "            p1_fainted_names.add(p1_state.get('name'))\n",
    "            if first_ko_turn_p1_taken is None:\n",
    "                first_ko_turn_p1_taken = tnum\n",
    "            if tnum <= early_threshold: p2_kos_early += 1\n",
    "            else: p2_kos_late += 1\n",
    "        if p2_state.get('fainted') and p2_state.get('name') not in p2_fainted_names:\n",
    "            p2_fainted += 1\n",
    "            p2_fainted_names.add(p2_state.get('name'))\n",
    "            if first_ko_turn_p1_inflicted is None:\n",
    "                first_ko_turn_p1_inflicted = tnum\n",
    "            if tnum <= early_threshold: p1_kos_early += 1\n",
    "            else: p1_kos_late += 1\n",
    "\n",
    "        p2_name, p2_hp = p2_state.get('name'), p2_state.get('hp_pct')\n",
    "        if p2_name and p2_hp is not None:\n",
    "            prev_hp = last_p2_hp.get(p2_name)\n",
    "            if prev_hp is not None:\n",
    "                delta = max(0.0, prev_hp - p2_hp)\n",
    "                p1_damage += delta\n",
    "                p1_dmg_by_turn[tnum] = p1_dmg_by_turn.get(tnum, 0.0) + delta\n",
    "                if turn.get('turn',999) <= 2:\n",
    "                    p1_damage_first2 += delta\n",
    "            last_p2_hp[p2_name] = p2_hp\n",
    "\n",
    "        p1_name, p1_hp = p1_state.get('name'), p1_state.get('hp_pct')\n",
    "        if p1_name and p1_hp is not None:\n",
    "            prev_hp = last_p1_hp.get(p1_name)\n",
    "            if prev_hp is not None:\n",
    "                delta = max(0.0, prev_hp - p1_hp)\n",
    "                p2_damage += delta\n",
    "                p2_dmg_by_turn[tnum] = p2_dmg_by_turn.get(tnum, 0.0) + delta\n",
    "                if turn.get('turn',999) <= 2:\n",
    "                    p2_damage_first2 += delta\n",
    "            last_p1_hp[p1_name] = p1_hp\n",
    "\n",
    "        damage_diff_so_far = p1_damage - p2_damage\n",
    "        if p2_fainted > prev_p2_fainted and damage_diff_so_far < -1.0:\n",
    "            p1_comeback_kos += 1\n",
    "        if p1_fainted > prev_p1_fainted and damage_diff_so_far > 1.0:\n",
    "            p2_comeback_kos += 1\n",
    "\n",
    "        p2_status = p2_state.get('status')\n",
    "        if p2_name and p2_status and p2_pokemon_statuses.get(p2_name) != p2_status:\n",
    "            p1_inflicted_statuses[p2_status] += 1\n",
    "            p2_pokemon_statuses[p2_name] = p2_status\n",
    "        p1_status = p1_state.get('status')\n",
    "        if p1_name and p1_status and p1_pokemon_statuses.get(p1_name) != p1_status:\n",
    "            p2_inflicted_statuses[p1_status] += 1\n",
    "            p1_pokemon_statuses[p1_name] = p1_status\n",
    "\n",
    "        p1_move = turn.get('p1_move_details') or {}\n",
    "        p2_move = turn.get('p2_move_details') or {}\n",
    "        if p1_move and p1_move.get('type'):\n",
    "            p1_move_type_counts[(p1_move.get('type') or '').lower()] += 1\n",
    "        if p2_move and p2_move.get('type'):\n",
    "            p2_move_type_counts[(p2_move.get('type') or '').lower()] += 1\n",
    "        if turn.get('p1_move_details'):\n",
    "            p1_moves += 1\n",
    "        if turn.get('p2_move_details'):\n",
    "            p2_moves += 1\n",
    "        p1_last_hp = p1_state.get('hp_pct', np.nan)\n",
    "        p2_last_hp = p2_state.get('hp_pct', np.nan)\n",
    "\n",
    "    # ...existing code computing out[...] baseline metrics...\n",
    "    out['tl_p1_moves'] = int(p1_moves)\n",
    "    out['tl_p2_moves'] = int(p2_moves)\n",
    "    out['tl_p1_est_damage'] = float(p1_damage)\n",
    "    out['tl_p2_est_damage'] = float(p2_damage)\n",
    "    # NUOVE FEATURE: conteggio KO per squadra e rate normalizzati per turno\n",
    "    out['tl_p1_fainted'] = int(p1_fainted)\n",
    "    out['tl_p2_fainted'] = int(p2_fainted)\n",
    "    turns_count = max(1, len(seen_turns))\n",
    "    out['tl_p1_fainted_rate'] = float(out['tl_p1_fainted'] / turns_count)\n",
    "    out['tl_p2_fainted_rate'] = float(out['tl_p2_fainted'] / turns_count)\n",
    "    # fine nuovi features\n",
    "    out['damage_diff'] = float(p1_damage - p2_damage)\n",
    "    out['fainted_diff'] = int(p1_fainted - p2_fainted)\n",
    "    out['tl_p1_last_hp'] = float(p1_last_hp) if not np.isnan(p1_last_hp) else 0.0\n",
    "    out['tl_p2_last_hp'] = float(p2_last_hp) if not np.isnan(p2_last_hp) else 0.0\n",
    "    out['tl_p1_last_active'] = p1_last_active\n",
    "    out['tl_p2_last_active'] = p2_last_active\n",
    "    if p1_team:\n",
    "        p1_total_hp_sum = sum(p.get('base_hp',0) for p in p1_team)\n",
    "        p1_avg_def = np.mean([p.get('base_def',0) for p in p1_team] or [0])\n",
    "        p1_avg_spd = np.mean([p.get('base_spd',0) for p in p1_team] or [0])\n",
    "        out['tl_p2_damage_vs_p1_hp_pool'] = float(p2_damage / (p1_total_hp_sum + 1e-6))\n",
    "        out['tl_p1_defensive_endurance'] = float((p1_avg_def + p1_avg_spd) / (p2_damage + 1e-6))\n",
    "    out['tl_p1_comeback_kos'] = int(p1_comeback_kos)\n",
    "    out['tl_p2_comeback_kos'] = int(p2_comeback_kos)\n",
    "    out['tl_comeback_kos_diff'] = int(p1_comeback_kos - p2_comeback_kos)\n",
    "\n",
    "    common_statuses = ['brn','par','slp','frz','psn','tox']\n",
    "    for status in common_statuses:\n",
    "        out[f'tl_p1_inflicted_{status}_count'] = int(p1_inflicted_statuses.get(status,0))\n",
    "        out[f'tl_p2_inflicted_{status}_count'] = int(p2_inflicted_statuses.get(status,0))\n",
    "        out[f'tl_inflicted_{status}_diff'] = int(p1_inflicted_statuses.get(status,0) - p2_inflicted_statuses.get(status,0))\n",
    "\n",
    "    common_move_types = ['normal','fire','water','electric','grass','psychic','ice','dragon','rock','ground','flying','ghost','bug','poison','fighting']\n",
    "    for mt in common_move_types:\n",
    "        out[f'tl_p1_move_type_{mt}_count'] = int(p1_move_type_counts.get(mt,0))\n",
    "        out[f'tl_p2_move_type_{mt}_count'] = int(p2_move_type_counts.get(mt,0))\n",
    "        out[f'tl_move_type_{mt}_count_diff'] = int(p1_move_type_counts.get(mt,0) - p2_move_type_counts.get(mt,0))\n",
    "\n",
    "    out['tl_p1_damage_first2'] = float(p1_damage_first2)\n",
    "    out['tl_p2_damage_first2'] = float(p2_damage_first2)\n",
    "    out['tl_first2_damage_diff'] = float(p1_damage_first2 - p2_damage_first2)\n",
    "\n",
    "    # NEW: derived, normalized and late-game features\n",
    "    turns_count = max(1, len(seen_turns))\n",
    "    out['tl_turns_count'] = int(turns_count)\n",
    "    out['tl_p1_moves_rate'] = float(p1_moves / turns_count)\n",
    "    out['tl_p2_moves_rate'] = float(p2_moves / turns_count)\n",
    "    out['tl_p1_damage_per_turn'] = float(p1_damage / turns_count)\n",
    "    out['tl_p2_damage_per_turn'] = float(p2_damage / turns_count)\n",
    "    out['tl_damage_rate_diff'] = float(out['tl_p1_damage_per_turn'] - out['tl_p2_damage_per_turn'])\n",
    "\n",
    "    # last-5-turns damage window\n",
    "    if seen_turns:\n",
    "        recent_turns = sorted(seen_turns)[-5:]\n",
    "        p1_last5 = sum(p1_dmg_by_turn.get(t,0.0) for t in recent_turns)\n",
    "        p2_last5 = sum(p2_dmg_by_turn.get(t,0.0) for t in recent_turns)\n",
    "    else:\n",
    "        p1_last5 = p2_last5 = 0.0\n",
    "    out['tl_p1_damage_last5'] = float(p1_last5)\n",
    "    out['tl_p2_damage_last5'] = float(p2_last5)\n",
    "    out['tl_last5_damage_diff'] = float(p1_last5 - p2_last5)\n",
    "    # NEW: ratio danno ultimi 5 turni vs totale\n",
    "    out['tl_p1_last5_damage_ratio'] = float(p1_last5 / (p1_damage + 1e-6))\n",
    "    out['tl_p2_last5_damage_ratio'] = float(p2_last5 / (p2_damage + 1e-6))\n",
    "    out['tl_last5_damage_ratio_diff'] = float(out['tl_p1_last5_damage_ratio'] - out['tl_p2_last5_damage_ratio'])\n",
    "\n",
    "    # time-weighted damage advantage (peso crescente con il turno)\n",
    "    if seen_turns:\n",
    "        ts = sorted(seen_turns)\n",
    "        w = np.linspace(1.0, 2.0, num=len(ts))  # pesi crescenti\n",
    "        w = w / (w.sum() + 1e-9)\n",
    "        adv = [(p1_dmg_by_turn.get(t,0.0) - p2_dmg_by_turn.get(t,0.0)) for t in ts]\n",
    "        out['tl_weighted_damage_diff'] = float(np.dot(w, adv))\n",
    "    else:\n",
    "        out['tl_weighted_damage_diff'] = 0.0\n",
    "\n",
    "    # NEW: comeback indicator (cambio di segno dell'adv cumulativo)\n",
    "    if seen_turns:\n",
    "        ts = sorted(seen_turns)\n",
    "        cum = 0.0\n",
    "        signs = []\n",
    "        for t in ts:\n",
    "            cum += (p1_dmg_by_turn.get(t,0.0) - p2_dmg_by_turn.get(t,0.0))\n",
    "            s = 1 if cum > 1e-9 else (-1 if cum < -1e-9 else 0)\n",
    "            if s != 0:\n",
    "                if not signs or signs[-1] != s:\n",
    "                    signs.append(s)\n",
    "        sign_flips = max(0, len(signs) - 1)\n",
    "        comeback_flag = 1 if (len(signs) >= 2 and signs[0] != signs[-1]) else 0\n",
    "    else:\n",
    "        sign_flips = 0\n",
    "        comeback_flag = 0\n",
    "    out['tl_damage_adv_sign_flips'] = int(sign_flips)\n",
    "    out['tl_comeback_flag'] = int(comeback_flag)\n",
    "\n",
    "    # KO timing and early/late counts\n",
    "    out['tl_first_ko_turn_p1_inflicted'] = int(first_ko_turn_p1_inflicted or 0)\n",
    "    out['tl_first_ko_turn_p1_taken'] = int(first_ko_turn_p1_taken or 0)\n",
    "    out['tl_first_ko_turn_diff'] = int((first_ko_turn_p1_inflicted or 0) - (first_ko_turn_p1_taken or 0))\n",
    "    out['tl_kos_early_p1'] = int(p1_kos_early)\n",
    "    out['tl_kos_late_p1'] = int(p1_kos_late)\n",
    "    out['tl_kos_early_p2'] = int(p2_kos_early)\n",
    "    out['tl_kos_late_p2'] = int(p2_kos_late)\n",
    "\n",
    "    # normalized status rates per turn\n",
    "    for status in common_statuses:\n",
    "        c1 = p1_inflicted_statuses.get(status,0)\n",
    "        c2 = p2_inflicted_statuses.get(status,0)\n",
    "        out[f'tl_p1_inflicted_{status}_rate'] = float(c1 / turns_count)\n",
    "        out[f'tl_p2_inflicted_{status}_rate'] = float(c2 / turns_count)\n",
    "        out[f'tl_inflicted_{status}_rate_diff'] = float((c1 - c2) / turns_count)\n",
    "\n",
    "    return out\n",
    "\n",
    "def ability_features(team: list, prefix: str) -> dict:\n",
    "    immunity_abilities = {'levitate':0,'volt_absorb':0,'water_absorb':0,'flash_fire':0}\n",
    "    stat_drop_abilities = {'intimidate':0}\n",
    "    weather_abilities = {'drought':0,'drizzle':0,'sand_stream':0}\n",
    "    out = {}\n",
    "    for pokemon in team:\n",
    "        ability = (pokemon.get('ability','') or '').lower().replace(' ','_')\n",
    "        if ability in immunity_abilities:\n",
    "            immunity_abilities[ability] += 1\n",
    "        if ability in stat_drop_abilities:\n",
    "            stat_drop_abilities[ability] += 1\n",
    "        if ability in weather_abilities:\n",
    "            weather_abilities[ability] += 1\n",
    "    for ability,count in immunity_abilities.items():\n",
    "        out[f'{prefix}ability_{ability}_count'] = int(count)\n",
    "    for ability,count in stat_drop_abilities.items():\n",
    "        out[f'{prefix}ability_{ability}_count'] = int(count)\n",
    "    for ability,count in weather_abilities.items():\n",
    "        out[f'{prefix}ability_{ability}_count'] = int(count)\n",
    "    out[f'{prefix}total_immunity_abilities'] = int(sum(immunity_abilities.values()))\n",
    "    out[f'{prefix}total_stat_drop_abilities'] = int(sum(stat_drop_abilities.values()))\n",
    "    return out\n",
    "\n",
    "def prepare_record_features(record: dict, max_turns: int = 30) -> dict:\n",
    "    out = {}\n",
    "    out['battle_id'] = record.get('battle_id')\n",
    "    if 'player_won' in record:\n",
    "        out['player_won'] = int(bool(record.get('player_won')))\n",
    "    p1_team = record.get('p1_team_details', [])\n",
    "    out.update(team_aggregate_features(p1_team, prefix='p1_'))\n",
    "    p2_lead = record.get('p2_lead_details', {})\n",
    "    out.update(lead_aggregate_features(p2_lead, prefix='p2_lead_'))\n",
    "    out.update(ability_features(p1_team, prefix='p1_'))\n",
    "    p1_lead = p1_team[0] if p1_team else {}\n",
    "    out.update(lead_vs_lead_features(p1_lead, p2_lead))\n",
    "    out.update(ability_features([p2_lead], prefix='p2_lead_'))\n",
    "    out['p1_intimidate_vs_lead'] = 1 if out.get('p1_ability_intimidate_count',0) > 0 else 0\n",
    "    tl = record.get('battle_timeline', [])\n",
    "    out.update(summary_from_timeline(tl[:max_turns], p1_team))\n",
    "    out['team_hp_sum_minus_p2lead_hp'] = out.get('p1_base_hp_sum', 0) - out.get('p2_lead_base_hp', 0)\n",
    "    out['team_spa_mean_minus_p2spa'] = out.get('p1_base_spa_mean', 0) - out.get('p2_lead_base_spa', 0)\n",
    "    out['speed_advantage'] = out.get('p1_base_spe_sum', 0) - out.get('p2_lead_base_spe', 0)\n",
    "    out['n_unique_types_diff'] = out.get('p1_n_unique_types', 0) - out.get('p2_lead_n_unique_types', 1)\n",
    "    p1_moves = max(out.get('tl_p1_moves',1),1)\n",
    "    p2_moves = max(out.get('tl_p2_moves',1),1)\n",
    "    out['damage_per_turn_diff'] = (out.get('tl_p1_est_damage',0.0)/p1_moves) - (out.get('tl_p2_est_damage',0.0)/p2_moves)\n",
    "    out['last_pair'] = f\"{out.get('tl_p1_last_active','')}_VS_{out.get('tl_p2_last_active','')}\"\n",
    "    out.update(calculate_type_advantage(p1_team, p2_lead))\n",
    "    p2_lead_bulk = out.get('p2_lead_base_def',1) + out.get('p2_lead_base_spd',1)\n",
    "    out['p1_se_options_vs_lead_bulk'] = out.get('p1_super_effective_options',0) / (p2_lead_bulk + 1e-6)\n",
    "    p2_team = record.get('p2_team_details', [])\n",
    "    if p2_team:\n",
    "        out.update(team_aggregate_features(p2_team, prefix='p2_'))\n",
    "        out['team_hp_sum_diff'] = out.get('p1_base_hp_sum',0) - out.get('p2_base_hp_sum',0)\n",
    "        out['team_spa_mean_diff'] = out.get('p1_base_spa_mean',0) - out.get('p2_base_spa_mean',0)\n",
    "        out['team_spe_mean_diff'] = out.get('p1_base_spe_mean',0) - out.get('p2_base_spe_mean',0)\n",
    "        out['n_unique_types_team_diff'] = out.get('p1_n_unique_types',0) - out.get('p2_n_unique_types',0)\n",
    "    return out\n",
    "\n",
    "def create_features_from_raw(data: list) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    for b in tqdm(data, desc='FE'):\n",
    "        try:\n",
    "            feat = prepare_record_features(b, max_turns=30)\n",
    "            if 'battle_id' not in feat:\n",
    "                feat['battle_id'] = b.get('battle_id')\n",
    "            rows.append(feat)\n",
    "        except Exception as e:\n",
    "            rows.append({'battle_id': b.get('battle_id'), 'error': 1})\n",
    "    df = pd.DataFrame(rows)\n",
    "    if 'player_won' in df.columns:\n",
    "        df['player_won'] = df['player_won'].astype(int)\n",
    "    return df.fillna(0)\n",
    "\n",
    "train_df = create_features_from_raw(train_raw)\n",
    "test_df = create_features_from_raw(test_raw)\n",
    "print('Feature shape train/test:', train_df.shape, test_df.shape)\n",
    "display(train_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6903c13a",
   "metadata": {},
   "source": [
    "# Search best features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "626ecdb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TreeExplainer non funzionante o formato non previsto, fallback a shap.Explainer: could not convert string to float: '[5.06E-1]'\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 59\u001b[39m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     explainer = \u001b[43mshap\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTreeExplainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshap_clf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     60\u001b[39m     shap_vals = explainer.shap_values(X_sample)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\shap\\explainers\\_tree.py:278\u001b[39m, in \u001b[36mTreeExplainer.__init__\u001b[39m\u001b[34m(self, model, data, model_output, feature_perturbation, feature_names, approximate, link, linearize_link)\u001b[39m\n\u001b[32m    277\u001b[39m \u001b[38;5;28mself\u001b[39m.expected_value = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m278\u001b[39m \u001b[38;5;28mself\u001b[39m.model = \u001b[43mTreeEnsemble\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdata_missing\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    279\u001b[39m \u001b[38;5;28mself\u001b[39m.model_output = model_output\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\shap\\explainers\\_tree.py:1261\u001b[39m, in \u001b[36mTreeEnsemble.__init__\u001b[39m\u001b[34m(self, model, data, data_missing, model_output)\u001b[39m\n\u001b[32m   1260\u001b[39m \u001b[38;5;28mself\u001b[39m.original_model = model.get_booster()\n\u001b[32m-> \u001b[39m\u001b[32m1261\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_set_xgboost_model_attributes\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1262\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1263\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_missing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1264\u001b[39m \u001b[43m    \u001b[49m\u001b[43mobjective_name_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1265\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtree_output_name_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1266\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1268\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.model_output == \u001b[33m\"\u001b[39m\u001b[33mpredict_proba\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\shap\\explainers\\_tree.py:1506\u001b[39m, in \u001b[36mTreeEnsemble._set_xgboost_model_attributes\u001b[39m\u001b[34m(self, data, data_missing, objective_name_map, tree_output_name_map)\u001b[39m\n\u001b[32m   1505\u001b[39m \u001b[38;5;28mself\u001b[39m.model_type = \u001b[33m\"\u001b[39m\u001b[33mxgboost\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1506\u001b[39m loader = \u001b[43mXGBTreeModelLoader\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moriginal_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1508\u001b[39m \u001b[38;5;28mself\u001b[39m.trees = loader.get_trees(data=data, data_missing=data_missing)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\shap\\explainers\\_tree.py:2104\u001b[39m, in \u001b[36mXGBTreeModelLoader.__init__\u001b[39m\u001b[34m(self, xgb_model)\u001b[39m\n\u001b[32m   2103\u001b[39m \u001b[38;5;28mself\u001b[39m.n_targets = n_targets\n\u001b[32m-> \u001b[39m\u001b[32m2104\u001b[39m \u001b[38;5;28mself\u001b[39m.base_score = \u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlearner_model_param\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mbase_score\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2105\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.n_trees_per_iter > \u001b[32m0\u001b[39m\n",
      "\u001b[31mValueError\u001b[39m: could not convert string to float: '[5.06E-1]'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 65\u001b[39m\n\u001b[32m     63\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTreeExplainer non funzionante o formato non previsto, fallback a shap.Explainer:\u001b[39m\u001b[33m\"\u001b[39m, e)\n\u001b[32m     64\u001b[39m explainer = shap.Explainer(shap_clf.predict_proba, X_sample)\n\u001b[32m---> \u001b[39m\u001b[32m65\u001b[39m ev = \u001b[43mexplainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_sample\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     66\u001b[39m vals = ev.values\n\u001b[32m     67\u001b[39m shap_arr = _to_shap_array(vals, n_samples, n_features)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\shap\\explainers\\_permutation.py:100\u001b[39m, in \u001b[36mPermutationExplainer.__call__\u001b[39m\u001b[34m(self, max_evals, main_effects, error_bounds, batch_size, outputs, silent, *args)\u001b[39m\n\u001b[32m     89\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\n\u001b[32m     90\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m     91\u001b[39m     *args,\n\u001b[32m   (...)\u001b[39m\u001b[32m     97\u001b[39m     silent=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m     98\u001b[39m ):\n\u001b[32m     99\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Explain the output of the model on the given arguments.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m100\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    101\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    102\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_evals\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_evals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    103\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmain_effects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmain_effects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    104\u001b[39m \u001b[43m        \u001b[49m\u001b[43merror_bounds\u001b[49m\u001b[43m=\u001b[49m\u001b[43merror_bounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    105\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    106\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    107\u001b[39m \u001b[43m        \u001b[49m\u001b[43msilent\u001b[49m\u001b[43m=\u001b[49m\u001b[43msilent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    108\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\shap\\explainers\\_explainer.py:364\u001b[39m, in \u001b[36mExplainer.__call__\u001b[39m\u001b[34m(self, max_evals, main_effects, error_bounds, batch_size, outputs, silent, *args, **kwargs)\u001b[39m\n\u001b[32m    362\u001b[39m     feature_names = [[] \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(args))]\n\u001b[32m    363\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m row_args \u001b[38;5;129;01min\u001b[39;00m show_progress(\u001b[38;5;28mzip\u001b[39m(*args), num_rows, \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m + \u001b[33m\"\u001b[39m\u001b[33m explainer\u001b[39m\u001b[33m\"\u001b[39m, silent):\n\u001b[32m--> \u001b[39m\u001b[32m364\u001b[39m     row_result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mexplain_row\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    365\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43mrow_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    366\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_evals\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_evals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    367\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmain_effects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmain_effects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    368\u001b[39m \u001b[43m        \u001b[49m\u001b[43merror_bounds\u001b[49m\u001b[43m=\u001b[49m\u001b[43merror_bounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    369\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    370\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    371\u001b[39m \u001b[43m        \u001b[49m\u001b[43msilent\u001b[49m\u001b[43m=\u001b[49m\u001b[43msilent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    372\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    373\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    374\u001b[39m     values.append(row_result.get(\u001b[33m\"\u001b[39m\u001b[33mvalues\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    375\u001b[39m     output_indices.append(row_result.get(\u001b[33m\"\u001b[39m\u001b[33moutput_indices\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\shap\\explainers\\_permutation.py:200\u001b[39m, in \u001b[36mPermutationExplainer.explain_row\u001b[39m\u001b[34m(self, max_evals, main_effects, error_bounds, batch_size, outputs, silent, *row_args)\u001b[39m\n\u001b[32m    198\u001b[39m     \u001b[38;5;66;03m# compute the main effects if we need to\u001b[39;00m\n\u001b[32m    199\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m main_effects:\n\u001b[32m--> \u001b[39m\u001b[32m200\u001b[39m         main_effect_values = \u001b[43mfm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmain_effects\u001b[49m\u001b[43m(\u001b[49m\u001b[43minds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    201\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    202\u001b[39m     masks = np.zeros(\u001b[32m1\u001b[39m, dtype=\u001b[38;5;28mint\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\shap\\utils\\_masked_model.py:270\u001b[39m, in \u001b[36mMaskedModel.main_effects\u001b[39m\u001b[34m(self, inds, batch_size)\u001b[39m\n\u001b[32m    267\u001b[39m     last_ind = inds[i]\n\u001b[32m    269\u001b[39m \u001b[38;5;66;03m# compute the main effects for the given indexes\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m270\u001b[39m outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmasks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    271\u001b[39m main_effects = outputs[\u001b[32m1\u001b[39m:] - outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    273\u001b[39m \u001b[38;5;66;03m# expand the vector to the full input size\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\shap\\utils\\_masked_model.py:57\u001b[39m, in \u001b[36mMaskedModel.__call__\u001b[39m\u001b[34m(self, masks, zero_index, batch_size)\u001b[39m\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(masks.shape) == \u001b[32m1\u001b[39m:\n\u001b[32m     56\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.masker, \u001b[33m\"\u001b[39m\u001b[33msupports_delta_masking\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_delta_masking_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmasks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mzero_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43mzero_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     59\u001b[39m     \u001b[38;5;66;03m# we need to convert from delta masking to a full masking call because we were given a delta masking\u001b[39;00m\n\u001b[32m     60\u001b[39m     \u001b[38;5;66;03m# input but the masker does not support delta masking\u001b[39;00m\n\u001b[32m     61\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     62\u001b[39m         full_masks = np.zeros((\u001b[38;5;28mint\u001b[39m(np.sum(masks >= \u001b[32m0\u001b[39m)), \u001b[38;5;28mself\u001b[39m._masker_cols), dtype=\u001b[38;5;28mbool\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\shap\\utils\\_masked_model.py:209\u001b[39m, in \u001b[36mMaskedModel._delta_masking_call\u001b[39m\u001b[34m(self, masks, zero_index, batch_size)\u001b[39m\n\u001b[32m    206\u001b[39m     batch_positions[i + \u001b[32m1\u001b[39m] = batch_positions[i] + num_varying_rows[i]\n\u001b[32m    208\u001b[39m \u001b[38;5;66;03m# joined_masked_inputs = self._stack_inputs(all_masked_inputs)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m209\u001b[39m outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43msubset_masked_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    210\u001b[39m _assert_output_input_match(subset_masked_inputs, outputs)\n\u001b[32m    212\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.linearize_link \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.link != links.identity \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._linearizing_weights \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\shap\\models\\_model.py:23\u001b[39m, in \u001b[36mModel.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args):\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m     out = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minner_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     24\u001b[39m     is_tensor = safe_isinstance(out, \u001b[33m\"\u001b[39m\u001b[33mtorch.Tensor\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     25\u001b[39m     out = out.cpu().detach().numpy() \u001b[38;5;28;01mif\u001b[39;00m is_tensor \u001b[38;5;28;01melse\u001b[39;00m np.array(out)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\xgboost\\sklearn.py:1918\u001b[39m, in \u001b[36mXGBClassifier.predict_proba\u001b[39m\u001b[34m(self, X, validate_features, base_margin, iteration_range)\u001b[39m\n\u001b[32m   1916\u001b[39m     class_prob = softmax(raw_predt, axis=\u001b[32m1\u001b[39m)\n\u001b[32m   1917\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m class_prob\n\u001b[32m-> \u001b[39m\u001b[32m1918\u001b[39m class_probs = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1919\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1920\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalidate_features\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalidate_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1921\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbase_margin\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbase_margin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1922\u001b[39m \u001b[43m    \u001b[49m\u001b[43miteration_range\u001b[49m\u001b[43m=\u001b[49m\u001b[43miteration_range\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1923\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1924\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _cls_predict_proba(\u001b[38;5;28mself\u001b[39m.n_classes_, class_probs, np.vstack)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\xgboost\\core.py:774\u001b[39m, in \u001b[36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    772\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig.parameters, args):\n\u001b[32m    773\u001b[39m     kwargs[k] = arg\n\u001b[32m--> \u001b[39m\u001b[32m774\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\xgboost\\sklearn.py:1443\u001b[39m, in \u001b[36mXGBModel.predict\u001b[39m\u001b[34m(self, X, output_margin, validate_features, base_margin, iteration_range)\u001b[39m\n\u001b[32m   1441\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._can_use_inplace_predict():\n\u001b[32m   1442\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1443\u001b[39m         predts = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_booster\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43minplace_predict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1444\u001b[39m \u001b[43m            \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1445\u001b[39m \u001b[43m            \u001b[49m\u001b[43miteration_range\u001b[49m\u001b[43m=\u001b[49m\u001b[43miteration_range\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1446\u001b[39m \u001b[43m            \u001b[49m\u001b[43mpredict_type\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmargin\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moutput_margin\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalue\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1447\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmissing\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmissing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1448\u001b[39m \u001b[43m            \u001b[49m\u001b[43mbase_margin\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbase_margin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1449\u001b[39m \u001b[43m            \u001b[49m\u001b[43mvalidate_features\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalidate_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1450\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1451\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m _is_cupy_alike(predts):\n\u001b[32m   1452\u001b[39m             cp = import_cupy()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\xgboost\\core.py:774\u001b[39m, in \u001b[36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    772\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig.parameters, args):\n\u001b[32m    773\u001b[39m     kwargs[k] = arg\n\u001b[32m--> \u001b[39m\u001b[32m774\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\xgboost\\core.py:2888\u001b[39m, in \u001b[36mBooster.inplace_predict\u001b[39m\u001b[34m(self, data, iteration_range, predict_type, missing, validate_features, base_margin, strict_shape)\u001b[39m\n\u001b[32m   2885\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _prediction_output(shape, dims, preds, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m   2886\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, (ArrowTransformed, PandasTransformed)):\n\u001b[32m   2887\u001b[39m     _check_call(\n\u001b[32m-> \u001b[39m\u001b[32m2888\u001b[39m         \u001b[43m_LIB\u001b[49m\u001b[43m.\u001b[49m\u001b[43mXGBoosterPredictFromColumnar\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2889\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2890\u001b[39m \u001b[43m            \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m.\u001b[49m\u001b[43marray_interface\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2891\u001b[39m \u001b[43m            \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2892\u001b[39m \u001b[43m            \u001b[49m\u001b[43mp_handle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2893\u001b[39m \u001b[43m            \u001b[49m\u001b[43mctypes\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbyref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2894\u001b[39m \u001b[43m            \u001b[49m\u001b[43mctypes\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbyref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdims\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2895\u001b[39m \u001b[43m            \u001b[49m\u001b[43mctypes\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbyref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2896\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2897\u001b[39m     )\n\u001b[32m   2898\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _prediction_output(shape, dims, preds, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m   2899\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, scipy.sparse.csr_matrix):\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Cell: estrai top-100 features con SHAP (robusta)\n",
    "import shap\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Candidate features: tutte le colonne numeriche tranne id/target\n",
    "candidate_features = [c for c in train_df.columns if c not in ('battle_id','player_won') and train_df[c].dtype != 'object']\n",
    "X_all = train_df[candidate_features].astype(float).fillna(0.0)\n",
    "y_all = train_df['player_won'].astype(int).values\n",
    "\n",
    "# Per velocità, campiona fino a N per training + spiegazioni\n",
    "RND = np.random.RandomState(42)\n",
    "N_SAMPLE = min(2000, len(X_all))\n",
    "sample_idx = RND.choice(len(X_all), size=N_SAMPLE, replace=False)\n",
    "X_sample = X_all.iloc[sample_idx]\n",
    "y_sample = y_all[sample_idx]\n",
    "\n",
    "# Allena un modello leggero (veloce) usato solo per SHAP\n",
    "shap_clf = XGBClassifier(n_estimators=300, max_depth=3, learning_rate=0.05,\n",
    "                         use_label_encoder=False, eval_metric='logloss',\n",
    "                         random_state=42, n_jobs=4, tree_method='hist')\n",
    "shap_clf.fit(X_sample, y_sample)\n",
    "\n",
    "# Calcola SHAP con fallback robusto\n",
    "shap_arr = None\n",
    "explainer = None\n",
    "ev = None\n",
    "\n",
    "def _to_shap_array(shap_vals, n_samples, n_features):\n",
    "    arr = np.array(shap_vals)\n",
    "    if arr.ndim == 3:\n",
    "        # Possibili layout: (n_samples, n_classes, n_features) o (n_classes, n_samples, n_features)\n",
    "        if arr.shape[0] == n_samples and arr.shape[2] == n_features:\n",
    "            class_idx = 1 if arr.shape[1] > 1 else 0\n",
    "            return arr[:, class_idx, :]\n",
    "        if arr.shape[1] == n_samples and arr.shape[2] == n_features:\n",
    "            class_idx = 1 if arr.shape[0] > 1 else 0\n",
    "            return arr[class_idx, :, :]\n",
    "        if arr.shape[0] == n_samples and arr.shape[1] == n_features:\n",
    "            class_idx = 1 if arr.shape[2] > 1 else 0\n",
    "            return arr[:, :, class_idx]\n",
    "        raise RuntimeError(f\"Formato 3D SHAP non riconosciuto: {arr.shape}\")\n",
    "    elif arr.ndim == 2:\n",
    "        # Atteso (n_samples, n_features) oppure trasposto\n",
    "        if arr.shape[0] == n_samples and arr.shape[1] == n_features:\n",
    "            return arr\n",
    "        if arr.shape[1] == n_samples and arr.shape[0] == n_features:\n",
    "            return arr.T\n",
    "        raise RuntimeError(f\"Formato 2D SHAP non riconosciuto: {arr.shape}\")\n",
    "    else:\n",
    "        raise RuntimeError(f\"Formato SHAP inatteso: {arr.shape}\")\n",
    "\n",
    "n_samples = X_sample.shape[0]\n",
    "n_features = X_sample.shape[1]\n",
    "\n",
    "try:\n",
    "    explainer = shap.TreeExplainer(shap_clf)\n",
    "    shap_vals = explainer.shap_values(X_sample)\n",
    "    shap_arr = _to_shap_array(shap_vals, n_samples, n_features)\n",
    "except Exception as e:\n",
    "    print(\"TreeExplainer non funzionante o formato non previsto, fallback a shap.Explainer:\", e)\n",
    "    explainer = shap.Explainer(shap_clf.predict_proba, X_sample)\n",
    "    ev = explainer(X_sample)\n",
    "    vals = ev.values\n",
    "    shap_arr = _to_shap_array(vals, n_samples, n_features)\n",
    "\n",
    "# Debug shapes se serve\n",
    "print(\"shap_arr.shape:\", getattr(shap_arr, \"shape\", None), \"X_sample.shape:\", getattr(X_sample, \"shape\", None), \"X_all.shape:\", getattr(X_all, \"shape\", None))\n",
    "\n",
    "# Allinea nomi feature e calcola importanza\n",
    "shap_imp = np.abs(shap_arr).mean(axis=0)\n",
    "\n",
    "# Recupera nomi feature nell'ordine usato per SHAP\n",
    "feat_names = None\n",
    "try:\n",
    "    if explainer is not None and getattr(explainer, 'feature_names', None) is not None:\n",
    "        feat_names = list(explainer.feature_names)\n",
    "except Exception:\n",
    "    feat_names = None\n",
    "\n",
    "if feat_names is None and ev is not None:\n",
    "    try:\n",
    "        if getattr(ev, 'feature_names', None) is not None:\n",
    "            feat_names = list(ev.feature_names)\n",
    "    except Exception:\n",
    "        feat_names = None\n",
    "\n",
    "if feat_names is None:\n",
    "    try:\n",
    "        feat_names = list(X_sample.columns)\n",
    "    except Exception:\n",
    "        feat_names = None\n",
    "\n",
    "if feat_names is None:\n",
    "    feat_names = list(X_all.columns)\n",
    "\n",
    "if len(shap_imp) != len(feat_names):\n",
    "    raise ValueError(f\"Incoerenza lunghezze: len(shap_imp)={len(shap_imp)}, len(feat_names)={len(feat_names)}. Controlla X_sample/colonne.\")\n",
    "\n",
    "imp_df = pd.DataFrame({'feature': feat_names, 'shap_mean_abs': shap_imp})\n",
    "imp_df = imp_df.sort_values('shap_mean_abs', ascending=False).reset_index(drop=True)\n",
    "\n",
    "TOP_K = min(100, len(imp_df))\n",
    "top100 = imp_df['feature'].iloc[:TOP_K].tolist()\n",
    "\n",
    "imp_df.to_csv('shap_feature_importances_all.csv', index=False)\n",
    "pd.DataFrame({'feature': top100}).to_csv('top100_shap_features.csv', index=False)\n",
    "\n",
    "print(f\"Top-{TOP_K} features SHAP salvate in top100_shap_features.csv\")\n",
    "print(imp_df.head(10))\n",
    "\n",
    "SHAP_TOP100 = top100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf62000",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2cede4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num FEATURES numeriche rilevate (ALL): 229\n",
      "Num FEATURES effettive usate (FEATURES): 229\n",
      "Num TOP100 caricate: 0\n",
      "Preprocessing (no transformers) completato.\n",
      "train_val size: 8000 holdout size: 2000\n",
      "Preprocessed feature count: 229\n"
     ]
    }
   ],
   "source": [
    "# ====== Preprocessing (senza transformer sklearn) =========\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# base exclusions\n",
    "exclude_cols = ['battle_id', 'player_won']\n",
    "string_cols = train_df.select_dtypes(include=['object']).columns.tolist()\n",
    "exclude_cols.extend(string_cols)\n",
    "\n",
    "# tutte le colonne numeriche candidate\n",
    "ALL_NUMERIC_FEATURES = [c for c in train_df.columns if c not in exclude_cols]\n",
    "\n",
    "# flag per usare top features se necessario\n",
    "use_top_features = False\n",
    "\n",
    "# carica TOP100 se presente (comportamento invariato)\n",
    "top100_path = r'features_top100_used.csv'\n",
    "try:\n",
    "    top100_df = pd.read_csv(top100_path)\n",
    "    TOP100 = [str(x).strip() for x in top100_df['feature'].tolist()]\n",
    "except Exception:\n",
    "    TOP100 = []\n",
    "\n",
    "if use_top_features and TOP100:\n",
    "    FEATURES = [f for f in TOP100 if f in ALL_NUMERIC_FEATURES]\n",
    "else:\n",
    "    FEATURES = ALL_NUMERIC_FEATURES\n",
    "\n",
    "print(f'Num FEATURES numeriche rilevate (ALL): {len(ALL_NUMERIC_FEATURES)}')\n",
    "print(f'Num FEATURES effettive usate (FEATURES): {len(FEATURES)}')\n",
    "print(f'Num TOP100 caricate: {len(TOP100)}')\n",
    "\n",
    "# costruisco DataFrame numerico raw\n",
    "num_df = train_df[FEATURES].astype(float).replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "# Imputazione semplice: usiamo la mediana per ogni feature calcolata sul train\n",
    "medians = num_df.median()\n",
    "train_imputed = num_df.fillna(medians)\n",
    "\n",
    "# NON eseguo alcuno scaling: lascio i valori nella loro scala naturale\n",
    "train_preproc_df = train_imputed.copy()\n",
    "\n",
    "# target\n",
    "y = train_df['player_won'].astype(int).values\n",
    "\n",
    "# split holdout (20%) - mantengo comportamento originale\n",
    "X = train_preproc_df.values\n",
    "X_train_val, X_holdout, y_train_val, y_holdout, idx_train_val, idx_holdout = train_test_split(\n",
    "    X, y, train_df.index.values, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "print('Preprocessing (no transformers) completato.')\n",
    "print('train_val size:', X_train_val.shape[0], 'holdout size:', X_holdout.shape[0])\n",
    "print('Preprocessed feature count:', len(FEATURES))\n",
    "\n",
    "# Allinea e imputa test_df usando le mediane del train (coerente con l'imputazione sopra)\n",
    "test_aligned = test_df.reindex(columns=FEATURES, fill_value=np.nan).astype(float).replace([np.inf, -np.inf], np.nan)\n",
    "test_imputed = test_aligned.fillna(medians)\n",
    "test_preproc_df = pd.DataFrame(test_imputed.values, columns=FEATURES, index=test_df.index)\n",
    "\n",
    "# Variabili pronte per le celle successive:\n",
    "# FEATURES, X, y, X_train_val, X_holdout, y_train_val, y_holdout, test_preproc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c65ea088",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature con |mean| > 0.001: 148\n",
      "tl_p1_inflicted_frz_count        0.009906\n",
      "tl_p1_inflicted_frz_rate         0.009906\n",
      "p2_lead_base_spe                 0.009240\n",
      "tl_p1_move_type_ghost_count      0.009158\n",
      "tl_inflicted_frz_rate_diff       0.008786\n",
      "tl_inflicted_frz_diff            0.008786\n",
      "tl_p1_damage_last5               0.008404\n",
      "tl_p1_move_type_rock_count       0.007686\n",
      "tl_p1_defensive_endurance        0.007493\n",
      "p1_base_hp_std                   0.007062\n",
      "p2_lead_type_ground              0.007000\n",
      "p2_lead_type_rock                0.007000\n",
      "p1_base_hp_max                   0.006676\n",
      "team_hp_sum_minus_p2lead_hp      0.006582\n",
      "tl_p1_move_type_ice_count        0.006506\n",
      "tl_move_type_ice_count_diff      0.006433\n",
      "tl_p2_move_type_rock_count       0.006360\n",
      "p1_type_normal_count             0.006317\n",
      "tl_move_type_ghost_count_diff    0.006228\n",
      "lead_diff_base_hp                0.006203\n",
      "dtype: float64\n",
      "\n",
      "Feature con |std-1| > 0.05: 51\n",
      "p1_n_unique_names                     0.0\n",
      "p1_ability_water_absorb_count         0.0\n",
      "p2_lead_n_unique_types                0.0\n",
      "p1_total_immunity_abilities           0.0\n",
      "p1_total_stat_drop_abilities          0.0\n",
      "p1_ability_volt_absorb_count          0.0\n",
      "p1_ability_levitate_count             0.0\n",
      "p1_ability_flash_fire_count           0.0\n",
      "p1_ability_drizzle_count              0.0\n",
      "p1_ability_drought_count              0.0\n",
      "p1_ability_intimidate_count           0.0\n",
      "p2_lead_ability_volt_absorb_count     0.0\n",
      "p1_ability_sand_stream_count          0.0\n",
      "p2_lead_ability_levitate_count        0.0\n",
      "p2_lead_ability_drizzle_count         0.0\n",
      "p2_lead_ability_drought_count         0.0\n",
      "p2_lead_ability_intimidate_count      0.0\n",
      "p2_lead_ability_flash_fire_count      0.0\n",
      "p2_lead_ability_water_absorb_count    0.0\n",
      "tl_turns_count                        0.0\n",
      "dtype: float64\n",
      "\n",
      "Feature costanti (std==0): 45\n",
      "['p1_n_unique_names', 'p2_lead_n_unique_types', 'p1_ability_levitate_count', 'p1_ability_volt_absorb_count', 'p1_ability_water_absorb_count', 'p1_ability_flash_fire_count', 'p1_ability_intimidate_count', 'p1_ability_drought_count', 'p1_ability_drizzle_count', 'p1_ability_sand_stream_count', 'p1_total_immunity_abilities', 'p1_total_stat_drop_abilities', 'p2_lead_ability_levitate_count', 'p2_lead_ability_volt_absorb_count', 'p2_lead_ability_water_absorb_count', 'p2_lead_ability_flash_fire_count', 'p2_lead_ability_intimidate_count', 'p2_lead_ability_drought_count', 'p2_lead_ability_drizzle_count', 'p2_lead_ability_sand_stream_count', 'p2_lead_total_immunity_abilities', 'p2_lead_total_stat_drop_abilities', 'p1_intimidate_vs_lead', 'tl_p1_fainted', 'tl_p2_fainted', 'tl_p1_fainted_rate', 'tl_p2_fainted_rate', 'fainted_diff', 'tl_p1_comeback_kos', 'tl_p2_comeback_kos', 'tl_comeback_kos_diff', 'tl_p1_move_type_dragon_count', 'tl_p2_move_type_dragon_count', 'tl_move_type_dragon_count_diff', 'tl_p1_move_type_bug_count', 'tl_p2_move_type_bug_count', 'tl_move_type_bug_count_diff', 'tl_turns_count', 'tl_first_ko_turn_p1_inflicted', 'tl_first_ko_turn_p1_taken', 'tl_first_ko_turn_diff', 'tl_kos_early_p1', 'tl_kos_late_p1', 'tl_kos_early_p2', 'tl_kos_late_p2']\n",
      "\\nCSV salvati: features_bad_mean.csv, features_bad_std.csv, features_constant.csv\n",
      "Nuovo numero FEATURES dopo rimozione costanti: 184\n"
     ]
    }
   ],
   "source": [
    "# Lista feature problematiche e rimozione opzionale delle costanti\n",
    "mean_tol = 1e-3\n",
    "std_tol = 0.05\n",
    "\n",
    "# df_Xtrainval, FEATURES devono esistere (calcolati nella cella di verifica)\n",
    "means = df_Xtrainval.mean()\n",
    "stds = df_Xtrainval.std(ddof=0)\n",
    "\n",
    "bad_mean = means[means.abs() > mean_tol].sort_values(ascending=False)\n",
    "bad_std = stds[(stds - 1.0).abs() > std_tol].sort_values(key=lambda s: (s-1.0).abs(), ascending=False)\n",
    "constant_feats = stds[stds == 0.0].index.tolist()\n",
    "\n",
    "print(f\"Feature con |mean| > {mean_tol}: {len(bad_mean)}\")\n",
    "print(bad_mean.head(20))\n",
    "print(\"\\nFeature con |std-1| > {0}: {1}\".format(std_tol, len(bad_std)))\n",
    "print(bad_std.head(20))\n",
    "print(f\"\\nFeature costanti (std==0): {len(constant_feats)}\")\n",
    "if constant_feats:\n",
    "    print(constant_feats[:100])\n",
    "\n",
    "# salva risultati per ispezione\n",
    "pd.DataFrame({'feature': bad_mean.index, 'mean': bad_mean.values}).to_csv('features_bad_mean.csv', index=False)\n",
    "pd.DataFrame({'feature': bad_std.index, 'std': bad_std.values}).to_csv('features_bad_std.csv', index=False)\n",
    "pd.DataFrame({'feature': constant_feats}).to_csv('features_constant.csv', index=False)\n",
    "print('\\\\nCSV salvati: features_bad_mean.csv, features_bad_std.csv, features_constant.csv')\n",
    "\n",
    "# Se vuoi rimuovere le costanti da FEATURES (decommenta)\n",
    "FEATURES = [f for f in FEATURES if f not in constant_feats]\n",
    "print(f\"Nuovo numero FEATURES dopo rimozione costanti: {len(FEATURES)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "65bec73c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max absolute difference between X_train_val reconstruction and train_preproc_df: 0.0\n",
      "Top 10 columns by mean absolute diff:\n",
      " p1_base_hp_sum      0.0\n",
      "p1_base_hp_mean     0.0\n",
      "p1_base_hp_max      0.0\n",
      "p1_base_hp_min      0.0\n",
      "p1_base_hp_std      0.0\n",
      "p1_base_atk_sum     0.0\n",
      "p1_base_atk_mean    0.0\n",
      "p1_base_atk_max     0.0\n",
      "p1_base_atk_min     0.0\n",
      "p1_base_atk_std     0.0\n",
      "dtype: float64\n",
      "\n",
      "Summary means (train_val): mean of feature-means = 2.390e-04, max abs mean = 1.083e-02\n",
      "Summary stds (train_val): mean of feature-stds = 0.803001, min std = 0.000000, max std = 1.117908\n",
      "\n",
      "Features with |mean| > 1e-06: 184/229\n",
      "Features with |std-1| > 0.001: 203/229\n",
      "\n",
      "Scaler mean (first 10): [678.7459 113.1243 233.6616  57.248   61.1826 466.2661  77.711  115.6441\n",
      "  12.6434  34.5575]\n",
      "Scaler scale (first 10): [80.4286 13.4048 39.9224  4.8416 14.0173 42.7095  7.1183  9.8181 18.3806\n",
      "  6.3772]\n",
      "Imputer statistics (first 10): [700.     116.6667 250.      55.      65.807  455.      75.8333 110.\n",
      "   5.      35.9108]\n",
      "\n",
      "Test preproc: feature-means (sample): [-0.026  -0.026  -0.0218 -0.0129 -0.0227  0.0038  0.0038 -0.0071  0.0229\n",
      " -0.0317]\n",
      "Test preproc: feature-stds  (sample): [1.0138 1.0138 1.0185 0.9662 1.0174 1.0075 1.0075 1.0004 1.0189 1.0114]\n"
     ]
    }
   ],
   "source": [
    "# Verifica coerenza normalizzazione\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Ricostruisco DataFrame a partire da X_train_val usando gli indici originali\n",
    "df_Xtrainval = pd.DataFrame(X_train_val, columns=FEATURES, index=idx_train_val)\n",
    "\n",
    "# 1) Confronto diretto con train_preproc_df (dove lo scaler è stato fit)\n",
    "diff_df = df_Xtrainval.loc[idx_train_val].subtract(train_preproc_df.loc[idx_train_val])\n",
    "max_abs_diff = diff_df.abs().max().max()\n",
    "cols_largest = diff_df.abs().mean().sort_values(ascending=False).head(10)\n",
    "\n",
    "print(\"Max absolute difference between X_train_val reconstruction and train_preproc_df:\", max_abs_diff)\n",
    "print(\"Top 10 columns by mean absolute diff:\\n\", cols_largest)\n",
    "\n",
    "# 2) Statistiche per-feature su X_train_val: mean ~ 0, std ~ 1\n",
    "means = df_Xtrainval.mean()\n",
    "stds = df_Xtrainval.std(ddof=0)\n",
    "\n",
    "print(\"\\nSummary means (train_val): mean of feature-means = {:.3e}, max abs mean = {:.3e}\".format(means.mean(), np.max(np.abs(means.values))))\n",
    "print(\"Summary stds (train_val): mean of feature-stds = {:.6f}, min std = {:.6f}, max std = {:.6f}\".format(stds.mean(), stds.min(), stds.max()))\n",
    "\n",
    "# 3) Conta feature fuori tolleranza\n",
    "mean_tol = 1e-6\n",
    "std_tol = 1e-3\n",
    "n_mean_off = (np.abs(means) > mean_tol).sum()\n",
    "n_std_off = (np.abs(stds - 1.0) > std_tol).sum()\n",
    "print(f\"\\nFeatures with |mean| > {mean_tol}: {n_mean_off}/{len(FEATURES)}\")\n",
    "print(f\"Features with |std-1| > {std_tol}: {n_std_off}/{len(FEATURES)}\")\n",
    "\n",
    "# 4) Controlla gli attributi dello scaler/imputer (fitted on full train)\n",
    "try:\n",
    "    print('\\nScaler mean (first 10):', np.round(scaler.mean_[:10], 4))\n",
    "    print('Scaler scale (first 10):', np.round(scaler.scale_[:10], 4))\n",
    "except Exception as e:\n",
    "    print('Scaler attributes not available:', e)\n",
    "\n",
    "try:\n",
    "    print('Imputer statistics (first 10):', np.round(imputer.statistics_[:10],4))\n",
    "except Exception as e:\n",
    "    print('Imputer attributes not available:', e)\n",
    "\n",
    "# 5) Verifica test_preproc_df (trasformato con gli stessi imputer/scaler)\n",
    "print(\"\\nTest preproc: feature-means (sample):\", np.round(test_preproc_df.mean().iloc[:10].values,4))\n",
    "print(\"Test preproc: feature-stds  (sample):\", np.round(test_preproc_df.std(ddof=0).iloc[:10].values,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb05f0e",
   "metadata": {},
   "source": [
    "# Hyperparameter serch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "38364108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Time-boxed GridSearchCV (<= ~2 ore) ===\n",
      "scale_pos_weight auto≈1.00 -> grid=[1.0, 1.25]\n",
      "Warmup per stimare t_fit...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 53\u001b[39m\n\u001b[32m     43\u001b[39m t0 = time.time()\n\u001b[32m     44\u001b[39m gs_warm = GridSearchCV(\n\u001b[32m     45\u001b[39m     base_clf,\n\u001b[32m     46\u001b[39m     param_grid=[{k:[v] \u001b[38;5;28;01mfor\u001b[39;00m k,v \u001b[38;5;129;01min\u001b[39;00m d.items()} \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m warm_params],\n\u001b[32m   (...)\u001b[39m\u001b[32m     51\u001b[39m     refit=\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m     52\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m \u001b[43mgs_warm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_val\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     54\u001b[39m elapsed_warm = time.time() - t0\n\u001b[32m     55\u001b[39m fits_warm = \u001b[38;5;28mlen\u001b[39m(warm_params) * cv_inner.get_n_splits()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1365\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1358\u001b[39m     estimator._validate_params()\n\u001b[32m   1360\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1361\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1362\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1363\u001b[39m     )\n\u001b[32m   1364\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1365\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\model_selection\\_search.py:1051\u001b[39m, in \u001b[36mBaseSearchCV.fit\u001b[39m\u001b[34m(self, X, y, **params)\u001b[39m\n\u001b[32m   1045\u001b[39m     results = \u001b[38;5;28mself\u001b[39m._format_results(\n\u001b[32m   1046\u001b[39m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[32m   1047\u001b[39m     )\n\u001b[32m   1049\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[32m-> \u001b[39m\u001b[32m1051\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1053\u001b[39m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[32m   1054\u001b[39m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[32m   1055\u001b[39m first_test_score = all_out[\u001b[32m0\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mtest_scores\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\model_selection\\_search.py:1605\u001b[39m, in \u001b[36mGridSearchCV._run_search\u001b[39m\u001b[34m(self, evaluate_candidates)\u001b[39m\n\u001b[32m   1603\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[32m   1604\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1605\u001b[39m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\model_selection\\_search.py:997\u001b[39m, in \u001b[36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[39m\u001b[34m(candidate_params, cv, more_results)\u001b[39m\n\u001b[32m    989\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.verbose > \u001b[32m0\u001b[39m:\n\u001b[32m    990\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[32m    991\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[33m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[33m candidates,\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    992\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m fits\u001b[39m\u001b[33m\"\u001b[39m.format(\n\u001b[32m    993\u001b[39m             n_splits, n_candidates, n_candidates * n_splits\n\u001b[32m    994\u001b[39m         )\n\u001b[32m    995\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m997\u001b[39m out = \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    998\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    999\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1000\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1001\u001b[39m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1002\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1003\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1004\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1005\u001b[39m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1006\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1007\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1008\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1009\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1010\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1011\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mrouted_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplitter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1012\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1013\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1015\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) < \u001b[32m1\u001b[39m:\n\u001b[32m   1016\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1017\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mNo fits were performed. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1018\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mWas the CV iterator empty? \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1019\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mWere there no candidates?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1020\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\parallel.py:82\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m     73\u001b[39m warning_filters = warnings.filters\n\u001b[32m     74\u001b[39m iterable_with_config_and_warning_filters = (\n\u001b[32m     75\u001b[39m     (\n\u001b[32m     76\u001b[39m         _with_config_and_warning_filters(delayed_func, config, warning_filters),\n\u001b[32m   (...)\u001b[39m\u001b[32m     80\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[32m     81\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config_and_warning_filters\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\joblib\\parallel.py:2072\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   2066\u001b[39m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[32m   2067\u001b[39m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[32m   2068\u001b[39m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[32m   2069\u001b[39m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[32m   2070\u001b[39m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[32m-> \u001b[39m\u001b[32m2072\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\joblib\\parallel.py:1682\u001b[39m, in \u001b[36mParallel._get_outputs\u001b[39m\u001b[34m(self, iterator, pre_dispatch)\u001b[39m\n\u001b[32m   1679\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[32m   1681\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backend.retrieval_context():\n\u001b[32m-> \u001b[39m\u001b[32m1682\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m._retrieve()\n\u001b[32m   1684\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[32m   1685\u001b[39m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[32m   1686\u001b[39m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[32m   1687\u001b[39m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[32m   1688\u001b[39m     \u001b[38;5;28mself\u001b[39m._exception = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\joblib\\parallel.py:1800\u001b[39m, in \u001b[36mParallel._retrieve\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1789\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_ordered:\n\u001b[32m   1790\u001b[39m     \u001b[38;5;66;03m# Case ordered: wait for completion (or error) of the next job\u001b[39;00m\n\u001b[32m   1791\u001b[39m     \u001b[38;5;66;03m# that have been dispatched and not retrieved yet. If no job\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1795\u001b[39m     \u001b[38;5;66;03m# control only have to be done on the amount of time the next\u001b[39;00m\n\u001b[32m   1796\u001b[39m     \u001b[38;5;66;03m# dispatched job is pending.\u001b[39;00m\n\u001b[32m   1797\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (nb_jobs == \u001b[32m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   1798\u001b[39m         \u001b[38;5;28mself\u001b[39m._jobs[\u001b[32m0\u001b[39m].get_status(timeout=\u001b[38;5;28mself\u001b[39m.timeout) == TASK_PENDING\n\u001b[32m   1799\u001b[39m     ):\n\u001b[32m-> \u001b[39m\u001b[32m1800\u001b[39m         time.sleep(\u001b[32m0.01\u001b[39m)\n\u001b[32m   1801\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m   1803\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m nb_jobs == \u001b[32m0\u001b[39m:\n\u001b[32m   1804\u001b[39m     \u001b[38;5;66;03m# Case unordered: jobs are added to the list of jobs to\u001b[39;00m\n\u001b[32m   1805\u001b[39m     \u001b[38;5;66;03m# retrieve `self._jobs` only once completed or in error, which\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1811\u001b[39m     \u001b[38;5;66;03m# timeouts before any other dispatched job has completed and\u001b[39;00m\n\u001b[32m   1812\u001b[39m     \u001b[38;5;66;03m# been added to `self._jobs` to be retrieved.\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# === Grid Search time-boxed (<= ~2 ore) ===\n",
    "print(\"=== Time-boxed GridSearchCV (<= ~2 ore) ===\")\n",
    "import time, os\n",
    "import numpy as np, pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV, ParameterGrid\n",
    "try:\n",
    "    import joblib\n",
    "    CPU_COUNT = joblib.cpu_count()\n",
    "except Exception:\n",
    "    CPU_COUNT = os.cpu_count() or 4\n",
    "\n",
    "cv_inner = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Base estimator\n",
    "base_clf = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42, n_jobs=1, tree_method='hist')\n",
    "\n",
    "# Calcola uno scale_pos_weight automatico e cerca attorno ad esso\n",
    "pos_rate = float(y_train_val.mean())\n",
    "spw_auto = float((1.0 - pos_rate) / max(pos_rate, 1e-9))\n",
    "spw_grid = sorted({1.0, max(1.0, spw_auto*0.75), max(1.0, spw_auto), max(1.0, spw_auto*1.25)})\n",
    "print(f'scale_pos_weight auto≈{spw_auto:.2f} -> grid={spw_grid}')\n",
    "\n",
    "# Griglia COARSE (regolarizzata) — include scale_pos_weight\n",
    "grid_coarse = {\n",
    "    'n_estimators':      [300, 500, 700],\n",
    "    'max_depth':         [3, 4],\n",
    "    'min_child_weight':  [3, 5, 7],\n",
    "    'learning_rate':     [0.03, 0.05, 0.07],\n",
    "    'subsample':         [0.7, 0.8, 0.9],\n",
    "    'colsample_bytree':  [0.7, 0.8, 0.9],\n",
    "    'gamma':             [0.1, 0.2, 0.3],\n",
    "    'reg_alpha':         [0.05, 0.1, 0.2],\n",
    "    'reg_lambda':        [2.0, 3.0, 4.0],\n",
    "    'scale_pos_weight':  spw_grid\n",
    "}\n",
    "\n",
    "# Stima tempo per-fit (warmup) — CORRETTO param_grid\n",
    "warm_params = [\n",
    "    {'n_estimators': 500, 'max_depth': 3, 'min_child_weight': 5, 'learning_rate': 0.05, 'subsample': 0.8, 'colsample_bytree': 0.8, 'gamma': 0.2, 'reg_alpha': 0.1, 'reg_lambda': 3.0, 'scale_pos_weight': max(1.0, spw_auto)},\n",
    "    {'n_estimators': 700, 'max_depth': 4, 'min_child_weight': 5, 'learning_rate': 0.03, 'subsample': 0.8, 'colsample_bytree': 0.8, 'gamma': 0.2, 'reg_alpha': 0.1, 'reg_lambda': 3.0, 'scale_pos_weight': max(1.0, spw_auto*1.25)}\n",
    "]\n",
    "print(\"Warmup per stimare t_fit...\")\n",
    "t0 = time.time()\n",
    "gs_warm = GridSearchCV(\n",
    "    base_clf,\n",
    "    param_grid=[{k:[v] for k,v in d.items()} for d in warm_params],\n",
    "    scoring='balanced_accuracy',\n",
    "    cv=cv_inner,\n",
    "    n_jobs=-1,\n",
    "    verbose=0,\n",
    "    refit=False\n",
    ")\n",
    "gs_warm.fit(X_train_val, y_train_val)\n",
    "elapsed_warm = time.time() - t0\n",
    "fits_warm = len(warm_params) * cv_inner.get_n_splits()\n",
    "t_fit_per_fold = max(0.01, elapsed_warm / fits_warm)\n",
    "print(f\"Warmup: {elapsed_warm:.2f}s per {fits_warm} fit -> ~{t_fit_per_fold:.3f}s/fit\")\n",
    "\n",
    "# Budget totale ~2 ore con margine sicurezza\n",
    "TARGET_SECONDS = int(2*3600*0.9)\n",
    "speedup = max(1, min(CPU_COUNT, cv_inner.get_n_splits()))\n",
    "max_combos = int((TARGET_SECONDS * speedup) / (t_fit_per_fold * cv_inner.get_n_splits()))\n",
    "max_combos = int(max(48, min(max_combos, 2000)))\n",
    "print(f\"CPU={CPU_COUNT}, speedup~{speedup}, max_combos≈{max_combos}\")\n",
    "\n",
    "# Costruisci tutte le combinazioni e campiona fino a max_combos\n",
    "all_points = list(ParameterGrid(grid_coarse))\n",
    "total = len(all_points)\n",
    "print(f\"Candidate totali nella griglia: {total}\")\n",
    "rng = np.random.default_rng(42)\n",
    "if total > max_combos:\n",
    "    idx = rng.choice(total, size=max_combos, replace=False)\n",
    "    sampled = [all_points[i] for i in idx]\n",
    "else:\n",
    "    sampled = all_points\n",
    "print(f\"Config selezionate: {len(sampled)}\")\n",
    "\n",
    "# Converte in lista di 'micro-grid' (1 punto ciascuno) — CORRETTO\n",
    "param_grid_list = [{k:[v] for k,v in pt.items()} for pt in sampled]\n",
    "\n",
    "print(\"Esecuzione GridSearch time-boxed...\")\n",
    "t1 = time.time()\n",
    "gs = GridSearchCV(\n",
    "    estimator=base_clf,\n",
    "    param_grid=param_grid_list,\n",
    "    scoring='balanced_accuracy',\n",
    "    cv=cv_inner,\n",
    "    n_jobs=-1,\n",
    "    verbose=2,\n",
    "    refit=True\n",
    ")\n",
    "gs.fit(X_train_val, y_train_val)\n",
    "elapsed = time.time() - t1\n",
    "\n",
    "results_df = pd.DataFrame(gs.cv_results_).sort_values('rank_test_score')\n",
    "csv_path = 'hp_search_results_timeboxed_grid.csv'\n",
    "results_df.to_csv(csv_path, index=False)\n",
    "best_params = gs.best_params_\n",
    "\n",
    "print(f\"\\n✅ Salvato {csv_path} ({len(results_df)} righe)\")\n",
    "print(\"Migliori iperparametri:\")\n",
    "for k, v in best_params.items():\n",
    "    print(f\"  {k}: {v}\")\n",
    "print(f\"Best CV (balanced_accuracy): {gs.best_score_:.4f}\")\n",
    "print(f\"Tempo GridSearch: {elapsed/60:.1f} min (budget ~{TARGET_SECONDS/60:.0f} min)\")\n",
    "print(\"Ora puoi usare 'best_params' nelle celle successive.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "239a614a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-03 14:33:24,414] A new study created in memory with name: no-name-1436204c-bbb6-468e-bbbe-5536bc1a4d17\n",
      "[I 2025-11-03 14:33:35,941] Trial 0 finished with value: 0.80975 and parameters: {'n_estimators': 600, 'learning_rate': 0.08927180304353628, 'max_depth': 5, 'min_child_weight': 7, 'gamma': 0.12481491235394922, 'subsample': 0.6467983561008608, 'colsample_bytree': 0.6174250836504598, 'colsample_bylevel': 0.9464704583099741, 'colsample_bynode': 0.8404460046972835, 'reg_alpha': 0.7080725777960455, 'reg_lambda': 0.5318033256270142, 'max_delta_step': 2, 'scale_pos_weight': 1.4994655844802531}. Best is trial 0 with value: 0.80975.\n",
      "[I 2025-11-03 14:33:44,280] Trial 1 finished with value: 0.8067499999999999 and parameters: {'n_estimators': 500, 'learning_rate': 0.015199348301309814, 'max_depth': 3, 'min_child_weight': 4, 'gamma': 0.4198051453057903, 'subsample': 0.7295835055926347, 'colsample_bytree': 0.6873687420594126, 'colsample_bylevel': 0.8447411578889518, 'colsample_bynode': 0.6557975442608167, 'reg_alpha': 0.29214464853521815, 'reg_lambda': 1.498365454855079, 'max_delta_step': 1, 'scale_pos_weight': 1.4711055768358081}. Best is trial 0 with value: 0.80975.\n",
      "[I 2025-11-03 14:33:53,813] Trial 2 finished with value: 0.819 and parameters: {'n_estimators': 400, 'learning_rate': 0.032676417657817626, 'max_depth': 5, 'min_child_weight': 2, 'gamma': 0.48603588152115074, 'subsample': 0.6511572371061874, 'colsample_bytree': 0.6195154778955838, 'colsample_bylevel': 0.9795542149013333, 'colsample_bynode': 0.9862528132298237, 'reg_alpha': 0.8083973481164611, 'reg_lambda': 1.2453219846912194, 'max_delta_step': 0, 'scale_pos_weight': 1.4105398159072942}. Best is trial 2 with value: 0.819.\n",
      "[I 2025-11-03 14:34:06,176] Trial 3 finished with value: 0.8145 and parameters: {'n_estimators': 700, 'learning_rate': 0.01324458134009936, 'max_depth': 4, 'min_child_weight': 2, 'gamma': 0.7274563216630257, 'subsample': 0.677633994480005, 'colsample_bytree': 0.7987566853061946, 'colsample_bylevel': 0.7246844304357644, 'colsample_bynode': 0.8080272084711243, 'reg_alpha': 0.5467102793432796, 'reg_lambda': 0.869903735564586, 'max_delta_step': 2, 'scale_pos_weight': 1.4650796940166688}. Best is trial 2 with value: 0.819.\n",
      "[I 2025-11-03 14:34:27,519] Trial 4 finished with value: 0.8098749999999999 and parameters: {'n_estimators': 1200, 'learning_rate': 0.07849235338159358, 'max_depth': 5, 'min_child_weight': 10, 'gamma': 0.0707940016415356, 'subsample': 0.6587948587257435, 'colsample_bytree': 0.6135681866731614, 'colsample_bylevel': 0.7301321323053057, 'colsample_bynode': 0.7554709158757928, 'reg_alpha': 0.2713490317738959, 'reg_lambda': 5.986629236379358, 'max_delta_step': 1, 'scale_pos_weight': 1.1685607058124285}. Best is trial 2 with value: 0.819.\n",
      "[I 2025-11-03 14:34:46,497] Trial 5 finished with value: 0.8195 and parameters: {'n_estimators': 800, 'learning_rate': 0.013833249975219963, 'max_depth': 6, 'min_child_weight': 2, 'gamma': 0.7895095492804138, 'subsample': 0.8316734307889972, 'colsample_bytree': 0.6596147044602517, 'colsample_bylevel': 0.602208846849441, 'colsample_bynode': 0.9261845713819337, 'reg_alpha': 0.7068573438476171, 'reg_lambda': 4.440482849384359, 'max_delta_step': 2, 'scale_pos_weight': 1.0444267910404543}. Best is trial 5 with value: 0.8195.\n",
      "[I 2025-11-03 14:35:00,991] Trial 6 finished with value: 0.8163750000000001 and parameters: {'n_estimators': 600, 'learning_rate': 0.013057771348997228, 'max_depth': 6, 'min_child_weight': 7, 'gamma': 0.26471841988211936, 'subsample': 0.6190675050858071, 'colsample_bytree': 0.6932946965146987, 'colsample_bylevel': 0.7300733288106989, 'colsample_bynode': 0.8918424713352255, 'reg_alpha': 0.6375574713552131, 'reg_lambda': 7.132805718188966, 'max_delta_step': 1, 'scale_pos_weight': 1.071756547562981}. Best is trial 5 with value: 0.8195.\n",
      "[I 2025-11-03 14:35:05,337] Trial 7 pruned. \n",
      "[I 2025-11-03 14:35:10,348] Trial 8 pruned. \n",
      "[I 2025-11-03 14:35:12,619] Trial 9 pruned. \n",
      "[I 2025-11-03 14:35:34,737] Trial 10 finished with value: 0.8176249999999999 and parameters: {'n_estimators': 900, 'learning_rate': 0.024681591575651834, 'max_depth': 6, 'min_child_weight': 4, 'gamma': 0.5910772166667708, 'subsample': 0.857987729197811, 'colsample_bytree': 0.887931223783626, 'colsample_bylevel': 0.8430935157723661, 'colsample_bynode': 0.9134518484596068, 'reg_alpha': 0.9923626654603367, 'reg_lambda': 2.937880776788149, 'max_delta_step': 2, 'scale_pos_weight': 1.0141075205616947}. Best is trial 5 with value: 0.8195.\n",
      "[I 2025-11-03 14:35:44,518] Trial 11 finished with value: 0.8178750000000001 and parameters: {'n_estimators': 300, 'learning_rate': 0.03714902137788296, 'max_depth': 6, 'min_child_weight': 2, 'gamma': 0.5643417528433443, 'subsample': 0.8260052940673948, 'colsample_bytree': 0.7937569322109723, 'colsample_bylevel': 0.9881320861304522, 'colsample_bynode': 0.9997101278830601, 'reg_alpha': 0.9905560684491923, 'reg_lambda': 1.549178256500361, 'max_delta_step': 0, 'scale_pos_weight': 1.3561281139142216}. Best is trial 5 with value: 0.8195.\n",
      "[I 2025-11-03 14:36:05,432] Trial 12 finished with value: 0.8172499999999999 and parameters: {'n_estimators': 900, 'learning_rate': 0.03331741541404792, 'max_depth': 6, 'min_child_weight': 4, 'gamma': 0.7994899509132409, 'subsample': 0.7924450801299063, 'colsample_bytree': 0.6501923387962646, 'colsample_bylevel': 0.9184246529345085, 'colsample_bynode': 0.929759505859453, 'reg_alpha': 0.46140483948688765, 'reg_lambda': 1.5968070243951733, 'max_delta_step': 0, 'scale_pos_weight': 1.1111558436304507}. Best is trial 5 with value: 0.8195.\n",
      "[I 2025-11-03 14:36:19,417] Trial 13 finished with value: 0.8167500000000001 and parameters: {'n_estimators': 800, 'learning_rate': 0.022734654308688994, 'max_depth': 4, 'min_child_weight': 3, 'gamma': 0.5513246583878899, 'subsample': 0.865363320386671, 'colsample_bytree': 0.7492244392210476, 'colsample_bylevel': 0.7885688360480123, 'colsample_bynode': 0.8661433738769575, 'reg_alpha': 0.7896845175879643, 'reg_lambda': 0.8123538810061028, 'max_delta_step': 1, 'scale_pos_weight': 1.5876553550636467}. Best is trial 5 with value: 0.8195.\n",
      "[I 2025-11-03 14:36:21,629] Trial 14 pruned. \n",
      "[I 2025-11-03 14:36:25,101] Trial 15 pruned. \n",
      "[I 2025-11-03 14:36:27,410] Trial 16 pruned. \n",
      "[I 2025-11-03 14:36:53,407] Trial 17 finished with value: 0.82 and parameters: {'n_estimators': 1000, 'learning_rate': 0.01088079023686719, 'max_depth': 6, 'min_child_weight': 3, 'gamma': 0.2987693351375503, 'subsample': 0.8094289920016203, 'colsample_bytree': 0.7971724699162732, 'colsample_bylevel': 0.9975371140198833, 'colsample_bynode': 0.8528955957114865, 'reg_alpha': 0.6846114912437019, 'reg_lambda': 1.0529041877721552, 'max_delta_step': 1, 'scale_pos_weight': 1.000339970379654}. Best is trial 17 with value: 0.82.\n",
      "[I 2025-11-03 14:37:18,283] Trial 18 finished with value: 0.818875 and parameters: {'n_estimators': 1000, 'learning_rate': 0.0100980899624385, 'max_depth': 6, 'min_child_weight': 3, 'gamma': 0.2766828240433545, 'subsample': 0.823595003374941, 'colsample_bytree': 0.8016055618926528, 'colsample_bylevel': 0.8519774955331508, 'colsample_bynode': 0.8387777935834964, 'reg_alpha': 0.32334076412440416, 'reg_lambda': 1.0283430605938404, 'max_delta_step': 1, 'scale_pos_weight': 1.0142808992689059}. Best is trial 17 with value: 0.82.\n",
      "[I 2025-11-03 14:37:24,100] Trial 19 pruned. \n",
      "[I 2025-11-03 14:37:42,927] Trial 20 finished with value: 0.8188749999999999 and parameters: {'n_estimators': 800, 'learning_rate': 0.016309189638605445, 'max_depth': 6, 'min_child_weight': 5, 'gamma': 0.03536466782920139, 'subsample': 0.7613531684843053, 'colsample_bytree': 0.8359009162201633, 'colsample_bylevel': 0.7639493407814485, 'colsample_bynode': 0.8074076155976588, 'reg_alpha': 0.694618158022859, 'reg_lambda': 2.0936186328751267, 'max_delta_step': 1, 'scale_pos_weight': 1.1340388951820584}. Best is trial 17 with value: 0.82.\n",
      "[I 2025-11-03 14:38:02,858] Trial 21 finished with value: 0.818 and parameters: {'n_estimators': 900, 'learning_rate': 0.02771574042977272, 'max_depth': 5, 'min_child_weight': 2, 'gamma': 0.32770930957334254, 'subsample': 0.8451008687424497, 'colsample_bytree': 0.7672552954718422, 'colsample_bylevel': 0.9933273519651367, 'colsample_bynode': 0.9411434176003156, 'reg_alpha': 0.8752994353736931, 'reg_lambda': 1.001364207948549, 'max_delta_step': 0, 'scale_pos_weight': 1.001924275966548}. Best is trial 17 with value: 0.82.\n",
      "[I 2025-11-03 14:38:07,603] Trial 22 pruned. \n",
      "[I 2025-11-03 14:38:14,060] Trial 23 pruned. \n",
      "[I 2025-11-03 14:38:17,487] Trial 24 pruned. \n",
      "[I 2025-11-03 14:38:20,869] Trial 25 pruned. \n",
      "[I 2025-11-03 14:38:23,604] Trial 26 pruned. \n",
      "[I 2025-11-03 14:38:29,193] Trial 27 pruned. \n",
      "[I 2025-11-03 14:38:46,063] Trial 28 finished with value: 0.8188749999999999 and parameters: {'n_estimators': 700, 'learning_rate': 0.01534005119829623, 'max_depth': 5, 'min_child_weight': 3, 'gamma': 0.22313999322735723, 'subsample': 0.8388726802360771, 'colsample_bytree': 0.8849069710300087, 'colsample_bylevel': 0.996899367141751, 'colsample_bynode': 0.8711142581172723, 'reg_alpha': 0.5200813513679245, 'reg_lambda': 1.8534522894642398, 'max_delta_step': 2, 'scale_pos_weight': 1.063704467763091}. Best is trial 17 with value: 0.82.\n",
      "[I 2025-11-03 14:38:48,592] Trial 29 pruned. \n",
      "[I 2025-11-03 14:38:51,603] Trial 30 pruned. \n",
      "[I 2025-11-03 14:39:15,738] Trial 31 finished with value: 0.818375 and parameters: {'n_estimators': 1000, 'learning_rate': 0.010425118609161352, 'max_depth': 6, 'min_child_weight': 3, 'gamma': 0.2986364191592012, 'subsample': 0.8177233534977278, 'colsample_bytree': 0.7879685472147819, 'colsample_bylevel': 0.8671548959565435, 'colsample_bynode': 0.8437807574292961, 'reg_alpha': 0.3334593451311379, 'reg_lambda': 1.0192865911043716, 'max_delta_step': 1, 'scale_pos_weight': 1.0315286846575489}. Best is trial 17 with value: 0.82.\n",
      "[I 2025-11-03 14:39:21,038] Trial 32 pruned. \n",
      "[I 2025-11-03 14:39:44,137] Trial 33 finished with value: 0.82125 and parameters: {'n_estimators': 1000, 'learning_rate': 0.014435601114465301, 'max_depth': 6, 'min_child_weight': 3, 'gamma': 0.35819430111991774, 'subsample': 0.798405275011201, 'colsample_bytree': 0.8596632597516238, 'colsample_bylevel': 0.7611715508295013, 'colsample_bynode': 0.8237508107290339, 'reg_alpha': 0.3505920401894866, 'reg_lambda': 0.9594364941397278, 'max_delta_step': 1, 'scale_pos_weight': 1.0053242149308106}. Best is trial 33 with value: 0.82125.\n",
      "[I 2025-11-03 14:40:07,446] Trial 34 finished with value: 0.8201249999999998 and parameters: {'n_estimators': 1100, 'learning_rate': 0.015874778960521134, 'max_depth': 6, 'min_child_weight': 4, 'gamma': 0.44231997494319797, 'subsample': 0.6591444051926686, 'colsample_bytree': 0.864458973730278, 'colsample_bylevel': 0.6458474741501206, 'colsample_bynode': 0.7167228473518426, 'reg_alpha': 0.21527449385290204, 'reg_lambda': 0.8521650729935968, 'max_delta_step': 1, 'scale_pos_weight': 1.0397041909784726}. Best is trial 33 with value: 0.82125.\n",
      "[I 2025-11-03 14:40:30,668] Trial 35 finished with value: 0.81875 and parameters: {'n_estimators': 1100, 'learning_rate': 0.014909923758663585, 'max_depth': 6, 'min_child_weight': 4, 'gamma': 0.4175438036605259, 'subsample': 0.8013704330851117, 'colsample_bytree': 0.8603770024042319, 'colsample_bylevel': 0.6444859237997373, 'colsample_bynode': 0.7044458054614302, 'reg_alpha': 0.20467434403861826, 'reg_lambda': 0.8225048027290129, 'max_delta_step': 1, 'scale_pos_weight': 1.048398052347116}. Best is trial 33 with value: 0.82125.\n",
      "[I 2025-11-03 14:40:47,671] Trial 36 finished with value: 0.819125 and parameters: {'n_estimators': 1200, 'learning_rate': 0.014302945177755228, 'max_depth': 3, 'min_child_weight': 5, 'gamma': 0.36032105867311576, 'subsample': 0.690911040110549, 'colsample_bytree': 0.8622659812271798, 'colsample_bylevel': 0.633183850882164, 'colsample_bynode': 0.7273153908463411, 'reg_alpha': 0.13323134532161057, 'reg_lambda': 0.7140862737070811, 'max_delta_step': 1, 'scale_pos_weight': 1.1678131528292288}. Best is trial 33 with value: 0.82125.\n",
      "[I 2025-11-03 14:40:52,671] Trial 37 pruned. \n",
      "[I 2025-11-03 14:40:58,265] Trial 38 pruned. \n",
      "[I 2025-11-03 14:41:23,574] Trial 39 finished with value: 0.8192499999999999 and parameters: {'n_estimators': 1100, 'learning_rate': 0.011843040036245318, 'max_depth': 6, 'min_child_weight': 3, 'gamma': 0.23990416434231968, 'subsample': 0.658218269339798, 'colsample_bytree': 0.8711460451095127, 'colsample_bylevel': 0.7527806374839894, 'colsample_bynode': 0.7394536027541903, 'reg_alpha': 0.08850415163393713, 'reg_lambda': 1.7510502380628519, 'max_delta_step': 1, 'scale_pos_weight': 1.0021494032718299}. Best is trial 33 with value: 0.82125.\n",
      "[I 2025-11-03 14:41:43,770] Trial 40 finished with value: 0.8188749999999999 and parameters: {'n_estimators': 900, 'learning_rate': 0.013509286599853492, 'max_depth': 6, 'min_child_weight': 7, 'gamma': 0.7164999397036589, 'subsample': 0.6381573148146885, 'colsample_bytree': 0.8120988210909956, 'colsample_bylevel': 0.6805797745418339, 'colsample_bynode': 0.7895591924592888, 'reg_alpha': 0.5004117316324969, 'reg_lambda': 8.296247702410101, 'max_delta_step': 2, 'scale_pos_weight': 1.0378155238731175}. Best is trial 33 with value: 0.82125.\n",
      "[I 2025-11-03 14:42:09,670] Trial 41 finished with value: 0.821875 and parameters: {'n_estimators': 1100, 'learning_rate': 0.011449485273634314, 'max_depth': 6, 'min_child_weight': 3, 'gamma': 0.22974650518834772, 'subsample': 0.6728123685295924, 'colsample_bytree': 0.8675845945186168, 'colsample_bylevel': 0.7677569197976835, 'colsample_bynode': 0.7501348676778313, 'reg_alpha': 0.0563276020486331, 'reg_lambda': 1.3620078099492614, 'max_delta_step': 1, 'scale_pos_weight': 1.0135567750586636}. Best is trial 41 with value: 0.821875.\n",
      "[I 2025-11-03 14:42:14,861] Trial 42 pruned. \n",
      "[I 2025-11-03 14:42:20,276] Trial 43 pruned. \n",
      "[I 2025-11-03 14:42:26,369] Trial 44 pruned. \n",
      "[I 2025-11-03 14:42:44,955] Trial 45 finished with value: 0.820875 and parameters: {'n_estimators': 1000, 'learning_rate': 0.012791182726347768, 'max_depth': 5, 'min_child_weight': 10, 'gamma': 0.24832285053687225, 'subsample': 0.7037136603814238, 'colsample_bytree': 0.8991465245887599, 'colsample_bylevel': 0.7417015938281718, 'colsample_bynode': 0.7410237550891126, 'reg_alpha': 0.3575802881070892, 'reg_lambda': 0.9330648038252198, 'max_delta_step': 1, 'scale_pos_weight': 1.0268919291488834}. Best is trial 41 with value: 0.821875.\n",
      "[I 2025-11-03 14:43:03,528] Trial 46 finished with value: 0.820625 and parameters: {'n_estimators': 1000, 'learning_rate': 0.015716955444417882, 'max_depth': 5, 'min_child_weight': 9, 'gamma': 0.2426709911626171, 'subsample': 0.6937605998712641, 'colsample_bytree': 0.8670594604631575, 'colsample_bylevel': 0.7445279029742291, 'colsample_bynode': 0.7438738639944339, 'reg_alpha': 0.3579056164853648, 'reg_lambda': 0.622487277559278, 'max_delta_step': 1, 'scale_pos_weight': 1.0163983225727171}. Best is trial 41 with value: 0.821875.\n",
      "[I 2025-11-03 14:43:08,165] Trial 47 pruned. \n",
      "[I 2025-11-03 14:43:17,482] Trial 48 pruned. \n",
      "[I 2025-11-03 14:43:34,109] Trial 49 finished with value: 0.8200000000000001 and parameters: {'n_estimators': 900, 'learning_rate': 0.01264948986830071, 'max_depth': 5, 'min_child_weight': 9, 'gamma': 0.16059478637653626, 'subsample': 0.6706673493578146, 'colsample_bytree': 0.8534093177747129, 'colsample_bylevel': 0.7098846212535361, 'colsample_bynode': 0.6532350897419049, 'reg_alpha': 0.2936886592752231, 'reg_lambda': 0.7723738262866379, 'max_delta_step': 1, 'scale_pos_weight': 1.1840387300591808}. Best is trial 41 with value: 0.821875.\n",
      "[I 2025-11-03 14:43:37,940] Trial 50 pruned. \n",
      "[I 2025-11-03 14:43:41,879] Trial 51 pruned. \n",
      "[I 2025-11-03 14:43:59,784] Trial 52 finished with value: 0.822125 and parameters: {'n_estimators': 1000, 'learning_rate': 0.01532482343357218, 'max_depth': 5, 'min_child_weight': 9, 'gamma': 0.18252755739135762, 'subsample': 0.6640032778095043, 'colsample_bytree': 0.8538605954817601, 'colsample_bylevel': 0.6961759954975156, 'colsample_bynode': 0.6207124327278246, 'reg_alpha': 0.38452984067709783, 'reg_lambda': 0.7643461135590525, 'max_delta_step': 1, 'scale_pos_weight': 1.0802518928123983}. Best is trial 52 with value: 0.822125.\n",
      "[I 2025-11-03 14:44:04,343] Trial 53 pruned. \n",
      "[I 2025-11-03 14:44:22,177] Trial 54 finished with value: 0.820625 and parameters: {'n_estimators': 1000, 'learning_rate': 0.015561669254940355, 'max_depth': 5, 'min_child_weight': 9, 'gamma': 0.07880413842821585, 'subsample': 0.6456117934517905, 'colsample_bytree': 0.8623286927043519, 'colsample_bylevel': 0.6781128716364876, 'colsample_bynode': 0.632052528297781, 'reg_alpha': 0.45772672174109186, 'reg_lambda': 0.7582960412953373, 'max_delta_step': 1, 'scale_pos_weight': 1.031889953835293}. Best is trial 52 with value: 0.822125.\n",
      "[I 2025-11-03 14:44:40,062] Trial 55 finished with value: 0.8213750000000001 and parameters: {'n_estimators': 1000, 'learning_rate': 0.0194692509757907, 'max_depth': 5, 'min_child_weight': 8, 'gamma': 0.06058938170917785, 'subsample': 0.6429518995704715, 'colsample_bytree': 0.8351583614939742, 'colsample_bylevel': 0.6742674369942573, 'colsample_bynode': 0.6191922062196892, 'reg_alpha': 0.4563983944572536, 'reg_lambda': 0.7853706900010979, 'max_delta_step': 1, 'scale_pos_weight': 1.020508037869641}. Best is trial 52 with value: 0.822125.\n",
      "[I 2025-11-03 14:44:56,091] Trial 56 finished with value: 0.8213750000000001 and parameters: {'n_estimators': 1000, 'learning_rate': 0.026195308046892112, 'max_depth': 4, 'min_child_weight': 8, 'gamma': 0.02761662749628427, 'subsample': 0.6196838491948596, 'colsample_bytree': 0.8328644364709644, 'colsample_bylevel': 0.8084112765642779, 'colsample_bynode': 0.6319568370339964, 'reg_alpha': 0.423843138024973, 'reg_lambda': 0.9690871976373716, 'max_delta_step': 1, 'scale_pos_weight': 1.0104490504435797}. Best is trial 52 with value: 0.822125.\n",
      "[I 2025-11-03 14:45:10,598] Trial 57 finished with value: 0.820875 and parameters: {'n_estimators': 900, 'learning_rate': 0.024172228161142704, 'max_depth': 4, 'min_child_weight': 8, 'gamma': 0.015649405010016743, 'subsample': 0.6237881747697555, 'colsample_bytree': 0.834314849891336, 'colsample_bylevel': 0.796741631439769, 'colsample_bynode': 0.6328970416354087, 'reg_alpha': 0.4251504609730074, 'reg_lambda': 0.9609232132207908, 'max_delta_step': 1, 'scale_pos_weight': 1.0009632817363872}. Best is trial 52 with value: 0.822125.\n",
      "[I 2025-11-03 14:45:13,946] Trial 58 pruned. \n",
      "[I 2025-11-03 14:45:25,336] Trial 59 pruned. \n",
      "[I 2025-11-03 14:45:29,165] Trial 60 pruned. \n",
      "[I 2025-11-03 14:45:44,133] Trial 61 finished with value: 0.822875 and parameters: {'n_estimators': 900, 'learning_rate': 0.02633182572847021, 'max_depth': 4, 'min_child_weight': 8, 'gamma': 0.005503790007509533, 'subsample': 0.6219040790854771, 'colsample_bytree': 0.8347115678250765, 'colsample_bylevel': 0.7972740127212694, 'colsample_bynode': 0.6341480500949653, 'reg_alpha': 0.41151197767773867, 'reg_lambda': 0.9573531506162529, 'max_delta_step': 1, 'scale_pos_weight': 1.0030482351916117}. Best is trial 61 with value: 0.822875.\n",
      "[I 2025-11-03 14:45:58,581] Trial 62 finished with value: 0.82225 and parameters: {'n_estimators': 900, 'learning_rate': 0.026853132377666455, 'max_depth': 4, 'min_child_weight': 8, 'gamma': 0.04854456259278969, 'subsample': 0.6177795726607435, 'colsample_bytree': 0.8432385929099652, 'colsample_bylevel': 0.7826367529288646, 'colsample_bynode': 0.6324837913707607, 'reg_alpha': 0.5180615384205287, 'reg_lambda': 0.77281135420835, 'max_delta_step': 1, 'scale_pos_weight': 1.019626545890763}. Best is trial 61 with value: 0.822875.\n",
      "[I 2025-11-03 14:46:11,706] Trial 63 finished with value: 0.8233750000000001 and parameters: {'n_estimators': 800, 'learning_rate': 0.026903035419902337, 'max_depth': 4, 'min_child_weight': 8, 'gamma': 0.039871447930199935, 'subsample': 0.6092684563246151, 'colsample_bytree': 0.8384119779272207, 'colsample_bylevel': 0.808324436329465, 'colsample_bynode': 0.6346001385909943, 'reg_alpha': 0.5173888577259714, 'reg_lambda': 0.7395028456377437, 'max_delta_step': 1, 'scale_pos_weight': 1.011137218440395}. Best is trial 63 with value: 0.8233750000000001.\n",
      "[I 2025-11-03 14:46:24,788] Trial 64 finished with value: 0.8194999999999999 and parameters: {'n_estimators': 800, 'learning_rate': 0.028583202419872156, 'max_depth': 4, 'min_child_weight': 8, 'gamma': 0.043882868697508, 'subsample': 0.612010732484576, 'colsample_bytree': 0.8424277421678574, 'colsample_bylevel': 0.8436679141973981, 'colsample_bynode': 0.6332916816661367, 'reg_alpha': 0.5866065016192481, 'reg_lambda': 0.7602403485840944, 'max_delta_step': 1, 'scale_pos_weight': 1.0529765495714025}. Best is trial 63 with value: 0.8233750000000001.\n",
      "[I 2025-11-03 14:46:28,033] Trial 65 pruned. \n",
      "[I 2025-11-03 14:46:42,598] Trial 66 finished with value: 0.82175 and parameters: {'n_estimators': 900, 'learning_rate': 0.02599136115459465, 'max_depth': 4, 'min_child_weight': 8, 'gamma': 0.025775864149318508, 'subsample': 0.6296723652016782, 'colsample_bytree': 0.8073089010054734, 'colsample_bylevel': 0.792570704387619, 'colsample_bynode': 0.624450030298109, 'reg_alpha': 0.6469256269868119, 'reg_lambda': 0.7070596839185695, 'max_delta_step': 0, 'scale_pos_weight': 1.0260631128884097}. Best is trial 63 with value: 0.8233750000000001.\n",
      "[I 2025-11-03 14:46:45,606] Trial 67 pruned. \n",
      "[I 2025-11-03 14:46:49,193] Trial 68 pruned. \n",
      "[I 2025-11-03 14:46:52,469] Trial 69 pruned. \n",
      "[I 2025-11-03 14:46:56,011] Trial 70 pruned. \n",
      "[I 2025-11-03 14:46:59,534] Trial 71 pruned. \n",
      "[I 2025-11-03 14:47:02,807] Trial 72 pruned. \n",
      "[I 2025-11-03 14:47:16,152] Trial 73 finished with value: 0.82225 and parameters: {'n_estimators': 800, 'learning_rate': 0.030095027584514086, 'max_depth': 4, 'min_child_weight': 7, 'gamma': 0.06436526353615417, 'subsample': 0.6115812889073406, 'colsample_bytree': 0.8558476753958486, 'colsample_bylevel': 0.7923751460541835, 'colsample_bynode': 0.600051886767161, 'reg_alpha': 0.3953582663137445, 'reg_lambda': 1.2318957378156115, 'max_delta_step': 2, 'scale_pos_weight': 1.0540495235664327}. Best is trial 63 with value: 0.8233750000000001.\n",
      "[I 2025-11-03 14:47:19,148] Trial 74 pruned. \n",
      "[I 2025-11-03 14:47:32,167] Trial 75 finished with value: 0.8231249999999999 and parameters: {'n_estimators': 800, 'learning_rate': 0.030226214584253692, 'max_depth': 4, 'min_child_weight': 7, 'gamma': 0.14180053484544775, 'subsample': 0.6514256341804038, 'colsample_bytree': 0.8550134380275001, 'colsample_bylevel': 0.6605757578259119, 'colsample_bynode': 0.6902724686064343, 'reg_alpha': 0.6010151043020487, 'reg_lambda': 2.2933770519141525, 'max_delta_step': 2, 'scale_pos_weight': 1.0846277428019055}. Best is trial 63 with value: 0.8233750000000001.\n",
      "[I 2025-11-03 14:47:35,511] Trial 76 pruned. \n",
      "[I 2025-11-03 14:47:48,483] Trial 77 finished with value: 0.82075 and parameters: {'n_estimators': 800, 'learning_rate': 0.023563442083629465, 'max_depth': 4, 'min_child_weight': 7, 'gamma': 0.11734820714463493, 'subsample': 0.6319860237040852, 'colsample_bytree': 0.8557426140544007, 'colsample_bylevel': 0.6557992252521027, 'colsample_bynode': 0.6582523213058261, 'reg_alpha': 0.7289878723907552, 'reg_lambda': 1.6577513539994668, 'max_delta_step': 2, 'scale_pos_weight': 1.0710104298450567}. Best is trial 63 with value: 0.8233750000000001.\n",
      "[I 2025-11-03 14:47:51,518] Trial 78 pruned. \n",
      "[I 2025-11-03 14:48:03,089] Trial 79 finished with value: 0.8231249999999999 and parameters: {'n_estimators': 800, 'learning_rate': 0.03897128037335753, 'max_depth': 3, 'min_child_weight': 7, 'gamma': 0.20949620381910417, 'subsample': 0.6051408929685854, 'colsample_bytree': 0.6978994343412416, 'colsample_bylevel': 0.8363628095591015, 'colsample_bynode': 0.6453840511589449, 'reg_alpha': 0.5993383733490183, 'reg_lambda': 2.037482788637421, 'max_delta_step': 2, 'scale_pos_weight': 1.262729664502447}. Best is trial 63 with value: 0.8233750000000001.\n",
      "[I 2025-11-03 14:48:05,558] Trial 80 pruned. \n",
      "[I 2025-11-03 14:48:17,358] Trial 81 finished with value: 0.82075 and parameters: {'n_estimators': 800, 'learning_rate': 0.032485878181042746, 'max_depth': 3, 'min_child_weight': 7, 'gamma': 0.17788663506053992, 'subsample': 0.6235623540799089, 'colsample_bytree': 0.7593923588364084, 'colsample_bylevel': 0.8557035071863784, 'colsample_bynode': 0.6699942061146401, 'reg_alpha': 0.6096051525002073, 'reg_lambda': 2.091481726687118, 'max_delta_step': 2, 'scale_pos_weight': 1.0630819823545017}. Best is trial 63 with value: 0.8233750000000001.\n",
      "[I 2025-11-03 14:48:20,315] Trial 82 pruned. \n",
      "[I 2025-11-03 14:48:23,857] Trial 83 pruned. \n",
      "[I 2025-11-03 14:48:26,796] Trial 84 pruned. \n",
      "[I 2025-11-03 14:48:30,007] Trial 85 pruned. \n",
      "[I 2025-11-03 14:48:33,532] Trial 86 pruned. \n",
      "[I 2025-11-03 14:48:36,597] Trial 87 pruned. \n",
      "[I 2025-11-03 14:48:40,129] Trial 88 pruned. \n",
      "[I 2025-11-03 14:48:45,223] Trial 89 pruned. \n",
      "[I 2025-11-03 14:48:48,535] Trial 90 pruned. \n",
      "[I 2025-11-03 14:48:54,771] Trial 91 pruned. \n",
      "[I 2025-11-03 14:48:58,292] Trial 92 pruned. \n",
      "[I 2025-11-03 14:49:12,673] Trial 93 finished with value: 0.821625 and parameters: {'n_estimators': 900, 'learning_rate': 0.024528191142884473, 'max_depth': 4, 'min_child_weight': 9, 'gamma': 0.07174458049434199, 'subsample': 0.6268496935854821, 'colsample_bytree': 0.878865301302216, 'colsample_bylevel': 0.7058262765855163, 'colsample_bynode': 0.6384498798928122, 'reg_alpha': 0.3811602690808067, 'reg_lambda': 0.935968780176735, 'max_delta_step': 1, 'scale_pos_weight': 1.0602031036227766}. Best is trial 63 with value: 0.8233750000000001.\n",
      "[I 2025-11-03 14:49:16,243] Trial 94 pruned. \n",
      "[I 2025-11-03 14:49:30,701] Trial 95 finished with value: 0.8223750000000001 and parameters: {'n_estimators': 900, 'learning_rate': 0.027022917721085532, 'max_depth': 4, 'min_child_weight': 9, 'gamma': 0.12333204150621102, 'subsample': 0.6359307721137791, 'colsample_bytree': 0.8782461784219558, 'colsample_bylevel': 0.6881010118362776, 'colsample_bynode': 0.6545490729308291, 'reg_alpha': 0.7710447582984057, 'reg_lambda': 1.6019645148270591, 'max_delta_step': 2, 'scale_pos_weight': 1.0381783898248436}. Best is trial 63 with value: 0.8233750000000001.\n",
      "[I 2025-11-03 14:49:33,904] Trial 96 pruned. \n",
      "[I 2025-11-03 14:49:37,469] Trial 97 pruned. \n",
      "[I 2025-11-03 14:49:47,071] Trial 98 pruned. \n",
      "[I 2025-11-03 14:49:50,012] Trial 99 pruned. \n",
      "[I 2025-11-03 14:49:53,573] Trial 100 pruned. \n",
      "[I 2025-11-03 14:50:08,008] Trial 101 finished with value: 0.8223750000000001 and parameters: {'n_estimators': 900, 'learning_rate': 0.02453939901930726, 'max_depth': 4, 'min_child_weight': 10, 'gamma': 0.046572145358053624, 'subsample': 0.638241470883443, 'colsample_bytree': 0.8761666496005301, 'colsample_bylevel': 0.6930406122096597, 'colsample_bynode': 0.6407169437852445, 'reg_alpha': 0.5669279199474923, 'reg_lambda': 1.1408397519594957, 'max_delta_step': 1, 'scale_pos_weight': 1.0692156744689874}. Best is trial 63 with value: 0.8233750000000001.\n",
      "[I 2025-11-03 14:50:11,807] Trial 102 pruned. \n",
      "[I 2025-11-03 14:50:26,641] Trial 103 finished with value: 0.82075 and parameters: {'n_estimators': 900, 'learning_rate': 0.02836252273166805, 'max_depth': 4, 'min_child_weight': 10, 'gamma': 0.10611673321771065, 'subsample': 0.6377884973596067, 'colsample_bytree': 0.8925453299786993, 'colsample_bylevel': 0.6950489271991109, 'colsample_bynode': 0.6534311449021057, 'reg_alpha': 0.5670212022485144, 'reg_lambda': 1.107245151893084, 'max_delta_step': 1, 'scale_pos_weight': 1.042051815356376}. Best is trial 63 with value: 0.8233750000000001.\n",
      "[I 2025-11-03 14:50:41,146] Trial 104 finished with value: 0.820375 and parameters: {'n_estimators': 900, 'learning_rate': 0.026669479678368642, 'max_depth': 4, 'min_child_weight': 10, 'gamma': 0.042746949898715844, 'subsample': 0.6163003110077644, 'colsample_bytree': 0.8549028349395397, 'colsample_bylevel': 0.6834300529478222, 'colsample_bynode': 0.631243310839964, 'reg_alpha': 0.6132028914332536, 'reg_lambda': 1.3551700712072208, 'max_delta_step': 1, 'scale_pos_weight': 1.024395369507918}. Best is trial 63 with value: 0.8233750000000001.\n",
      "[I 2025-11-03 14:50:46,883] Trial 105 pruned. \n",
      "[I 2025-11-03 14:51:04,240] Trial 106 finished with value: 0.822125 and parameters: {'n_estimators': 1100, 'learning_rate': 0.024184502022347027, 'max_depth': 4, 'min_child_weight': 9, 'gamma': 0.018059471567310786, 'subsample': 0.6298237236299038, 'colsample_bytree': 0.8134516571696235, 'colsample_bylevel': 0.8010372554048267, 'colsample_bynode': 0.6129374275514587, 'reg_alpha': 0.5938265762029903, 'reg_lambda': 1.0165135775059164, 'max_delta_step': 1, 'scale_pos_weight': 1.0132553261163957}. Best is trial 63 with value: 0.8233750000000001.\n",
      "[I 2025-11-03 14:51:08,361] Trial 107 pruned. \n",
      "[I 2025-11-03 14:51:24,362] Trial 108 finished with value: 0.8216249999999998 and parameters: {'n_estimators': 1000, 'learning_rate': 0.024499384109814632, 'max_depth': 4, 'min_child_weight': 10, 'gamma': 0.22457554496741727, 'subsample': 0.6359130033417544, 'colsample_bytree': 0.8729522158256398, 'colsample_bylevel': 0.8248700022256157, 'colsample_bynode': 0.6062090924956152, 'reg_alpha': 0.9179761484377642, 'reg_lambda': 1.0190807299360416, 'max_delta_step': 1, 'scale_pos_weight': 1.0004240743584298}. Best is trial 63 with value: 0.8233750000000001.\n",
      "[I 2025-11-03 14:51:27,854] Trial 109 pruned. \n",
      "[I 2025-11-03 14:51:35,283] Trial 110 pruned. \n",
      "[I 2025-11-03 14:51:39,397] Trial 111 pruned. \n",
      "[I 2025-11-03 14:51:43,791] Trial 112 pruned. \n",
      "[I 2025-11-03 14:51:58,272] Trial 113 finished with value: 0.8219999999999998 and parameters: {'n_estimators': 900, 'learning_rate': 0.027839543745580314, 'max_depth': 4, 'min_child_weight': 8, 'gamma': 0.040934218381680404, 'subsample': 0.6407421564432201, 'colsample_bytree': 0.7779474239420167, 'colsample_bylevel': 0.8389599556836981, 'colsample_bynode': 0.6364378975372237, 'reg_alpha': 0.25978937082925224, 'reg_lambda': 0.8393719119598931, 'max_delta_step': 1, 'scale_pos_weight': 1.0570814051464754}. Best is trial 63 with value: 0.8233750000000001.\n",
      "[I 2025-11-03 14:52:11,493] Trial 114 finished with value: 0.82325 and parameters: {'n_estimators': 800, 'learning_rate': 0.027144019453356767, 'max_depth': 4, 'min_child_weight': 8, 'gamma': 0.04623143935256948, 'subsample': 0.6412883117932909, 'colsample_bytree': 0.7765699225072813, 'colsample_bylevel': 0.8429991046460433, 'colsample_bynode': 0.6367893485016191, 'reg_alpha': 0.05163619640931697, 'reg_lambda': 1.0029327081153112, 'max_delta_step': 1, 'scale_pos_weight': 1.1184867969085954}. Best is trial 63 with value: 0.8233750000000001.\n",
      "[I 2025-11-03 14:52:14,887] Trial 115 pruned. \n",
      "[I 2025-11-03 14:52:18,186] Trial 116 pruned. \n",
      "[I 2025-11-03 14:52:21,486] Trial 117 pruned. \n",
      "[I 2025-11-03 14:52:24,979] Trial 118 pruned. \n",
      "[I 2025-11-03 14:52:27,981] Trial 119 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best balanced_accuracy: 0.8234\n",
      "Best params:\n",
      "  n_estimators: 800\n",
      "  learning_rate: 0.026903035419902337\n",
      "  max_depth: 4\n",
      "  min_child_weight: 8\n",
      "  gamma: 0.039871447930199935\n",
      "  subsample: 0.6092684563246151\n",
      "  colsample_bytree: 0.8384119779272207\n",
      "  colsample_bylevel: 0.808324436329465\n",
      "  colsample_bynode: 0.6346001385909943\n",
      "  reg_alpha: 0.5173888577259714\n",
      "  reg_lambda: 0.7395028456377437\n",
      "  max_delta_step: 1\n",
      "  scale_pos_weight: 1.011137218440395\n",
      "✅ Salvato optuna_trials_xgb.csv\n",
      "\n",
      "best_params pronti per la 10-Fold CV:\n",
      "{'n_estimators': 800, 'learning_rate': 0.026903035419902337, 'max_depth': 4, 'min_child_weight': 8, 'gamma': 0.039871447930199935, 'subsample': 0.6092684563246151, 'colsample_bytree': 0.8384119779272207, 'colsample_bylevel': 0.808324436329465, 'colsample_bynode': 0.6346001385909943, 'reg_alpha': 0.5173888577259714, 'reg_lambda': 0.7395028456377437, 'max_delta_step': 1, 'scale_pos_weight': 1.011137218440395, 'booster': 'gbtree'}\n"
     ]
    }
   ],
   "source": [
    "# === Optuna Study per selezione iperparametri XGBoost (balanced_accuracy) ===\n",
    "# Se Optuna non è installato: togli il commento alla riga sotto in Jupyter\n",
    "# %pip install -q optuna\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from xgboost import XGBClassifier\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Usa lo stesso split interno del GridSearch\n",
    "cv_inner = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Calibra il range di scale_pos_weight attorno al valore automatico\n",
    "pos_rate = float(y_train_val.mean())\n",
    "spw_auto = float((1.0 - pos_rate) / max(pos_rate, 1e-9))\n",
    "spw_low = max(1.0, spw_auto * 0.6)\n",
    "spw_high = min(10.0, spw_auto * 1.6)\n",
    "\n",
    "EARLY_STOPPING_ROUNDS = 50\n",
    "\n",
    "def _predict_proba_best(clf, X):\n",
    "    # Compat con diverse versioni XGBoost/SKLearn wrapper\n",
    "    best_iter = getattr(clf, \"best_iteration\", None)\n",
    "    try:\n",
    "        if best_iter is not None:\n",
    "            return clf.predict_proba(X, iteration_range=(0, int(best_iter)+1))[:, 1]\n",
    "    except TypeError:\n",
    "        pass\n",
    "    try:\n",
    "        booster = clf.get_booster()\n",
    "        best_ntree_limit = getattr(booster, \"best_ntree_limit\", None)\n",
    "        if best_ntree_limit is not None:\n",
    "            return clf.predict_proba(X, ntree_limit=int(best_ntree_limit))[:, 1]\n",
    "    except Exception:\n",
    "        pass\n",
    "    return clf.predict_proba(X)[:, 1]\n",
    "\n",
    "def objective(trial: optuna.Trial) -> float:\n",
    "    params = {\n",
    "        \"booster\": \"gbtree\",\n",
    "        \"tree_method\": \"hist\",\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 300, 1200, step=100),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.10, log=True),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 6),\n",
    "        \"min_child_weight\": trial.suggest_int(\"min_child_weight\", 2, 10),\n",
    "        \"gamma\": trial.suggest_float(\"gamma\", 0.0, 0.8),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.6, 0.9),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.6, 0.9),\n",
    "        \"colsample_bylevel\": trial.suggest_float(\"colsample_bylevel\", 0.6, 1.0),\n",
    "        \"colsample_bynode\": trial.suggest_float(\"colsample_bynode\", 0.6, 1.0),\n",
    "        \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 0.0, 1.0),\n",
    "        \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 0.5, 10.0, log=True),\n",
    "        \"max_delta_step\": trial.suggest_int(\"max_delta_step\", 0, 2),\n",
    "        \"scale_pos_weight\": trial.suggest_float(\"scale_pos_weight\", spw_low, spw_high),\n",
    "    }\n",
    "\n",
    "    scores = []\n",
    "    for fold_idx, (tr_idx, va_idx) in enumerate(cv_inner.split(X_train_val, y_train_val), start=1):\n",
    "        X_tr, X_va = X_train_val[tr_idx], X_train_val[va_idx]\n",
    "        y_tr, y_va = y_train_val[tr_idx], y_train_val[va_idx]\n",
    "\n",
    "        clf = XGBClassifier(\n",
    "            **params,\n",
    "            use_label_encoder=False,\n",
    "            eval_metric=\"logloss\",\n",
    "            random_state=42,\n",
    "            n_jobs=1\n",
    "        )\n",
    "\n",
    "        # Early stopping su fold\n",
    "        try:\n",
    "            clf.fit(X_tr, y_tr, eval_set=[(X_va, y_va)], early_stopping_rounds=EARLY_STOPPING_ROUNDS, verbose=False)\n",
    "        except TypeError:\n",
    "            clf.fit(X_tr, y_tr, eval_set=[(X_va, y_va)], verbose=False)\n",
    "\n",
    "        proba = _predict_proba_best(clf, X_va)\n",
    "        preds = (proba >= 0.5).astype(int)\n",
    "        score = balanced_accuracy_score(y_va, preds)\n",
    "        scores.append(score)\n",
    "\n",
    "        # Pruning\n",
    "        trial.report(float(np.mean(scores)), step=fold_idx)\n",
    "        if trial.should_prune():\n",
    "            raise optuna.TrialPruned()\n",
    "\n",
    "    return float(np.mean(scores))\n",
    "\n",
    "N_TRIALS = 120    # aumenta/riduci in base al tempo a disposizione\n",
    "TIMEOUT = None    # in secondi; ad es. 7200 per ~2h\n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\", sampler=TPESampler(seed=42))\n",
    "study.optimize(objective, n_trials=N_TRIALS, timeout=TIMEOUT, gc_after_trial=True)\n",
    "\n",
    "print(f\"Best balanced_accuracy: {study.best_value:.4f}\")\n",
    "print(\"Best params:\")\n",
    "for k, v in study.best_params.items():\n",
    "    print(f\"  {k}: {v}\")\n",
    "\n",
    "# Esporta risultati\n",
    "trials_df = study.trials_dataframe()\n",
    "trials_df.to_csv(\"optuna_trials_xgb.csv\", index=False)\n",
    "print(\"✅ Salvato optuna_trials_xgb.csv\")\n",
    "\n",
    "# Prepara best_params per le celle successive (CV/holdout/submission)\n",
    "best_params = dict(study.best_params)\n",
    "best_params.update({\n",
    "    \"booster\": \"gbtree\",\n",
    "    # Valori sicuri che non cerchiamo con Optuna, ma che il tuo codice usa\n",
    "    # (lasciare invariati se non li vuoi forzare)\n",
    "})\n",
    "print(\"\\nbest_params pronti per la 10-Fold CV:\")\n",
    "print(best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4ff77b",
   "metadata": {},
   "source": [
    "# Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cab0d514",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 10-Fold Cross-Validation (9 train + 1 validation) ===\n",
      "Parametri utilizzati: {'booster': 'gbtree', 'tree_method': 'hist', 'max_bin': 256, 'learning_rate': 0.035, 'n_estimators': 900, 'max_depth': 3, 'min_child_weight': 9, 'gamma': 0.5, 'subsample': 0.7, 'colsample_bytree': 0.7, 'colsample_bynode': 0.7, 'colsample_bylevel': 0.8, 'reg_alpha': 0.4, 'reg_lambda': 10.0, 'max_delta_step': 1}\n",
      "\n",
      "Fold 1: no ES, train=7200, val=800, acc_val=80.88%, acc_val_opt=81.12% @thr=0.490, acc_train=86.99%, gap=6.11%\n",
      "Fold 2: no ES, train=7200, val=800, acc_val=81.25%, acc_val_opt=81.75% @thr=0.439, acc_train=86.93%, gap=5.68%\n",
      "Fold 3: no ES, train=7200, val=800, acc_val=82.25%, acc_val_opt=82.38% @thr=0.400, acc_train=86.82%, gap=4.57%\n",
      "Fold 4: no ES, train=7200, val=800, acc_val=83.88%, acc_val_opt=84.88% @thr=0.429, acc_train=87.00%, gap=3.12%\n",
      "Fold 5: no ES, train=7200, val=800, acc_val=85.25%, acc_val_opt=85.38% @thr=0.504, acc_train=86.71%, gap=1.46%\n",
      "Fold 6: no ES, train=7200, val=800, acc_val=81.38%, acc_val_opt=81.75% @thr=0.522, acc_train=87.10%, gap=5.72%\n",
      "Fold 7: no ES, train=7200, val=800, acc_val=83.50%, acc_val_opt=84.12% @thr=0.514, acc_train=86.79%, gap=3.29%\n",
      "Fold 8: no ES, train=7200, val=800, acc_val=83.50%, acc_val_opt=83.75% @thr=0.506, acc_train=86.76%, gap=3.26%\n",
      "Fold 9: no ES, train=7200, val=800, acc_val=82.75%, acc_val_opt=82.88% @thr=0.490, acc_train=87.03%, gap=4.28%\n",
      "Fold 10: no ES, train=7200, val=800, acc_val=81.38%, acc_val_opt=81.88% @thr=0.489, acc_train=87.25%, gap=5.88%\n",
      "\n",
      "============================================================\n",
      "Risultati Cross-Validation\n",
      "============================================================\n",
      "  Fold 1: val_acc=80.88%, val_acc_opt=81.12% @thr=0.490, train_acc=86.99%, gap=6.11%\n",
      "  Fold 2: val_acc=81.25%, val_acc_opt=81.75% @thr=0.439, train_acc=86.93%, gap=5.68%\n",
      "  Fold 3: val_acc=82.25%, val_acc_opt=82.38% @thr=0.400, train_acc=86.82%, gap=4.57%\n",
      "  Fold 4: val_acc=83.88%, val_acc_opt=84.88% @thr=0.429, train_acc=87.00%, gap=3.12%\n",
      "  Fold 5: val_acc=85.25%, val_acc_opt=85.38% @thr=0.504, train_acc=86.71%, gap=1.46%\n",
      "  Fold 6: val_acc=81.38%, val_acc_opt=81.75% @thr=0.522, train_acc=87.10%, gap=5.72%\n",
      "  Fold 7: val_acc=83.50%, val_acc_opt=84.12% @thr=0.514, train_acc=86.79%, gap=3.29%\n",
      "  Fold 8: val_acc=83.50%, val_acc_opt=83.75% @thr=0.506, train_acc=86.76%, gap=3.26%\n",
      "  Fold 9: val_acc=82.75%, val_acc_opt=82.88% @thr=0.490, train_acc=87.03%, gap=4.28%\n",
      "  Fold 10: val_acc=81.38%, val_acc_opt=81.88% @thr=0.489, train_acc=87.25%, gap=5.88%\n",
      "\n",
      "Mean CV accuracy (0.5): 82.60%\n",
      "Mean CV accuracy (opt thr): 82.99%\n",
      "Mean train accuracy: 86.94%\n",
      "Mean gap (train - val): 4.34%\n",
      "Std CV accuracy:  1.35%\n",
      "Min/Max val acc:  80.88% / 85.25%\n",
      "\n",
      "Peggiore fold: #1 con acc_val=80.88% | acc_val_opt=81.12% | acc_train=86.99% | gap=6.11%\n"
     ]
    }
   ],
   "source": [
    "# === 10-Fold Cross-Validation con iperparametri FISSI ===\n",
    "# IMPORTANTE: Assegna qui i migliori iperparametri trovati dalla cella precedente\n",
    "# Oppure lascia questi di default (conservativi per ridurre overfitting)\n",
    "\n",
    "best_params = {\n",
    "    'booster': 'gbtree',\n",
    "    'tree_method': 'hist',\n",
    "    'max_bin': 256,          # istogrammi più grossolani = meno varianza e più veloce\n",
    "    'learning_rate': 0.035,  # leggermente più alto con meno alberi\n",
    "    'n_estimators': 900,     # meno alberi per ridurre overfitting\n",
    "    'max_depth': 3,\n",
    "    'min_child_weight': 9,   # nodi con più peso => meno overfit\n",
    "    'gamma': 0.5,            # penalizza split deboli\n",
    "    'subsample': 0.7,\n",
    "    'colsample_bytree': 0.7,\n",
    "    'colsample_bynode': 0.7,\n",
    "    'colsample_bylevel': 0.8,\n",
    "    'reg_alpha': 0.4,        # L1\n",
    "    'reg_lambda': 10.0,      # L2 più alta\n",
    "    'max_delta_step': 1\n",
    "}\n",
    "\n",
    "print(\"=== 10-Fold Cross-Validation (9 train + 1 validation) ===\")\n",
    "print(f\"Parametri utilizzati: {best_params}\\n\")\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import xgboost as xgb  # per callback EarlyStopping se disponibile\n",
    "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "outer_accuracies = []\n",
    "folds_info = []\n",
    "train_accuracies = []\n",
    "train_val_gaps = []\n",
    "outer_accuracies_opt = []\n",
    "\n",
    "EARLY_STOPPING_ROUNDS = 50\n",
    "\n",
    "def best_threshold_for_accuracy(y_true, proba, n_grid=201):\n",
    "    y_true = np.asarray(y_true).astype(int)\n",
    "    proba = np.asarray(proba).astype(float)\n",
    "    grid = np.unique(np.quantile(proba, np.linspace(0, 1, n_grid)))\n",
    "    best_thr, best_acc = 0.5, 0.0\n",
    "    for t in grid:\n",
    "        acc = ( ((proba >= t).astype(int) == y_true).mean() )\n",
    "        if (acc > best_acc) or (abs(acc - best_acc) < 1e-12 and abs(t - 0.5) < abs(best_thr - 0.5)):\n",
    "            best_acc, best_thr = float(acc), float(t)\n",
    "    return best_thr, best_acc\n",
    "\n",
    "def _fit_with_es(clf, X_tr, y_tr, X_val, y_val):\n",
    "    \"\"\"Fit con EarlyStopping via callback se supportato; fallback senza ES.\"\"\"\n",
    "    try:\n",
    "        cb = getattr(xgb.callback, 'EarlyStopping', None)\n",
    "        if cb is not None:\n",
    "            clf.fit(X_tr, y_tr, eval_set=[(X_val, y_val)], callbacks=[cb(rounds=EARLY_STOPPING_ROUNDS, save_best=True, maximize=False)], verbose=False)\n",
    "            return True\n",
    "    except TypeError:\n",
    "        pass\n",
    "    clf.fit(X_tr, y_tr, eval_set=[(X_val, y_val)], verbose=False)\n",
    "    return False\n",
    "\n",
    "def _predict_proba_best(clf, X, best_iter=None, best_ntree_limit=None):\n",
    "    \"\"\"Version-safe predict_proba using either iteration_range (new) or ntree_limit (old).\"\"\"\n",
    "    try:\n",
    "        if best_iter is not None:\n",
    "            return clf.predict_proba(X, iteration_range=(0, int(best_iter)+1))[:, 1]\n",
    "    except TypeError:\n",
    "        pass\n",
    "    try:\n",
    "        if best_ntree_limit is not None:\n",
    "            return clf.predict_proba(X, ntree_limit=int(best_ntree_limit))[:, 1]\n",
    "    except TypeError:\n",
    "        pass\n",
    "    return clf.predict_proba(X)[:, 1]\n",
    "\n",
    "fold_idx = 0\n",
    "for train_idx, val_idx in skf.split(X_train_val, y_train_val):\n",
    "    fold_idx += 1\n",
    "    X_tr, X_val = X_train_val[train_idx], X_train_val[val_idx]\n",
    "    y_tr, y_val = y_train_val[train_idx], y_train_val[val_idx]\n",
    "\n",
    "    clf = XGBClassifier(**best_params, use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
    "    used_es = _fit_with_es(clf, X_tr, y_tr, X_val, y_val)\n",
    "\n",
    "    best_iter = getattr(clf, 'best_iteration', None)\n",
    "    try:\n",
    "        booster = clf.get_booster()\n",
    "    except Exception:\n",
    "        booster = None\n",
    "    best_ntree_limit = getattr(booster, 'best_ntree_limit', None) if booster is not None else None\n",
    "\n",
    "    y_val_proba = _predict_proba_best(clf, X_val, best_iter, best_ntree_limit)\n",
    "    y_pred = (y_val_proba >= 0.5).astype(int)\n",
    "    acc = accuracy_score(y_val, y_pred)\n",
    "    outer_accuracies.append(acc)\n",
    "\n",
    "    y_tr_proba = _predict_proba_best(clf, X_tr, best_iter, best_ntree_limit)\n",
    "    y_tr_pred = (y_tr_proba >= 0.5).astype(int)\n",
    "    tr_acc = accuracy_score(y_tr, y_tr_pred)\n",
    "    gap = float(tr_acc - acc)\n",
    "    train_accuracies.append(tr_acc)\n",
    "    train_val_gaps.append(gap)\n",
    "\n",
    "    thr_acc, acc_opt = best_threshold_for_accuracy(y_val, y_val_proba, n_grid=301)\n",
    "    outer_accuracies_opt.append(acc_opt)\n",
    "\n",
    "    val_index_global = idx_train_val[val_idx]\n",
    "    train_index_global = idx_train_val[train_idx]\n",
    "\n",
    "    folds_info.append({\n",
    "        'fold': fold_idx,\n",
    "        'acc': float(acc),\n",
    "        'train_acc': float(tr_acc),\n",
    "        'gap_train_minus_val': float(gap),\n",
    "        'acc_opt': float(acc_opt),\n",
    "        'thr_acc': float(thr_acc),\n",
    "        'best_iteration': int(best_iter) if best_iter is not None else None,\n",
    "        'train_idx': train_idx,\n",
    "        'val_idx': val_idx,\n",
    "        'train_index_global': train_index_global,\n",
    "        'val_index_global': val_index_global,\n",
    "        'y_true': y_val.astype(int),\n",
    "        'y_pred': y_pred.astype(int),\n",
    "        'y_proba': y_val_proba.astype(float)\n",
    "    })\n",
    "\n",
    "    es_tag = 'with ES' if used_es else 'no ES'\n",
    "    print(f'Fold {fold_idx}: {es_tag}, train={len(y_tr)}, val={len(y_val)}, acc_val={acc*100:.2f}%, acc_val_opt={acc_opt*100:.2f}% @thr={thr_acc:.3f}, acc_train={tr_acc*100:.2f}%, gap={(gap)*100:.2f}%')\n",
    "\n",
    "print('\\n' + '='*60)\n",
    "print('Risultati Cross-Validation')\n",
    "print('='*60)\n",
    "for i, a in enumerate(outer_accuracies, 1):\n",
    "    print(f'  Fold {i}: val_acc={a*100:.2f}%, val_acc_opt={outer_accuracies_opt[i-1]*100:.2f}% @thr={folds_info[i-1][\"thr_acc\"]:.3f}, train_acc={train_accuracies[i-1]*100:.2f}%, gap={train_val_gaps[i-1]*100:.2f}%')\n",
    "print(f'\\nMean CV accuracy (0.5): {np.mean(outer_accuracies)*100:.2f}%')\n",
    "print(f'Mean CV accuracy (opt thr): {np.mean(outer_accuracies_opt)*100:.2f}%')\n",
    "print(f'Mean train accuracy: {np.mean(train_accuracies)*100:.2f}%')\n",
    "print(f'Mean gap (train - val): {np.mean(train_val_gaps)*100:.2f}%')\n",
    "print(f'Std CV accuracy:  {np.std(outer_accuracies)*100:.2f}%')\n",
    "print(f'Min/Max val acc:  {np.min(outer_accuracies)*100:.2f}% / {np.max(outer_accuracies)*100:.2f}%')\n",
    "\n",
    "WORST_FOLD_IDX = int(np.argmin(outer_accuracies))\n",
    "WORST_FOLD_NUM = int(folds_info[WORST_FOLD_IDX]['fold'])\n",
    "print(f\"\\nPeggiore fold: #{WORST_FOLD_NUM} con acc_val={outer_accuracies[WORST_FOLD_IDX]*100:.2f}% | acc_val_opt={outer_accuracies_opt[WORST_FOLD_IDX]*100:.2f}% | acc_train={train_accuracies[WORST_FOLD_IDX]*100:.2f}% | gap={train_val_gaps[WORST_FOLD_IDX]*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aef63ee",
   "metadata": {},
   "source": [
    "# Holdout validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8c175e44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start holdout evaluation\n",
      "Using best_params from notebook\n",
      "Holdout size: 2000\n",
      "Accuracy: 0.8270, Balanced Accuracy: 0.8270, ROC AUC: 0.8936\n",
      "Confusion matrix:\n",
      " [[833 167]\n",
      " [179 821]]\n",
      "\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8231    0.8330    0.8280      1000\n",
      "           1     0.8310    0.8210    0.8260      1000\n",
      "\n",
      "    accuracy                         0.8270      2000\n",
      "   macro avg     0.8270    0.8270    0.8270      2000\n",
      "weighted avg     0.8270    0.8270    0.8270      2000\n",
      "\n",
      "Holdout predictions saved to holdout_predictions.csv (rows=2000)\n"
     ]
    }
   ],
   "source": [
    "# === Valutazione su holdout ===\n",
    "from sklearn.metrics import balanced_accuracy_score, roc_auc_score, confusion_matrix, classification_report\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "print('Start holdout evaluation')\n",
    "\n",
    "# Usa best_params se esiste, fallback a parametri di base\n",
    "try:\n",
    "    params = dict(best_params)\n",
    "    print('Using best_params from notebook')\n",
    "except Exception:\n",
    "    params = {}\n",
    "    print('best_params non trovato: uso parametri di default')\n",
    "\n",
    "clf = XGBClassifier(**params, use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
    "clf.fit(X_train_val, y_train_val)\n",
    "\n",
    "# predizioni e probabilità su holdout\n",
    "try:\n",
    "    proba = clf.predict_proba(X_holdout)[:, 1]\n",
    "except Exception:\n",
    "    # fallback: predict then map to 0/1 probs\n",
    "    preds_tmp = clf.predict(X_holdout)\n",
    "    proba = preds_tmp.astype(float)\n",
    "\n",
    "pred = (proba >= 0.5).astype(int)\n",
    "\n",
    "# metriche\n",
    "acc = (pred == y_holdout).mean()\n",
    "bacc = balanced_accuracy_score(y_holdout, pred)\n",
    "try:\n",
    "    roc = roc_auc_score(y_holdout, proba)\n",
    "except Exception:\n",
    "    roc = float('nan')\n",
    "\n",
    "cm = confusion_matrix(y_holdout, pred)\n",
    "cr = classification_report(y_holdout, pred, digits=4)\n",
    "\n",
    "print(f'Holdout size: {len(y_holdout)}')\n",
    "print(f'Accuracy: {acc:.4f}, Balanced Accuracy: {bacc:.4f}, ROC AUC: {roc:.4f}')\n",
    "print('Confusion matrix:\\n', cm)\n",
    "print('\\nClassification report:\\n', cr)\n",
    "\n",
    "# Salva predizioni holdout per ispezione (include battle_id se disponibili)\n",
    "try:\n",
    "    holdout_idx = idx_holdout\n",
    "    holdout_ids = train_df.loc[holdout_idx, 'battle_id'] if 'battle_id' in train_df.columns else pd.Series(holdout_idx, index=holdout_idx)\n",
    "    out_df = pd.DataFrame({'battle_id': holdout_ids.values, 'y_true': y_holdout, 'y_pred': pred, 'y_proba': proba})\n",
    "except Exception:\n",
    "    out_df = pd.DataFrame({'y_true': y_holdout, 'y_pred': pred, 'y_proba': proba})\n",
    "\n",
    "out_path = 'holdout_predictions.csv'\n",
    "out_df.to_csv(out_path, index=False)\n",
    "print(f'Holdout predictions saved to {out_path} (rows={len(out_df)})')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8873db82",
   "metadata": {},
   "source": [
    "# Make submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f3b87173",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Submission rapida post-CV ===\n",
      "✅ File di submission salvato in submission.csv\n",
      "   battle_id  player_won\n",
      "0          0           0\n",
      "1          1           1\n",
      "2          2           1\n",
      "3          3           1\n",
      "4          4           1\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Submission rapida post-CV ===\")\n",
    "cv_submission_model = XGBClassifier(**best_params, use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
    "cv_submission_model.fit(X_train_val, y_train_val)\n",
    "\n",
    "test_aligned = test_df.reindex(columns=FEATURES, fill_value=0)\n",
    "X_test_matrix = test_aligned.astype(float).to_numpy()\n",
    "test_predictions = cv_submission_model.predict(X_test_matrix).astype(int)\n",
    "\n",
    "submission_df = pd.DataFrame({\n",
    "    'battle_id': test_df['battle_id'].astype(np.int64),\n",
    "    'player_won': test_predictions.astype(np.int64)\n",
    "})\n",
    "\n",
    "submission_path = 'submission.csv'\n",
    "submission_df.to_csv(submission_path, index=False)\n",
    "print(f\"✅ File di submission salvato in {submission_path}\")\n",
    "print(submission_df.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
