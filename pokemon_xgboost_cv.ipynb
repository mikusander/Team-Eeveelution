{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9b7eeca",
   "metadata": {},
   "source": [
    "# Pokémon battles — XGBoost with 10-fold outer CV\n",
    "Notebook breve che esegue: feature engineering, split train/val/test, 10-fold outer CV con GridSearchCV interno, valutazione per fold, valutazione su holdout e generazione submission.csv."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae11f50",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3aebef2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caricamento dati...\n",
      "Train records: 10000, Test records: 5000\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from xgboost import XGBClassifier\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# --- Percorsi (modificare se necessario) ---\n",
    "COMPETITION_NAME = 'fds-pokemon-battles-prediction-2025'\n",
    "train_file_path = 'train.jsonl'\n",
    "test_file_path = 'test.jsonl'\n",
    "\n",
    "def load_jsonl(path):\n",
    "    data = []\n",
    "    with open(path, 'r') as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line))\n",
    "    return data\n",
    "\n",
    "print('Caricamento dati...')\n",
    "train_raw = load_jsonl(train_file_path)\n",
    "test_raw = load_jsonl(test_file_path)\n",
    "print(f'Train records: {len(train_raw)}, Test records: {len(test_raw)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea38e58a",
   "metadata": {},
   "source": [
    "# Features engeneering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8e15c30c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db266436078a4fc68e828d7b7cb2c2c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FE (V5 Complete):   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d23516c0f35f41da89b4c7f8102c4eeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FE (V5 Complete):   0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape train/test: (10000, 307) (5000, 306)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>battle_id</th>\n",
       "      <th>player_won</th>\n",
       "      <th>p1_base_hp_sum</th>\n",
       "      <th>p1_base_hp_mean</th>\n",
       "      <th>p1_base_hp_max</th>\n",
       "      <th>p1_base_hp_min</th>\n",
       "      <th>p1_base_hp_std</th>\n",
       "      <th>p1_base_atk_sum</th>\n",
       "      <th>p1_base_atk_mean</th>\n",
       "      <th>p1_base_atk_max</th>\n",
       "      <th>...</th>\n",
       "      <th>tl_hp_diff_end</th>\n",
       "      <th>tl_heal_diff</th>\n",
       "      <th>tl_freeze_adv</th>\n",
       "      <th>p1_fastest_spe</th>\n",
       "      <th>p1_slowest_spe</th>\n",
       "      <th>p1_max_bulk</th>\n",
       "      <th>p1_max_offense</th>\n",
       "      <th>p1_max_common_weakness</th>\n",
       "      <th>p1_fastest_vs_lead_spe</th>\n",
       "      <th>p1_max_bulk_vs_lead_offense</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>695.0</td>\n",
       "      <td>115.833333</td>\n",
       "      <td>250.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>69.367179</td>\n",
       "      <td>435.0</td>\n",
       "      <td>72.500000</td>\n",
       "      <td>110.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.279549</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>120.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>27500.0</td>\n",
       "      <td>220.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>157.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>740.0</td>\n",
       "      <td>123.333333</td>\n",
       "      <td>250.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>64.204534</td>\n",
       "      <td>435.0</td>\n",
       "      <td>72.500000</td>\n",
       "      <td>110.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.320000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>27500.0</td>\n",
       "      <td>220.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>148.648649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>745.0</td>\n",
       "      <td>124.166667</td>\n",
       "      <td>250.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>64.382753</td>\n",
       "      <td>505.0</td>\n",
       "      <td>84.166667</td>\n",
       "      <td>130.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.240000</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>27500.0</td>\n",
       "      <td>220.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>250.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>730.0</td>\n",
       "      <td>121.666667</td>\n",
       "      <td>250.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>65.362239</td>\n",
       "      <td>465.0</td>\n",
       "      <td>77.500000</td>\n",
       "      <td>110.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.060000</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>27500.0</td>\n",
       "      <td>220.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>161.764706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>685.0</td>\n",
       "      <td>114.166667</td>\n",
       "      <td>250.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>70.794107</td>\n",
       "      <td>455.0</td>\n",
       "      <td>75.833333</td>\n",
       "      <td>110.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>27500.0</td>\n",
       "      <td>220.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>157.142857</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 307 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   battle_id  player_won  p1_base_hp_sum  p1_base_hp_mean  p1_base_hp_max  \\\n",
       "0          0           1           695.0       115.833333           250.0   \n",
       "1          1           1           740.0       123.333333           250.0   \n",
       "2          2           1           745.0       124.166667           250.0   \n",
       "3          3           1           730.0       121.666667           250.0   \n",
       "4          4           1           685.0       114.166667           250.0   \n",
       "\n",
       "   p1_base_hp_min  p1_base_hp_std  p1_base_atk_sum  p1_base_atk_mean  \\\n",
       "0            55.0       69.367179            435.0         72.500000   \n",
       "1            65.0       64.204534            435.0         72.500000   \n",
       "2            60.0       64.382753            505.0         84.166667   \n",
       "3            60.0       65.362239            465.0         77.500000   \n",
       "4            50.0       70.794107            455.0         75.833333   \n",
       "\n",
       "   p1_base_atk_max  ...  tl_hp_diff_end  tl_heal_diff  tl_freeze_adv  \\\n",
       "0            110.0  ...        0.279549             0              1   \n",
       "1            110.0  ...       -0.320000             0              0   \n",
       "2            130.0  ...       -0.240000            -1              0   \n",
       "3            110.0  ...       -0.060000             2              0   \n",
       "4            110.0  ...        0.320000            -1              0   \n",
       "\n",
       "   p1_fastest_spe  p1_slowest_spe  p1_max_bulk  p1_max_offense  \\\n",
       "0           120.0            30.0      27500.0           220.0   \n",
       "1           110.0            30.0      27500.0           220.0   \n",
       "2           110.0            30.0      27500.0           220.0   \n",
       "3           110.0            30.0      27500.0           220.0   \n",
       "4           120.0            30.0      27500.0           220.0   \n",
       "\n",
       "   p1_max_common_weakness  p1_fastest_vs_lead_spe  p1_max_bulk_vs_lead_offense  \n",
       "0                     3.0                     5.0                   157.142857  \n",
       "1                     3.0                   -10.0                   148.648649  \n",
       "2                     4.0                    60.0                   250.000000  \n",
       "3                     3.0                     0.0                   161.764706  \n",
       "4                     4.0                     5.0                   157.142857  \n",
       "\n",
       "[5 rows x 307 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. COSTANTI & FUNZIONI BASE\n",
    "# ==============================================================================\n",
    "\n",
    "TYPE_CHART = {\n",
    "    'normal': {'rock': 0.5, 'ghost': 0},\n",
    "    'fire': {'fire': 0.5, 'water': 0.5, 'grass': 2, 'ice': 2, 'bug': 2, 'rock': 0.5, 'dragon': 0.5},\n",
    "    'water': {'fire': 2, 'water': 0.5, 'grass': 0.5, 'ground': 2, 'rock': 2, 'dragon': 0.5},\n",
    "    'grass': {'fire': 0.5, 'water': 2, 'grass': 0.5, 'poison': 0.5, 'ground': 2, 'flying': 0.5, 'bug': 0.5, 'rock': 2, 'dragon': 0.5},\n",
    "    'electric': {'water': 2, 'grass': 0.5, 'electric': 0.5, 'ground': 0, 'flying': 2, 'dragon': 0.5},\n",
    "    'ice': {'fire': 0.5, 'water': 0.5, 'grass': 2, 'ground': 2, 'flying': 2, 'dragon': 2},\n",
    "    'fighting': {'normal': 2, 'ice': 2, 'poison': 0.5, 'flying': 0.5, 'psychic': 0.5, 'bug': 0.5, 'rock': 2, 'ghost': 0},\n",
    "    'poison': {'grass': 2, 'poison': 0.5, 'ground': 0.5, 'bug': 2, 'rock': 0.5, 'ghost': 0.5},\n",
    "    'ground': {'fire': 2, 'grass': 0.5, 'electric': 2, 'poison': 2, 'flying': 0, 'bug': 0.5, 'rock': 2},\n",
    "    'flying': {'grass': 2, 'electric': 0.5, 'fighting': 2, 'bug': 2, 'rock': 0.5},\n",
    "    'psychic': {'fighting': 2, 'poison': 2, 'psychic': 0.5, 'ghost': 0},\n",
    "    'bug': {'fire': 0.5, 'grass': 2, 'fighting': 0.5, 'poison': 2, 'flying': 0.5, 'psychic': 2, 'ghost': 0.5},\n",
    "    'rock': {'fire': 2, 'ice': 2, 'fighting': 0.5, 'ground': 0.5, 'flying': 2, 'bug': 2},\n",
    "    'ghost': {'normal': 0, 'psychic': 0, 'ghost': 2},\n",
    "    'dragon': {'dragon': 2}\n",
    "}\n",
    "\n",
    "ALL_ATTACK_TYPES = list(TYPE_CHART.keys())\n",
    "\n",
    "def get_effectiveness(attack_type: str, defense_types: list) -> float:\n",
    "    if not attack_type or not defense_types: return 1.0\n",
    "    eff = 1.0\n",
    "    for d in defense_types: eff *= TYPE_CHART.get(attack_type, {}).get(d, 1.0)\n",
    "    return eff\n",
    "\n",
    "def _entropy(counter: Counter) -> float:\n",
    "    total = sum(counter.values())\n",
    "    if total == 0: return 0.0\n",
    "    ent = 0.0\n",
    "    for v in counter.values():\n",
    "        p = v / total\n",
    "        if p > 0: ent -= p * math.log(p, 2)\n",
    "    return ent\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. ESTRATTORI FEATURE ORIGINALI (CONSERVATI)\n",
    "# ==============================================================================\n",
    "# (Queste sono le funzioni V1 che avevi già)\n",
    "\n",
    "def calculate_type_advantage(team1: list, team2_lead: dict) -> dict:\n",
    "    out = {'p1_vs_lead_avg_effectiveness': 0.0, 'p1_vs_lead_max_effectiveness': 0.0, 'p1_super_effective_options': 0}\n",
    "    if not team1 or not team2_lead: return out\n",
    "    lead_types = [t.lower() for t in team2_lead.get('types', [])]\n",
    "    if not lead_types: return out\n",
    "    effs = []\n",
    "    for p in team1:\n",
    "        p_types = [t.lower() for t in p.get('types', [])]\n",
    "        max_eff = 0.0\n",
    "        for pt in p_types: max_eff = max(max_eff, get_effectiveness(pt, lead_types))\n",
    "        effs.append(max_eff)\n",
    "    if not effs: return out\n",
    "    out['p1_vs_lead_avg_effectiveness'] = float(np.mean(effs))\n",
    "    out['p1_vs_lead_max_effectiveness'] = float(np.max(effs))\n",
    "    out['p1_super_effective_options'] = int(sum(1 for e in effs if e >= 2))\n",
    "    return out\n",
    "\n",
    "def team_aggregate_features(team: list, prefix: str = 'p1_') -> dict:\n",
    "    stats = ['base_hp','base_atk','base_def','base_spa','base_spd','base_spe']\n",
    "    out = {}\n",
    "    vals = {s: [] for s in stats}\n",
    "    levels = []; types_counter = Counter(); names = []\n",
    "    for p in team:\n",
    "        names.append(p.get('name',''))\n",
    "        for s in stats: vals[s].append(p.get(s, 0))\n",
    "        levels.append(p.get('level', 0))\n",
    "        for t in p.get('types', []): types_counter[t.lower()] += 1\n",
    "    for s in stats:\n",
    "        arr = np.array(vals[s], dtype=float)\n",
    "        out[f'{prefix}{s}_sum'] = float(arr.sum())\n",
    "        out[f'{prefix}{s}_mean'] = float(arr.mean())\n",
    "        out[f'{prefix}{s}_max'] = float(arr.max())\n",
    "        out[f'{prefix}{s}_min'] = float(arr.min())\n",
    "        out[f'{prefix}{s}_std'] = float(arr.std())\n",
    "    level_arr = np.array(levels, dtype=float)\n",
    "    out[f'{prefix}level_mean'] = float(level_arr.mean()) if level_arr.size else 0.0\n",
    "    out[f'{prefix}level_sum'] = float(level_arr.sum()) if level_arr.size else 0.0\n",
    "    out[f'{prefix}n_unique_types'] = int(len(types_counter))\n",
    "    for t in ['normal','fire','water','electric','grass','psychic','ice','dragon','rock','ground','flying']:\n",
    "        out[f'{prefix}type_{t}_count'] = int(types_counter.get(t, 0))\n",
    "    out[f'{prefix}lead_name'] = names[0] if names else ''\n",
    "    out[f'{prefix}n_unique_names'] = int(len(set(names)))\n",
    "    out[f'{prefix}type_entropy'] = float(_entropy(types_counter))\n",
    "    spe_arr = np.array(vals['base_spe'], dtype=float)\n",
    "    out[f'{prefix}spe_p25'] = float(np.percentile(spe_arr, 25)) if spe_arr.size else 0.0\n",
    "    out[f'{prefix}spe_p50'] = float(np.percentile(spe_arr, 50)) if spe_arr.size else 0.0\n",
    "    out[f'{prefix}spe_p75'] = float(np.percentile(spe_arr, 75)) if spe_arr.size else 0.0\n",
    "    return out\n",
    "\n",
    "def lead_vs_lead_features(p1_lead: dict, p2_lead: dict) -> dict:\n",
    "    out = {}\n",
    "    for s in ['base_hp','base_atk','base_def','base_spa','base_spd','base_spe']:\n",
    "        out[f'lead_diff_{s}'] = float(p1_lead.get(s,0) - p2_lead.get(s,0))\n",
    "    out['lead_speed_advantage'] = float(p1_lead.get('base_spe',0) - p2_lead.get('base_spe',0))\n",
    "    p1_types = [t.lower() for t in p1_lead.get('types', [])]\n",
    "    p2_types = [t.lower() for t in p2_lead.get('types', [])]\n",
    "    max_eff = 0.0\n",
    "    for pt in p1_types: max_eff = max(max_eff, get_effectiveness(pt, p2_types))\n",
    "    out['lead_p1_vs_p2_effectiveness'] = float(max_eff)\n",
    "    return out\n",
    "\n",
    "def lead_aggregate_features(pokemon: dict, prefix: str = 'p2_lead_') -> dict:\n",
    "    out = {}\n",
    "    for s in ['base_hp','base_atk','base_def','base_spa','base_spd','base_spe']:\n",
    "        out[f'{prefix}{s}'] = float(pokemon.get(s,0))\n",
    "    out[f'{prefix}level'] = int(pokemon.get('level',0))\n",
    "    types = [x.lower() for x in pokemon.get('types', [])]\n",
    "    for t in ['normal','fire','water','electric','grass','psychic','ice','dragon','rock','ground','flying']:\n",
    "        out[f'{prefix}type_{t}'] = int(t in types)\n",
    "    out[f'{prefix}name'] = pokemon.get('name','')\n",
    "    out[f'{prefix}n_unique_types'] = int(len(set(types)))\n",
    "    return out\n",
    "\n",
    "def quick_boost_features_v2(record: dict) -> dict:\n",
    "    out = {}\n",
    "    p1_team = record.get('p1_team_details', [])\n",
    "    p2_lead = record.get('p2_lead_details', {})\n",
    "    timeline = record.get('battle_timeline', [])\n",
    "    if not p1_team: return out\n",
    "    \n",
    "    p2_lead_spe = p2_lead.get('base_spe', 0)\n",
    "    faster_count = sum(1 for p in p1_team if p.get('base_spe', 0) > p2_lead_spe)\n",
    "    slower_count = sum(1 for p in p1_team if p.get('base_spe', 0) <= p2_lead_spe)\n",
    "    out['p1_faster_than_lead_count'] = faster_count\n",
    "    out['p1_slower_than_lead_count'] = slower_count\n",
    "    out['p1_speed_control_ratio'] = faster_count / max(1, len(p1_team))\n",
    "    \n",
    "    p1_avg_bulk = np.mean([p.get('base_hp', 0)*(p.get('base_def', 0)+p.get('base_spd', 0)) for p in p1_team])\n",
    "    p2_lead_bulk = p2_lead.get('base_hp', 1)*(p2_lead.get('base_def', 1)+p2_lead.get('base_spd', 1))\n",
    "    out['p1_avg_bulk_vs_lead'] = p1_avg_bulk / max(p2_lead_bulk, 1)\n",
    "    \n",
    "    p1_total_atk = sum(p.get('base_atk', 0) + p.get('base_spa', 0) for p in p1_team)\n",
    "    p2_lead_offense = p2_lead.get('base_atk', 0) + p2_lead.get('base_spa', 0)\n",
    "    out['p1_total_offense'] = p1_total_atk\n",
    "    out['p1_offense_advantage'] = p1_total_atk / max(p2_lead_offense, 1)\n",
    "    \n",
    "    p2_lead_types = [t.lower() for t in p2_lead.get('types', [])]\n",
    "    if p2_lead_types:\n",
    "        coverage_scores = []\n",
    "        for p in p1_team:\n",
    "            p_types = [t.lower() for t in p.get('types', [])]\n",
    "            max_eff = max([get_effectiveness(pt, p2_lead_types) for pt in p_types] or [1.0])\n",
    "            coverage_scores.append(max_eff)\n",
    "        out['p1_avg_effectiveness_vs_lead'] = float(np.mean(coverage_scores))\n",
    "        out['p1_max_effectiveness_vs_lead'] = float(np.max(coverage_scores))\n",
    "        out['p1_se_count_vs_lead'] = sum(1 for s in coverage_scores if s >= 2.0)\n",
    "        out['p1_weak_count_vs_lead'] = sum(1 for s in coverage_scores if s <= 0.5)\n",
    "        \n",
    "    if timeline:\n",
    "        first_p1_ko = False; first_p2_ko = False\n",
    "        for turn in timeline[:30]:\n",
    "            if not first_p2_ko and turn.get('p2_pokemon_state', {}).get('fainted'):\n",
    "                first_p1_ko = True; out['p1_first_blood'] = 1; out['p1_first_blood_turn'] = turn.get('turn', 0); break\n",
    "            if not first_p1_ko and turn.get('p1_pokemon_state', {}).get('fainted'):\n",
    "                first_p2_ko = True; out['p1_first_blood'] = 0; out['p1_first_blood_turn'] = turn.get('turn', 0); break\n",
    "        if not first_p1_ko and not first_p2_ko:\n",
    "            out['p1_first_blood'] = -1; out['p1_first_blood_turn'] = 0\n",
    "            \n",
    "    p1_avg_level = np.mean([p.get('level', 50) for p in p1_team])\n",
    "    out['p1_avg_level_advantage'] = p1_avg_level - p2_lead.get('level', 50)\n",
    "    \n",
    "    p1_stat_products = [(p.get('base_hp',1)*p.get('base_atk',1)*p.get('base_def',1)*p.get('base_spa',1)*p.get('base_spd',1)*p.get('base_spe',1)) for p in p1_team]\n",
    "    out['p1_avg_stat_product'] = float(np.mean(p1_stat_products))\n",
    "    out['p1_max_stat_product'] = float(np.max(p1_stat_products))\n",
    "    p2_prod = p2_lead.get('base_hp',1)*p2_lead.get('base_atk',1)*p2_lead.get('base_def',1)*p2_lead.get('base_spa',1)*p2_lead.get('base_spd',1)*p2_lead.get('base_spe',1)\n",
    "    out['p1_stat_product_advantage'] = out['p1_avg_stat_product'] / max(p2_prod, 1)\n",
    "    return out\n",
    "\n",
    "def summary_from_timeline(timeline: list, p1_team: list) -> dict:\n",
    "    out = {}\n",
    "    if not timeline: return {'tl_p1_moves':0,'tl_p2_moves':0,'tl_p1_est_damage':0.0,'tl_p2_est_damage':0.0,'damage_diff':0.0}\n",
    "    p1_moves = p2_moves = 0; p1_damage = p2_damage = 0.0\n",
    "    p1_last_active = p2_last_active = ''; p1_last_hp = p2_last_hp = np.nan\n",
    "    p1_fainted = p2_fainted = 0\n",
    "    p1_fainted_names = set(); p2_fainted_names = set()\n",
    "    last_p1_hp = {}; last_p2_hp = {}\n",
    "    p1_comeback_kos = p2_comeback_kos = 0\n",
    "    p1_inflicted_statuses = Counter(); p2_inflicted_statuses = Counter()\n",
    "    p1_pokemon_statuses = {}; p2_pokemon_statuses = {}\n",
    "    p1_move_type_counts = Counter(); p2_move_type_counts = Counter()\n",
    "    p1_damage_first2 = 0.0; p2_damage_first2 = 0.0\n",
    "    p1_dmg_by_turn = {}; p2_dmg_by_turn = {}; seen_turns = set()\n",
    "    first_ko_turn_p1_taken = None; first_ko_turn_p1_inflicted = None\n",
    "    early_threshold = 10; p1_kos_early = p1_kos_late = p2_kos_early = p2_kos_late = 0\n",
    "    \n",
    "    # --- NUOVA AGGIUNTA V5: Tracciamento Crit/Turni Persi ---\n",
    "    p1_crit_count = 0; p2_crit_count = 0\n",
    "    p1_lost_turns_status = 0; p2_lost_turns_status = 0\n",
    "\n",
    "    for i, turn in enumerate(timeline[:30]):\n",
    "        prev_p1_fainted, prev_p2_fainted = p1_fainted, p2_fainted\n",
    "        p1_state = turn.get('p1_pokemon_state',{}) or {}; p2_state = turn.get('p2_pokemon_state',{}) or {}\n",
    "        tnum = turn.get('turn', len(seen_turns) + 1); seen_turns.add(tnum)\n",
    "\n",
    "        if p1_state.get('name'): p1_last_active = p1_state.get('name')\n",
    "        if p2_state.get('name'): p2_last_active = p2_state.get('name')\n",
    "\n",
    "        if p1_state.get('fainted') and p1_state.get('name') not in p1_fainted_names:\n",
    "            p1_fainted += 1; p1_fainted_names.add(p1_state.get('name'))\n",
    "            if first_ko_turn_p1_taken is None: first_ko_turn_p1_taken = tnum\n",
    "            if tnum <= early_threshold: p2_kos_early += 1\n",
    "            else: p2_kos_late += 1\n",
    "        if p2_state.get('fainted') and p2_state.get('name') not in p2_fainted_names:\n",
    "            p2_fainted += 1; p2_fainted_names.add(p2_state.get('name'))\n",
    "            if first_ko_turn_p1_inflicted is None: first_ko_turn_p1_inflicted = tnum\n",
    "            if tnum <= early_threshold: p1_kos_early += 1\n",
    "            else: p1_kos_late += 1\n",
    "\n",
    "        p2_name, p2_hp = p2_state.get('name'), p2_state.get('hp_pct')\n",
    "        if p2_name and p2_hp is not None:\n",
    "            prev_hp = last_p2_hp.get(p2_name)\n",
    "            if prev_hp is not None:\n",
    "                delta = max(0.0, prev_hp - p2_hp)\n",
    "                p1_damage += delta\n",
    "                p1_dmg_by_turn[tnum] = p1_dmg_by_turn.get(tnum, 0.0) + delta\n",
    "                if turn.get('turn',999) <= 2: p1_damage_first2 += delta\n",
    "            last_p2_hp[p2_name] = p2_hp\n",
    "\n",
    "        p1_name, p1_hp = p1_state.get('name'), p1_state.get('hp_pct')\n",
    "        if p1_name and p1_hp is not None:\n",
    "            prev_hp = last_p1_hp.get(p1_name)\n",
    "            if prev_hp is not None:\n",
    "                delta = max(0.0, prev_hp - p1_hp)\n",
    "                p2_damage += delta\n",
    "                p2_dmg_by_turn[tnum] = p2_dmg_by_turn.get(tnum, 0.0) + delta\n",
    "                if turn.get('turn',999) <= 2: p2_damage_first2 += delta\n",
    "            last_p1_hp[p1_name] = p1_hp\n",
    "\n",
    "        damage_diff_so_far = p1_damage - p2_damage\n",
    "        if p2_fainted > prev_p2_fainted and damage_diff_so_far < -1.0: p1_comeback_kos += 1\n",
    "        if p1_fainted > prev_p1_fainted and damage_diff_so_far > 1.0: p2_comeback_kos += 1\n",
    "\n",
    "        p2_status = p2_state.get('status')\n",
    "        if p2_name and p2_status and p2_pokemon_statuses.get(p2_name) != p2_status:\n",
    "            p1_inflicted_statuses[p2_status] += 1; p2_pokemon_statuses[p2_name] = p2_status\n",
    "        p1_status = p1_state.get('status')\n",
    "        if p1_name and p1_status and p1_pokemon_statuses.get(p1_name) != p1_status:\n",
    "            p2_inflicted_statuses[p1_status] += 1; p1_pokemon_statuses[p1_name] = p1_status\n",
    "\n",
    "        p1_move = turn.get('p1_move_details') or {}; p2_move = turn.get('p2_move_details') or {}\n",
    "        if p1_move and p1_move.get('type'): p1_move_type_counts[(p1_move.get('type') or '').lower()] += 1\n",
    "        if p2_move and p2_move.get('type'): p2_move_type_counts[(p2_move.get('type') or '').lower()] += 1\n",
    "        if p1_move: p1_moves += 1\n",
    "        if p2_move: p2_moves += 1\n",
    "        \n",
    "        # --- NUOVA AGGIUNTA V5: Logica Crit/Turni Persi ---\n",
    "        if p1_move.get('critical_hit', False): p1_crit_count += 1\n",
    "        if p2_move.get('critical_hit', False): p2_crit_count += 1\n",
    "        \n",
    "        prev_p1_name = timeline[i-1].get('p1_pokemon_state',{}).get('name') if i > 0 else None\n",
    "        if p1_state.get('status') in ['par', 'slp'] and not p1_move and p1_state.get('name') == prev_p1_name:\n",
    "            p1_lost_turns_status += 1\n",
    "            \n",
    "        prev_p2_name = timeline[i-1].get('p2_pokemon_state',{}).get('name') if i > 0 else None\n",
    "        if p2_state.get('status') in ['par', 'slp'] and not p2_move and p2_state.get('name') == prev_p2_name:\n",
    "            p2_lost_turns_status += 1\n",
    "            \n",
    "        p1_last_hp = p1_state.get('hp_pct', np.nan); p2_last_hp = p2_state.get('hp_pct', np.nan)\n",
    "    \n",
    "    # --- Fine Loop Timeline ---\n",
    "\n",
    "    out['tl_p1_moves'] = int(p1_moves); out['tl_p2_moves'] = int(p2_moves)\n",
    "    out['tl_p1_est_damage'] = float(p1_damage); out['tl_p2_est_damage'] = float(p2_damage)\n",
    "    out['tl_p1_fainted'] = int(p1_fainted); out['tl_p2_fainted'] = int(p2_fainted)\n",
    "    turns_count = max(1, len(seen_turns))\n",
    "    out['tl_p1_fainted_rate'] = float(out['tl_p1_fainted'] / turns_count)\n",
    "    out['tl_p2_fainted_rate'] = float(out['tl_p2_fainted'] / turns_count)\n",
    "    out['damage_diff'] = float(p1_damage - p2_damage)\n",
    "    out['fainted_diff'] = int(p1_fainted - p2_fainted)\n",
    "    out['tl_p1_last_hp'] = float(p1_last_hp) if not np.isnan(p1_last_hp) else 0.0\n",
    "    out['tl_p2_last_hp'] = float(p2_last_hp) if not np.isnan(p2_last_hp) else 0.0\n",
    "    out['tl_p1_last_active'] = p1_last_active; out['tl_p2_last_active'] = p2_last_active\n",
    "    \n",
    "    if p1_team:\n",
    "        p1_total_hp_sum = sum(p.get('base_hp',0) for p in p1_team)\n",
    "        p1_avg_def = np.mean([p.get('base_def',0) for p in p1_team] or [0])\n",
    "        p1_avg_spd = np.mean([p.get('base_spd',0) for p in p1_team] or [0])\n",
    "        out['tl_p2_damage_vs_p1_hp_pool'] = float(p2_damage / (p1_total_hp_sum + 1e-6))\n",
    "        out['tl_p1_defensive_endurance'] = float((p1_avg_def + p1_avg_spd) / (p2_damage + 1e-6))\n",
    "        \n",
    "    out['tl_p1_comeback_kos'] = int(p1_comeback_kos); out['tl_p2_comeback_kos'] = int(p2_comeback_kos)\n",
    "    out['tl_comeback_kos_diff'] = int(p1_comeback_kos - p2_comeback_kos)\n",
    "\n",
    "    common_statuses = ['brn','par','slp','frz','psn','tox']\n",
    "    for status in common_statuses:\n",
    "        out[f'tl_p1_inflicted_{status}_count'] = int(p1_inflicted_statuses.get(status,0))\n",
    "        out[f'tl_p2_inflicted_{status}_count'] = int(p2_inflicted_statuses.get(status,0))\n",
    "        out[f'tl_inflicted_{status}_diff'] = int(p1_inflicted_statuses.get(status,0) - p2_inflicted_statuses.get(status,0))\n",
    "        c1 = p1_inflicted_statuses.get(status,0); c2 = p2_inflicted_statuses.get(status,0)\n",
    "        out[f'tl_p1_inflicted_{status}_rate'] = float(c1 / turns_count)\n",
    "        out[f'tl_p2_inflicted_{status}_rate'] = float(c2 / turns_count)\n",
    "        out[f'tl_inflicted_{status}_rate_diff'] = float((c1 - c2) / turns_count)\n",
    "\n",
    "    common_move_types = ['normal','fire','water','electric','grass','psychic','ice','dragon','rock','ground','flying','ghost','bug','poison','fighting']\n",
    "    for mt in common_move_types:\n",
    "        out[f'tl_p1_move_type_{mt}_count'] = int(p1_move_type_counts.get(mt,0))\n",
    "        out[f'tl_p2_move_type_{mt}_count'] = int(p2_move_type_counts.get(mt,0))\n",
    "        out[f'tl_move_type_{mt}_count_diff'] = int(p1_move_type_counts.get(mt,0) - p2_move_type_counts.get(mt,0))\n",
    "\n",
    "    out['tl_p1_damage_first2'] = float(p1_damage_first2)\n",
    "    out['tl_p2_damage_first2'] = float(p2_damage_first2)\n",
    "    out['tl_first2_damage_diff'] = float(p1_damage_first2 - p2_damage_first2)\n",
    "    out['tl_turns_count'] = int(turns_count)\n",
    "    out['tl_p1_moves_rate'] = float(p1_moves / turns_count); out['tl_p2_moves_rate'] = float(p2_moves / turns_count)\n",
    "    out['tl_p1_damage_per_turn'] = float(p1_damage / turns_count); out['tl_p2_damage_per_turn'] = float(p2_damage / turns_count)\n",
    "    out['tl_damage_rate_diff'] = float(out['tl_p1_damage_per_turn'] - out['tl_p2_damage_per_turn'])\n",
    "\n",
    "    recent_turns = sorted(seen_turns)[-5:] if seen_turns else []\n",
    "    p1_last5 = sum(p1_dmg_by_turn.get(t,0.0) for t in recent_turns)\n",
    "    p2_last5 = sum(p2_dmg_by_turn.get(t,0.0) for t in recent_turns)\n",
    "    out['tl_p1_damage_last5'] = float(p1_last5); out['tl_p2_damage_last5'] = float(p2_last5)\n",
    "    out['tl_last5_damage_diff'] = float(p1_last5 - p2_last5)\n",
    "    out['tl_p1_last5_damage_ratio'] = float(p1_last5 / (p1_damage + 1e-6))\n",
    "    out['tl_p2_last5_damage_ratio'] = float(p2_last5 / (p2_damage + 1e-6))\n",
    "    out['tl_last5_damage_ratio_diff'] = float(out['tl_p1_last5_damage_ratio'] - out['tl_p2_last5_damage_ratio'])\n",
    "\n",
    "    if seen_turns:\n",
    "        ts = sorted(seen_turns); w = np.linspace(1.0, 2.0, num=len(ts)); w = w / (w.sum() + 1e-9)\n",
    "        adv = [(p1_dmg_by_turn.get(t,0.0) - p2_dmg_by_turn.get(t,0.0)) for t in ts]\n",
    "        out['tl_weighted_damage_diff'] = float(np.dot(w, adv))\n",
    "        cum = 0.0; signs = []\n",
    "        for t in ts:\n",
    "            cum += (p1_dmg_by_turn.get(t,0.0) - p2_dmg_by_turn.get(t,0.0))\n",
    "            s = 1 if cum > 1e-9 else (-1 if cum < -1e-9 else 0)\n",
    "            if s != 0 and (not signs or signs[-1] != s): signs.append(s)\n",
    "        out['tl_damage_adv_sign_flips'] = int(max(0, len(signs) - 1))\n",
    "        out['tl_comeback_flag'] = int(1 if (len(signs) >= 2 and signs[0] != signs[-1]) else 0)\n",
    "    else:\n",
    "        out['tl_weighted_damage_diff'] = 0.0; out['tl_damage_adv_sign_flips'] = 0; out['tl_comeback_flag'] = 0\n",
    "\n",
    "    out['tl_first_ko_turn_p1_inflicted'] = int(first_ko_turn_p1_inflicted or 0)\n",
    "    out['tl_first_ko_turn_p1_taken'] = int(first_ko_turn_p1_taken or 0)\n",
    "    out['tl_first_ko_turn_diff'] = int((first_ko_turn_p1_inflicted or 0) - (first_ko_turn_p1_taken or 0))\n",
    "    out['tl_kos_early_p1'] = int(p1_kos_early); out['tl_kos_late_p1'] = int(p1_kos_late)\n",
    "    out['tl_kos_early_p2'] = int(p2_kos_early); out['tl_kos_late_p2'] = int(p2_kos_late)\n",
    "\n",
    "    # --- NUOVA AGGIUNTA V5: Salva risultati RNG/Hax ---\n",
    "    out['tl_p1_crit_count'] = int(p1_crit_count)\n",
    "    out['tl_p2_crit_count'] = int(p2_crit_count)\n",
    "    out['tl_crit_diff'] = int(p1_crit_count - p2_crit_count)\n",
    "    out['tl_p1_lost_turns_status'] = int(p1_lost_turns_status)\n",
    "    out['tl_p2_lost_turns_status'] = int(p2_lost_turns_status)\n",
    "    out['tl_status_luck_diff'] = int(p2_lost_turns_status - p1_lost_turns_status)\n",
    "    \n",
    "    return out\n",
    "\n",
    "def extract_move_coverage_from_timeline(timeline: list, prefix: str = 'p1_') -> dict:\n",
    "    out = {}; move_types_used = set(); move_categories_used = Counter()\n",
    "    unique_moves = set(); stab_count = 0\n",
    "    for turn in timeline[:30]:\n",
    "        move_details = turn.get(f'{prefix[:-1]}_move_details')\n",
    "        pokemon_state = turn.get(f'{prefix[:-1]}_pokemon_state', {})\n",
    "        if not move_details: continue\n",
    "        move_name = move_details.get('name', ''); move_type = (move_details.get('type') or '').lower()\n",
    "        move_category = move_details.get('category', '')\n",
    "        if move_name: unique_moves.add(move_name)\n",
    "        if move_type: move_types_used.add(move_type)\n",
    "        if move_category: move_categories_used[move_category] += 1\n",
    "        if move_type in [t.lower() for t in pokemon_state.get('types', [])]: stab_count += 1\n",
    "    \n",
    "    out[f'{prefix}tl_unique_move_types'] = len(move_types_used)\n",
    "    out[f'{prefix}tl_unique_moves_used'] = len(unique_moves)\n",
    "    out[f'{prefix}tl_stab_moves'] = stab_count\n",
    "    out[f'{prefix}tl_physical_moves'] = move_categories_used.get('physical', 0)\n",
    "    out[f'{prefix}tl_special_moves'] = move_categories_used.get('special', 0)\n",
    "    out[f'{prefix}tl_status_moves'] = move_categories_used.get('status', 0)\n",
    "    out[f'{prefix}tl_coverage_score'] = len(move_types_used) / max(1, len(unique_moves))\n",
    "    total_moves = sum(move_categories_used.values())\n",
    "    if total_moves > 0:\n",
    "        out[f'{prefix}tl_offensive_ratio'] = (move_categories_used.get('physical',0)+move_categories_used.get('special',0)) / total_moves\n",
    "        out[f'{prefix}tl_status_ratio'] = move_categories_used.get('status', 0) / total_moves\n",
    "    else:\n",
    "        out[f'{prefix}tl_offensive_ratio'] = 0.0; out[f'{prefix}tl_status_ratio'] = 0.0\n",
    "    return out\n",
    "\n",
    "def ability_features(team: list, prefix: str) -> dict:\n",
    "    immunity_abilities = {'levitate':0,'volt_absorb':0,'water_absorb':0,'flash_fire':0}\n",
    "    stat_drop_abilities = {'intimidate':0}; weather_abilities = {'drought':0,'drizzle':0,'sand_stream':0}\n",
    "    out = {}\n",
    "    for pokemon in team:\n",
    "        ability = (pokemon.get('ability','') or '').lower().replace(' ','_')\n",
    "        if ability in immunity_abilities: immunity_abilities[ability] += 1\n",
    "        if ability in stat_drop_abilities: stat_drop_abilities[ability] += 1\n",
    "        if ability in weather_abilities: weather_abilities[ability] += 1\n",
    "    for ability,count in immunity_abilities.items(): out[f'{prefix}ability_{ability}_count'] = int(count)\n",
    "    for ability,count in stat_drop_abilities.items(): out[f'{prefix}ability_{ability}_count'] = int(count)\n",
    "    for ability,count in weather_abilities.items(): out[f'{prefix}ability_{ability}_count'] = int(count)\n",
    "    out[f'{prefix}total_immunity_abilities'] = int(sum(immunity_abilities.values()))\n",
    "    out[f'{prefix}total_stat_drop_abilities'] = int(sum(stat_drop_abilities.values()))\n",
    "    return out\n",
    "\n",
    "def momentum_features(timeline: list) -> dict:\n",
    "    out = {}; p1_advantages = []; cumulative_advantage = 0.0\n",
    "    if not timeline: return out\n",
    "    for i, turn in enumerate(timeline[:30]):\n",
    "        p1_hp = turn.get('p1_pokemon_state', {}).get('hp_pct', 100)\n",
    "        p2_hp = turn.get('p2_pokemon_state', {}).get('hp_pct', 100)\n",
    "        turn_advantage = p1_hp - p2_hp\n",
    "        cumulative_advantage += turn_advantage; p1_advantages.append(cumulative_advantage)\n",
    "    if p1_advantages:\n",
    "        x = np.arange(len(p1_advantages)); slope, intercept = np.polyfit(x, p1_advantages, 1)\n",
    "        out['p1_momentum_slope'] = float(slope); out['p1_momentum_intercept'] = float(intercept)\n",
    "        out['p1_final_advantage'] = float(p1_advantages[-1])\n",
    "        out['p1_advantage_volatility'] = float(np.std(p1_advantages))\n",
    "        out['p1_max_advantage'] = float(np.max(p1_advantages)); out['p1_min_advantage'] = float(np.min(p1_advantages))\n",
    "        out['p1_advantage_range'] = float(out['p1_max_advantage'] - out['p1_min_advantage'])\n",
    "    return out\n",
    "\n",
    "def extract_opponent_team_from_timeline(timeline: list, p1_team: list) -> dict:\n",
    "    out = {}; p2_pokemon_seen = set(); p2_pokemon_types = []\n",
    "    for turn in timeline[:30]:\n",
    "        p2_state = turn.get('p2_pokemon_state', {})\n",
    "        if not p2_state: continue\n",
    "        p2_name = p2_state.get('name')\n",
    "        if p2_name and p2_name not in p2_pokemon_seen:\n",
    "            p2_pokemon_seen.add(p2_name)\n",
    "            p2_pokemon_types.extend([t.lower() for t in p2_state.get('types', [])])\n",
    "    \n",
    "    out['p2_tl_unique_pokemon_seen'] = len(p2_pokemon_seen)\n",
    "    out['p2_tl_switches_count'] = len(p2_pokemon_seen) - 1\n",
    "    p2_type_counter = Counter(p2_pokemon_types)\n",
    "    out['p2_tl_unique_types_seen'] = len(p2_type_counter)\n",
    "    out['p2_tl_type_entropy'] = _entropy(p2_type_counter)\n",
    "    \n",
    "    if p2_pokemon_types and p1_team:\n",
    "        matchup_advantages = 0\n",
    "        for p1_poke in p1_team:\n",
    "            p1_types = [t.lower() for t in p1_poke.get('types', [])]\n",
    "            for p1_type in p1_types:\n",
    "                for p2_type in set(p2_pokemon_types):\n",
    "                    eff = get_effectiveness(p1_type, [p2_type])\n",
    "                    if eff >= 2.0: matchup_advantages += 1\n",
    "        out['p1_vs_p2_tl_type_advantages'] = matchup_advantages\n",
    "        out['p1_vs_p2_tl_type_advantages_per_poke'] = matchup_advantages / max(1, len(p1_team))\n",
    "    \n",
    "    total_turns = len(timeline[:30])\n",
    "    out['p2_tl_switch_rate'] = len(p2_pokemon_seen) / max(1, total_turns)\n",
    "    return out\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. NUOVE FEATURE AVANZATE (DATA-DRIVEN, V4/V5)\n",
    "# ==============================================================================\n",
    "\n",
    "def extract_information_advantage(timeline: list) -> dict:\n",
    "    p1_rev = set(); p2_rev = set(); reveal_turns_p2 = []\n",
    "    for turn in timeline[:30]:\n",
    "        t = turn.get('turn', 0)\n",
    "        if n1 := turn.get('p1_pokemon_state', {}).get('name'): p1_rev.add(n1)\n",
    "        if n2 := turn.get('p2_pokemon_state', {}).get('name'):\n",
    "            if n2 not in p2_rev: p2_rev.add(n2); reveal_turns_p2.append(t)\n",
    "    return {\n",
    "        'tl_p1_revealed_count': len(p1_rev),\n",
    "        'tl_p2_revealed_count': len(p2_rev),\n",
    "        'tl_info_advantage': len(p2_rev) - len(p1_rev),\n",
    "        'tl_p2_avg_reveal_turn': float(np.mean(reveal_turns_p2)) if reveal_turns_p2 else 30.0\n",
    "    }\n",
    "\n",
    "def extract_advanced_momentum(timeline: list) -> dict:\n",
    "    p1_immune = 0; p2_forced = 0\n",
    "    for i, turn in enumerate(timeline[:30]):\n",
    "        if i == 0: continue\n",
    "        prev = timeline[i-1]\n",
    "        c1 = turn.get('p1_pokemon_state', {}).get('name')\n",
    "        p1 = prev.get('p1_pokemon_state', {}).get('name')\n",
    "        if c1 != p1 and not prev.get('p1_pokemon_state', {}).get('fainted'):\n",
    "            m2_type = (turn.get('p2_move_details') or {}).get('type', '').lower()\n",
    "            p1_types = [t.lower() for t in turn.get('p1_pokemon_state', {}).get('types', [])]\n",
    "            if m2_type and p1_types and get_effectiveness(m2_type, p1_types) == 0.0:\n",
    "                p1_immune += 1\n",
    "        c2 = turn.get('p2_pokemon_state', {}).get('name')\n",
    "        p2 = prev.get('p2_pokemon_state', {}).get('name')\n",
    "        if c2 != p2 and not prev.get('p2_pokemon_state', {}).get('fainted'):\n",
    "            if prev.get('p2_pokemon_state', {}).get('hp_pct', 1.0) < 0.50:\n",
    "                p2_forced += 1\n",
    "    return {'tl_p1_immune_switches': p1_immune, 'tl_p2_forced_switches': p2_forced}\n",
    "\n",
    "def extract_gamestate_snapshots(timeline: list) -> dict:\n",
    "    turns_lead = 0; hp_diff_10 = 0.0; hp_diff_20 = 0.0; hp_diff_end = 0.0\n",
    "    for i, turn in enumerate(timeline[:30]):\n",
    "        t = i + 1\n",
    "        h1 = turn.get('p1_pokemon_state', {}).get('hp_pct', 0)\n",
    "        h2 = turn.get('p2_pokemon_state', {}).get('hp_pct', 0)\n",
    "        if h1 > h2: turns_lead += 1\n",
    "        if t == 10: hp_diff_10 = h1 - h2\n",
    "        if t == 20: hp_diff_20 = h1 - h2\n",
    "        hp_diff_end = h1 - h2 # Ultimo stato disponibile\n",
    "    return {\n",
    "        'tl_turns_with_hp_lead': turns_lead,\n",
    "        'tl_hp_diff_turn_10': float(hp_diff_10),\n",
    "        'tl_hp_diff_turn_20': float(hp_diff_20),\n",
    "        'tl_hp_diff_end': float(hp_diff_end)\n",
    "    }\n",
    "\n",
    "def extract_observed_mechanics(timeline: list) -> dict:\n",
    "    p1_heals = 0; p2_heals = 0; p1_frz = 0; p2_frz = 0\n",
    "    for i, turn in enumerate(timeline[:30]):\n",
    "        if i == 0: continue\n",
    "        prev = timeline[i-1]\n",
    "        p1s = turn.get('p1_pokemon_state', {}); p1s_prev = prev.get('p1_pokemon_state', {})\n",
    "        p2s = turn.get('p2_pokemon_state', {}); p2s_prev = prev.get('p2_pokemon_state', {})\n",
    "        if p1s.get('name') == p1s_prev.get('name'):\n",
    "             if p1s.get('hp_pct', 0) > p1s_prev.get('hp_pct', 0): p1_heals += 1\n",
    "        if p2s.get('name') == p2s_prev.get('name'):\n",
    "             if p2s.get('hp_pct', 0) > p2s_prev.get('hp_pct', 0): p2_heals += 1\n",
    "        if p1s.get('status') == 'frz': p1_frz = 1\n",
    "        if p2s.get('status') == 'frz': p2_frz = 1\n",
    "    return {'tl_heal_diff': p1_heals - p2_heals, 'tl_freeze_adv': p2_frz - p1_frz}\n",
    "\n",
    "# --- NUOVE FEATURE V5 (RUOLI E COESIONE) ---\n",
    "\n",
    "def team_role_features(team: list, prefix: str = 'p1_') -> dict:\n",
    "    \"\"\"Estrae gli specialisti del team (Muro, Sweeper, etc.)\"\"\"\n",
    "    if not team: return {}\n",
    "    \n",
    "    spe_list = []; bulk_list = []; offense_list = []\n",
    "    for p in team:\n",
    "        spe_list.append(p.get('base_spe', 0))\n",
    "        bulk_list.append(p.get('base_hp', 1) * (p.get('base_def', 1) + p.get('base_spd', 1)))\n",
    "        offense_list.append(p.get('base_atk', 1) + p.get('base_spa', 1))\n",
    "        \n",
    "    return {\n",
    "        f'{prefix}fastest_spe': float(np.max(spe_list)),\n",
    "        f'{prefix}slowest_spe': float(np.min(spe_list)),\n",
    "        f'{prefix}max_bulk': float(np.max(bulk_list)),\n",
    "        f'{prefix}max_offense': float(np.max(offense_list))\n",
    "    }\n",
    "\n",
    "def calculate_defensive_cohesion(team: list, prefix: str = 'p1_') -> dict:\n",
    "    \"\"\"Calcola quanto è debole il team a un singolo tipo (max debolezza comune)\"\"\"\n",
    "    if not team: return {}\n",
    "    \n",
    "    weakness_counts = Counter()\n",
    "    for atk_type in ALL_ATTACK_TYPES:\n",
    "        count = 0\n",
    "        for p in team:\n",
    "            def_types = [t.lower() for t in p.get('types', [])]\n",
    "            if not def_types: continue\n",
    "            if get_effectiveness(atk_type, def_types) >= 2.0:\n",
    "                count += 1\n",
    "        weakness_counts[atk_type] = count\n",
    "        \n",
    "    return {\n",
    "        f'{prefix}max_common_weakness': float(max(weakness_counts.values()))\n",
    "    }\n",
    "\n",
    "def role_vs_lead_comparison(p1_roles: dict, p2_lead: dict) -> dict:\n",
    "    \"\"\"Confronta gli specialisti P1 con il lead P2\"\"\"\n",
    "    out = {}\n",
    "    p2_lead_spe = p2_lead.get('base_spe', 0)\n",
    "    p2_lead_offense = p2_lead.get('base_atk', 1) + p2_lead.get('base_spa', 1)\n",
    "    \n",
    "    out['p1_fastest_vs_lead_spe'] = p1_roles.get('p1_fastest_spe', 0) - p2_lead_spe\n",
    "    out['p1_max_bulk_vs_lead_offense'] = p1_roles.get('p1_max_bulk', 1) / max(1, p2_lead_offense)\n",
    "    return out\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# 5. MASTER FUNCTION (TUTTO INCLUSO)\n",
    "# ==============================================================================\n",
    "\n",
    "def prepare_record_features_COMPLETE_V5(record: dict, max_turns: int = 30) -> dict:\n",
    "    out = {'battle_id': record.get('battle_id')}\n",
    "    if 'player_won' in record: out['player_won'] = int(bool(record['player_won']))\n",
    "    \n",
    "    p1_team = record.get('p1_team_details', [])\n",
    "    p2_lead = record.get('p2_lead_details', {})\n",
    "    p1_lead = p1_team[0] if p1_team else {}\n",
    "    tl = record.get('battle_timeline', [])\n",
    "    tl_limited = tl[:max_turns]\n",
    "    \n",
    "    # --- V1: Feature Originali Statiche ---\n",
    "    out.update(team_aggregate_features(p1_team, 'p1_'))\n",
    "    out.update(lead_aggregate_features(p2_lead, 'p2_lead_'))\n",
    "    out.update(ability_features(p1_team, 'p1_'))\n",
    "    out.update(lead_vs_lead_features(p1_lead, p2_lead))\n",
    "    out.update(ability_features([p2_lead], 'p2_lead_'))\n",
    "    out['p1_intimidate_vs_lead'] = int(out.get('p1_ability_intimidate_count',0) > 0)\n",
    "    \n",
    "    # --- V1: Feature Originali Dinamiche ---\n",
    "    out.update(summary_from_timeline(tl_limited, p1_team))\n",
    "    out.update(extract_move_coverage_from_timeline(tl_limited, 'p1_'))\n",
    "    out.update(extract_move_coverage_from_timeline(tl_limited, 'p2_'))\n",
    "    out.update(extract_opponent_team_from_timeline(tl_limited, p1_team))\n",
    "    out.update(quick_boost_features_v2(record)) # quick_boost usa 'record'\n",
    "    out.update(momentum_features(tl_limited))\n",
    "    \n",
    "    # --- V1: Feature Originali Calcolate a Mano ---\n",
    "    out['team_hp_sum_minus_p2lead_hp'] = out.get('p1_base_hp_sum', 0) - out.get('p2_lead_base_hp', 0)\n",
    "    out['team_spa_mean_minus_p2spa'] = out.get('p1_base_spa_mean', 0) - out.get('p2_lead_base_spa', 0)\n",
    "    out['speed_advantage'] = out.get('p1_base_spe_sum', 0) - out.get('p2_lead_base_spe', 0)\n",
    "    out['n_unique_types_diff'] = out.get('p1_n_unique_types', 0) - out.get('p2_lead_n_unique_types', 1)\n",
    "    p1_moves = max(out.get('tl_p1_moves',1),1); p2_moves = max(out.get('tl_p2_moves',1),1)\n",
    "    out['damage_per_turn_diff'] = (out.get('tl_p1_est_damage',0.0)/p1_moves) - (out.get('tl_p2_est_damage',0.0)/p2_moves)\n",
    "    out['last_pair'] = f\"{out.get('tl_p1_last_active','')}_VS_{out.get('tl_p2_last_active','')}\"\n",
    "    out.update(calculate_type_advantage(p1_team, p2_lead))\n",
    "    p2_lead_bulk = out.get('p2_lead_base_def',1) + out.get('p2_lead_base_spd',1)\n",
    "    out['p1_se_options_vs_lead_bulk'] = out.get('p1_super_effective_options',0) / (p2_lead_bulk + 1e-6)\n",
    "    \n",
    "    if p2_team := record.get('p2_team_details', []):\n",
    "        out.update(team_aggregate_features(p2_team, 'p2_'))\n",
    "        out['team_hp_sum_diff'] = out.get('p1_base_hp_sum',0) - out.get('p2_base_hp_sum',0)\n",
    "        out['team_spa_mean_diff'] = out.get('p1_base_spa_mean',0) - out.get('p2_base_spa_mean',0)\n",
    "        out['team_spe_mean_diff'] = out.get('p1_base_spe_mean',0) - out.get('p2_base_spe_mean',0)\n",
    "        out['n_unique_types_team_diff'] = out.get('p1_n_unique_types',0) - out.get('p2_n_unique_types',0)\n",
    "        \n",
    "    # --- NUOVE FEATURE (V2/V4) ---\n",
    "    if tl_limited:\n",
    "        out.update(extract_information_advantage(tl_limited))\n",
    "        out.update(extract_advanced_momentum(tl_limited))\n",
    "        out.update(extract_gamestate_snapshots(tl_limited))\n",
    "        out.update(extract_observed_mechanics(tl_limited))\n",
    "    else:\n",
    "        out.update({\n",
    "            'tl_p1_revealed_count': 1, 'tl_p2_revealed_count': 1, 'tl_info_advantage': 0,\n",
    "            'tl_p2_avg_reveal_turn': 30.0, 'tl_p1_immune_switches': 0, 'tl_p2_forced_switches': 0,\n",
    "            'tl_turns_with_hp_lead': 0, 'tl_hp_diff_turn_10': 0.0, 'tl_hp_diff_turn_20': 0.0,\n",
    "            'tl_hp_diff_end': 0.0, 'tl_heal_diff': 0, 'tl_freeze_adv': 0\n",
    "        })\n",
    "        \n",
    "    # --- NUOVE FEATURE (V5) ---\n",
    "    p1_role_feats = team_role_features(p1_team, 'p1_')\n",
    "    out.update(p1_role_feats)\n",
    "    out.update(calculate_defensive_cohesion(p1_team, 'p1_'))\n",
    "    out.update(role_vs_lead_comparison(p1_role_feats, p2_lead))\n",
    "        \n",
    "    return out\n",
    "\n",
    "def create_features_from_raw(data: list, feature_func=prepare_record_features_COMPLETE_V5) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    for b in tqdm(data, desc='FE (V5 Complete)'):\n",
    "        try:\n",
    "            feat = feature_func(b, max_turns=30)\n",
    "            if 'battle_id' not in feat: feat['battle_id'] = b.get('battle_id')\n",
    "            rows.append(feat)\n",
    "        except Exception as e:\n",
    "            rows.append({'battle_id': b.get('battle_id'), 'error': 1})\n",
    "    df = pd.DataFrame(rows)\n",
    "    if 'player_won' in df.columns:\n",
    "        df['player_won'] = df['player_won'].astype(int)\n",
    "    return df.fillna(0)\n",
    "\n",
    "# ==============================================================================\n",
    "# 5. ESECUZIONE\n",
    "# ==============================================================================\n",
    "train_df = create_features_from_raw(train_raw)\n",
    "test_df = create_features_from_raw(test_raw)\n",
    "print('Feature shape train/test:', train_df.shape, test_df.shape)\n",
    "display(train_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf62000",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f2cede4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num FEATURES numeriche rilevate (ALL): 300\n",
      "Num FEATURES effettive usate (FEATURES): 300\n",
      "Num TOP100 caricate: 0\n",
      "Preprocessing (no transformers) completato.\n",
      "Dataset completo size: 10000\n",
      "Preprocessed feature count: 300\n"
     ]
    }
   ],
   "source": [
    "# ====== Preprocessing (senza transformer sklearn) =========\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# base exclusions\n",
    "exclude_cols = ['battle_id', 'player_won']\n",
    "string_cols = train_df.select_dtypes(include=['object']).columns.tolist()\n",
    "exclude_cols.extend(string_cols)\n",
    "\n",
    "# tutte le colonne numeriche candidate\n",
    "ALL_NUMERIC_FEATURES = [c for c in train_df.columns if c not in exclude_cols]\n",
    "\n",
    "# flag per usare top features se necessario\n",
    "use_top_features = False\n",
    "\n",
    "# carica TOP100 se presente (comportamento invariato)\n",
    "top100_path = r'top100_shap_features.csv'\n",
    "try:\n",
    "    top100_df = pd.read_csv(top100_path)\n",
    "    TOP100 = [str(x).strip() for x in top100_df['feature'].tolist()]\n",
    "except Exception:\n",
    "    TOP100 = []\n",
    "\n",
    "# --- INIZIO: filtro dalle keep_features_list se richiesto ---\n",
    "features_filter = False  # imposta True per applicare il filtro, False per comportamento attuale\n",
    "keep_list_path = 'keep_features_list.txt'\n",
    "\n",
    "if features_filter:\n",
    "    try:\n",
    "        import os\n",
    "        if os.path.exists(keep_list_path):\n",
    "            keep_df = pd.read_csv(keep_list_path, header=None)\n",
    "            keep_list = [str(x).strip() for x in keep_df.iloc[:, 0].tolist()]\n",
    "            # mantieni solo feature numeriche valide presenti in ALL_NUMERIC_FEATURES\n",
    "            filtered = [f for f in ALL_NUMERIC_FEATURES if f in keep_list]\n",
    "            if filtered:\n",
    "                # sovrascrive FEATURES più avanti: qui memorizziamo in temp\n",
    "                FEATURES_FROM_KEEP = filtered\n",
    "                print(f\"features_filter=ON: trovato {len(filtered)} feature valide in {keep_list_path}\")\n",
    "            else:\n",
    "                FEATURES_FROM_KEEP = None\n",
    "                print(f\"features_filter=ON: nessuna feature di {keep_list_path} presente in ALL_NUMERIC_FEATURES\")\n",
    "        else:\n",
    "            FEATURES_FROM_KEEP = None\n",
    "            print(f\"features_filter=ON ma file {keep_list_path} non trovato. Nessun filtro applicato.\")\n",
    "    except Exception as e:\n",
    "        FEATURES_FROM_KEEP = None\n",
    "        print(\"Errore caricando keep_features_list.txt, nessun filtro applicato:\", e)\n",
    "else:\n",
    "    FEATURES_FROM_KEEP = None\n",
    "# --- FINE: filtro dalle keep_features_list ---\n",
    "\n",
    "if use_top_features and TOP100:\n",
    "    FEATURES = [f for f in TOP100 if f in ALL_NUMERIC_FEATURES]\n",
    "elif features_filter:\n",
    "    FEATURES = FEATURES_FROM_KEEP\n",
    "else:\n",
    "    FEATURES = ALL_NUMERIC_FEATURES\n",
    "\n",
    "print(f'Num FEATURES numeriche rilevate (ALL): {len(ALL_NUMERIC_FEATURES)}')\n",
    "print(f'Num FEATURES effettive usate (FEATURES): {len(FEATURES)}')\n",
    "print(f'Num TOP100 caricate: {len(TOP100)}')\n",
    "\n",
    "# costruisco DataFrame numerico raw\n",
    "num_df = train_df[FEATURES].astype(float).replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "# Imputazione semplice: usiamo la mediana per ogni feature calcolata sul train\n",
    "medians = num_df.median()\n",
    "train_imputed = num_df.fillna(medians)\n",
    "\n",
    "# NON eseguo alcuno scaling: lascio i valori nella loro scala naturale\n",
    "train_preproc_df = train_imputed.copy()\n",
    "\n",
    "# target\n",
    "y = train_df['player_won'].astype(int).values\n",
    "\n",
    "# MODIFICATO: usa tutto il dataset per CV, nessun holdout\n",
    "X = train_preproc_df.values\n",
    "print('Preprocessing (no transformers) completato.')\n",
    "print('Dataset completo size:', X.shape[0])\n",
    "print('Preprocessed feature count:', len(FEATURES))\n",
    "\n",
    "# Allinea e imputa test_df usando le mediane del train (coerente con l'imputazione sopra)\n",
    "test_aligned = test_df.reindex(columns=FEATURES, fill_value=np.nan).astype(float).replace([np.inf, -np.inf], np.nan)\n",
    "test_imputed = test_aligned.fillna(medians)\n",
    "test_preproc_df = pd.DataFrame(test_imputed.values, columns=FEATURES, index=test_df.index)\n",
    "\n",
    "# Variabili pronte per le celle successive:\n",
    "# FEATURES, X, y, test_preproc_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb05f0e",
   "metadata": {},
   "source": [
    "# Hyperparameter serch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "239a614a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-09 12:51:51,956] A new study created in memory with name: no-name-4c7854ab-d063-4f84-bb78-6d74304f305b\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avvio ottimizzazione Optuna per 150 trial (o 7200 secondi)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "575ceabe55ec4463a390faf395994f0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-09 12:51:54,780] Trial 0 finished with value: 0.8295999999999999 and parameters: {'learning_rate': 0.057443186136344686, 'max_depth': 5, 'min_child_weight': 1, 'gamma': 4.430714555554042, 'subsample': 0.8668876182416037, 'colsample_bytree': 0.7135505134979183, 'reg_alpha': 0.020893581492795237, 'reg_lambda': 0.5280998338916184}. Best is trial 0 with value: 0.8295999999999999.\n",
      "[I 2025-11-09 12:52:00,446] Trial 1 finished with value: 0.8295 and parameters: {'learning_rate': 0.026617626701253033, 'max_depth': 6, 'min_child_weight': 5, 'gamma': 3.6290418937760034, 'subsample': 0.7364479724550128, 'colsample_bytree': 0.8176109584584819, 'reg_alpha': 0.10884042327241734, 'reg_lambda': 0.07878737301093909}. Best is trial 0 with value: 0.8295999999999999.\n",
      "[I 2025-11-09 12:52:10,629] Trial 2 finished with value: 0.8300000000000001 and parameters: {'learning_rate': 0.013215255640929511, 'max_depth': 5, 'min_child_weight': 9, 'gamma': 1.6289453860866572, 'subsample': 0.6280654777094311, 'colsample_bytree': 0.7623989151542007, 'reg_alpha': 1.6135529892122942, 'reg_lambda': 3.1415545521091977}. Best is trial 2 with value: 0.8300000000000001.\n",
      "[I 2025-11-09 12:52:19,295] Trial 3 finished with value: 0.8294 and parameters: {'learning_rate': 0.017526414858874188, 'max_depth': 7, 'min_child_weight': 2, 'gamma': 2.4529496700147, 'subsample': 0.652215196877557, 'colsample_bytree': 0.7545753612537895, 'reg_alpha': 0.03902239317456017, 'reg_lambda': 0.10818054380534807}. Best is trial 2 with value: 0.8300000000000001.\n",
      "[I 2025-11-09 12:52:25,544] Trial 4 finished with value: 0.8306000000000001 and parameters: {'learning_rate': 0.023456241739188633, 'max_depth': 3, 'min_child_weight': 2, 'gamma': 1.603960974005736, 'subsample': 0.8014734136462762, 'colsample_bytree': 0.868337789179751, 'reg_alpha': 0.6639129641028179, 'reg_lambda': 0.07277063804624205}. Best is trial 4 with value: 0.8306000000000001.\n",
      "[I 2025-11-09 12:52:30,202] Trial 5 finished with value: 0.8308 and parameters: {'learning_rate': 0.030102960512086926, 'max_depth': 4, 'min_child_weight': 2, 'gamma': 3.3466890480924194, 'subsample': 0.6448354228405542, 'colsample_bytree': 0.8372568602009394, 'reg_alpha': 0.0018758971560743191, 'reg_lambda': 0.007976967423586922}. Best is trial 5 with value: 0.8308.\n",
      "[I 2025-11-09 12:52:30,546] Trial 6 pruned. \n",
      "[I 2025-11-09 12:52:32,180] Trial 7 pruned. \n",
      "[I 2025-11-09 12:52:35,183] Trial 8 pruned. \n",
      "[I 2025-11-09 12:52:40,487] Trial 9 pruned. \n",
      "[I 2025-11-09 12:52:40,909] Trial 10 pruned. \n",
      "[I 2025-11-09 12:52:46,672] Trial 11 finished with value: 0.8311 and parameters: {'learning_rate': 0.026594963362623944, 'max_depth': 3, 'min_child_weight': 3, 'gamma': 1.2969130399462419, 'subsample': 0.8106283307179435, 'colsample_bytree': 0.9065068915459157, 'reg_alpha': 0.48144816592414524, 'reg_lambda': 0.0020549657249706015}. Best is trial 11 with value: 0.8311.\n",
      "[I 2025-11-09 12:52:50,951] Trial 12 finished with value: 0.8314999999999999 and parameters: {'learning_rate': 0.03295614196775498, 'max_depth': 4, 'min_child_weight': 4, 'gamma': 0.055628472705663734, 'subsample': 0.6628698261689934, 'colsample_bytree': 0.9645039394072243, 'reg_alpha': 0.4641276533834993, 'reg_lambda': 0.0018009881167516967}. Best is trial 12 with value: 0.8314999999999999.\n",
      "[I 2025-11-09 12:52:51,623] Trial 13 pruned. \n",
      "[I 2025-11-09 12:52:52,224] Trial 14 pruned. \n",
      "[I 2025-11-09 12:52:55,130] Trial 15 pruned. \n",
      "[I 2025-11-09 12:52:57,053] Trial 16 pruned. \n",
      "[I 2025-11-09 12:52:57,931] Trial 17 pruned. \n",
      "[I 2025-11-09 12:52:59,217] Trial 18 pruned. \n",
      "[I 2025-11-09 12:53:01,634] Trial 19 pruned. \n",
      "[I 2025-11-09 12:53:04,717] Trial 20 pruned. \n",
      "[I 2025-11-09 12:53:09,557] Trial 21 finished with value: 0.8317 and parameters: {'learning_rate': 0.030445193546819453, 'max_depth': 4, 'min_child_weight': 3, 'gamma': 3.736323176373301, 'subsample': 0.6587783196183964, 'colsample_bytree': 0.848122040186853, 'reg_alpha': 0.01280505470698012, 'reg_lambda': 0.007623038105478583}. Best is trial 21 with value: 0.8317.\n",
      "[I 2025-11-09 12:53:12,099] Trial 22 pruned. \n",
      "[I 2025-11-09 12:53:12,794] Trial 23 pruned. \n",
      "[I 2025-11-09 12:53:14,081] Trial 24 pruned. \n",
      "[I 2025-11-09 12:53:14,784] Trial 25 pruned. \n",
      "[I 2025-11-09 12:53:17,030] Trial 26 pruned. \n",
      "[I 2025-11-09 12:53:20,645] Trial 27 pruned. \n",
      "[I 2025-11-09 12:53:21,456] Trial 28 pruned. \n",
      "[I 2025-11-09 12:53:23,474] Trial 29 pruned. \n",
      "[I 2025-11-09 12:53:26,234] Trial 30 pruned. \n",
      "[I 2025-11-09 12:53:27,111] Trial 31 pruned. \n",
      "[I 2025-11-09 12:53:29,087] Trial 32 pruned. \n",
      "[I 2025-11-09 12:53:31,776] Trial 33 pruned. \n",
      "[I 2025-11-09 12:53:32,732] Trial 34 pruned. \n",
      "[I 2025-11-09 12:53:33,988] Trial 35 pruned. \n",
      "[I 2025-11-09 12:53:38,957] Trial 36 finished with value: 0.8303 and parameters: {'learning_rate': 0.030884657098112566, 'max_depth': 5, 'min_child_weight': 2, 'gamma': 4.378558685776953, 'subsample': 0.7503482484384378, 'colsample_bytree': 0.8799666256106081, 'reg_alpha': 0.5684034801565271, 'reg_lambda': 0.008193894890909423}. Best is trial 21 with value: 0.8317.\n",
      "[I 2025-11-09 12:53:41,535] Trial 37 pruned. \n",
      "[I 2025-11-09 12:53:43,086] Trial 38 pruned. \n",
      "[I 2025-11-09 12:53:43,929] Trial 39 pruned. \n",
      "[I 2025-11-09 12:53:52,710] Trial 40 finished with value: 0.8309 and parameters: {'learning_rate': 0.014301736389101516, 'max_depth': 3, 'min_child_weight': 3, 'gamma': 1.8783550707487562, 'subsample': 0.6542865469116076, 'colsample_bytree': 0.7240328926478674, 'reg_alpha': 0.829661749429958, 'reg_lambda': 0.01626250152296029}. Best is trial 21 with value: 0.8317.\n",
      "[I 2025-11-09 12:54:02,338] Trial 41 finished with value: 0.8320000000000001 and parameters: {'learning_rate': 0.013569095769322734, 'max_depth': 3, 'min_child_weight': 3, 'gamma': 1.9253288107336801, 'subsample': 0.6488894942995962, 'colsample_bytree': 0.6961729669802681, 'reg_alpha': 0.9101690636702318, 'reg_lambda': 0.014280860509894945}. Best is trial 41 with value: 0.8320000000000001.\n",
      "[I 2025-11-09 12:54:15,000] Trial 42 finished with value: 0.8309 and parameters: {'learning_rate': 0.01081423611289632, 'max_depth': 3, 'min_child_weight': 3, 'gamma': 1.8385831181508885, 'subsample': 0.6560458145369474, 'colsample_bytree': 0.6855408670589299, 'reg_alpha': 2.563552920254903, 'reg_lambda': 0.014432866869543249}. Best is trial 41 with value: 0.8320000000000001.\n",
      "[I 2025-11-09 12:54:25,548] Trial 43 finished with value: 0.8309000000000001 and parameters: {'learning_rate': 0.012778491023942393, 'max_depth': 3, 'min_child_weight': 5, 'gamma': 2.288953852890733, 'subsample': 0.6332348245631451, 'colsample_bytree': 0.6627492958957009, 'reg_alpha': 0.8750581633985709, 'reg_lambda': 0.005637113089161829}. Best is trial 41 with value: 0.8320000000000001.\n",
      "[I 2025-11-09 12:54:30,296] Trial 44 pruned. \n",
      "[I 2025-11-09 12:54:34,205] Trial 45 pruned. \n",
      "[I 2025-11-09 12:54:39,153] Trial 46 pruned. \n",
      "[I 2025-11-09 12:54:46,511] Trial 47 finished with value: 0.8328000000000001 and parameters: {'learning_rate': 0.01725498972725979, 'max_depth': 4, 'min_child_weight': 5, 'gamma': 1.4747046376553035, 'subsample': 0.6172003804197562, 'colsample_bytree': 0.7386305611125725, 'reg_alpha': 0.17667116956121198, 'reg_lambda': 0.02343261946715976}. Best is trial 47 with value: 0.8328000000000001.\n",
      "[I 2025-11-09 12:54:49,353] Trial 48 pruned. \n",
      "[I 2025-11-09 12:54:50,811] Trial 49 pruned. \n",
      "[I 2025-11-09 12:54:55,625] Trial 50 pruned. \n",
      "[I 2025-11-09 12:54:59,410] Trial 51 pruned. \n",
      "[I 2025-11-09 12:55:10,240] Trial 52 finished with value: 0.8322999999999998 and parameters: {'learning_rate': 0.011987218130525393, 'max_depth': 3, 'min_child_weight': 6, 'gamma': 1.2053585706922005, 'subsample': 0.6213351568311396, 'colsample_bytree': 0.7081316049416071, 'reg_alpha': 1.066418466310664, 'reg_lambda': 0.008339709193504904}. Best is trial 47 with value: 0.8328000000000001.\n",
      "[I 2025-11-09 12:55:14,877] Trial 53 pruned. \n",
      "[I 2025-11-09 12:55:23,888] Trial 54 finished with value: 0.8318 and parameters: {'learning_rate': 0.013836483448073563, 'max_depth': 3, 'min_child_weight': 6, 'gamma': 0.7904646343283095, 'subsample': 0.6867117252319936, 'colsample_bytree': 0.7275806433120144, 'reg_alpha': 0.24831980186484165, 'reg_lambda': 0.09198246278445174}. Best is trial 47 with value: 0.8328000000000001.\n",
      "[I 2025-11-09 12:55:32,784] Trial 55 finished with value: 0.8311999999999999 and parameters: {'learning_rate': 0.013809555817269333, 'max_depth': 5, 'min_child_weight': 6, 'gamma': 0.7925252686001971, 'subsample': 0.6677072274671519, 'colsample_bytree': 0.7377153810505812, 'reg_alpha': 0.08089917163446093, 'reg_lambda': 0.07356363129963701}. Best is trial 47 with value: 0.8328000000000001.\n",
      "[I 2025-11-09 12:55:33,110] Trial 56 pruned. \n",
      "[I 2025-11-09 12:55:41,173] Trial 57 finished with value: 0.8333 and parameters: {'learning_rate': 0.01598453953676744, 'max_depth': 4, 'min_child_weight': 6, 'gamma': 0.7739881249541041, 'subsample': 0.6660717809018186, 'colsample_bytree': 0.7075112966738586, 'reg_alpha': 0.2201966238068113, 'reg_lambda': 0.3277434732270064}. Best is trial 57 with value: 0.8333.\n",
      "[I 2025-11-09 12:55:42,662] Trial 58 pruned. \n",
      "[I 2025-11-09 12:55:45,910] Trial 59 pruned. \n",
      "[I 2025-11-09 12:55:50,552] Trial 60 pruned. \n",
      "[I 2025-11-09 12:55:51,893] Trial 61 pruned. \n",
      "[I 2025-11-09 12:55:56,330] Trial 62 pruned. \n",
      "[I 2025-11-09 12:56:00,124] Trial 63 pruned. \n",
      "[I 2025-11-09 12:56:01,809] Trial 64 pruned. \n",
      "[I 2025-11-09 12:56:03,392] Trial 65 pruned. \n",
      "[I 2025-11-09 12:56:10,379] Trial 66 finished with value: 0.833 and parameters: {'learning_rate': 0.024456270984320547, 'max_depth': 3, 'min_child_weight': 5, 'gamma': 1.8098937155771015, 'subsample': 0.6437772243124203, 'colsample_bytree': 0.7585336709786594, 'reg_alpha': 1.116110359632541, 'reg_lambda': 0.022456556772066084}. Best is trial 57 with value: 0.8333.\n",
      "[I 2025-11-09 12:56:11,562] Trial 67 pruned. \n",
      "[I 2025-11-09 12:56:18,319] Trial 68 finished with value: 0.8339000000000001 and parameters: {'learning_rate': 0.021233713886499382, 'max_depth': 3, 'min_child_weight': 5, 'gamma': 1.970286128066985, 'subsample': 0.6456505306245477, 'colsample_bytree': 0.7614796253977821, 'reg_alpha': 0.36694528325391146, 'reg_lambda': 0.011611480772568114}. Best is trial 68 with value: 0.8339000000000001.\n",
      "[I 2025-11-09 12:56:26,204] Trial 69 finished with value: 0.8318 and parameters: {'learning_rate': 0.0161513223471853, 'max_depth': 3, 'min_child_weight': 5, 'gamma': 2.00331301330801, 'subsample': 0.6413414955442133, 'colsample_bytree': 0.7823445409997319, 'reg_alpha': 0.6439262900004922, 'reg_lambda': 0.011828825048063353}. Best is trial 68 with value: 0.8339000000000001.\n",
      "[I 2025-11-09 12:56:33,205] Trial 70 finished with value: 0.8333999999999999 and parameters: {'learning_rate': 0.0189876822952609, 'max_depth': 3, 'min_child_weight': 6, 'gamma': 1.34217714942515, 'subsample': 0.6006081717133958, 'colsample_bytree': 0.7610159118847282, 'reg_alpha': 0.1307521514160452, 'reg_lambda': 0.08524392616489758}. Best is trial 68 with value: 0.8339000000000001.\n",
      "[I 2025-11-09 12:56:36,176] Trial 71 pruned. \n",
      "[I 2025-11-09 12:56:42,549] Trial 72 finished with value: 0.8327 and parameters: {'learning_rate': 0.021499545849220803, 'max_depth': 3, 'min_child_weight': 5, 'gamma': 1.611170504293638, 'subsample': 0.6229529928143841, 'colsample_bytree': 0.7309226634997985, 'reg_alpha': 0.35810135620873335, 'reg_lambda': 0.08276354274210962}. Best is trial 68 with value: 0.8339000000000001.\n",
      "[I 2025-11-09 12:56:43,614] Trial 73 pruned. \n",
      "[I 2025-11-09 12:56:51,366] Trial 74 finished with value: 0.8318 and parameters: {'learning_rate': 0.017178356392749304, 'max_depth': 3, 'min_child_weight': 5, 'gamma': 1.9088451004372633, 'subsample': 0.622091389378423, 'colsample_bytree': 0.7996426110003076, 'reg_alpha': 1.0953495011154795, 'reg_lambda': 0.12835683580148058}. Best is trial 68 with value: 0.8339000000000001.\n",
      "[I 2025-11-09 12:56:57,641] Trial 75 finished with value: 0.8321999999999999 and parameters: {'learning_rate': 0.02295346649346972, 'max_depth': 3, 'min_child_weight': 5, 'gamma': 1.4363871225884883, 'subsample': 0.6005498474654136, 'colsample_bytree': 0.7587114635414022, 'reg_alpha': 0.6108122286511738, 'reg_lambda': 0.17494118123145794}. Best is trial 68 with value: 0.8339000000000001.\n",
      "[I 2025-11-09 12:57:04,052] Trial 76 finished with value: 0.8327 and parameters: {'learning_rate': 0.022379556444707176, 'max_depth': 3, 'min_child_weight': 5, 'gamma': 1.365253686131167, 'subsample': 0.6037610324574768, 'colsample_bytree': 0.7571294562184334, 'reg_alpha': 0.6293219812162327, 'reg_lambda': 0.18856827938963977}. Best is trial 68 with value: 0.8339000000000001.\n",
      "[I 2025-11-09 12:57:09,889] Trial 77 finished with value: 0.8318 and parameters: {'learning_rate': 0.025692152302042245, 'max_depth': 3, 'min_child_weight': 6, 'gamma': 1.266552058842322, 'subsample': 0.6309936370802249, 'colsample_bytree': 0.7836837649814448, 'reg_alpha': 1.5891145917761815, 'reg_lambda': 0.2628046619263782}. Best is trial 68 with value: 0.8339000000000001.\n",
      "[I 2025-11-09 12:57:11,717] Trial 78 pruned. \n",
      "[I 2025-11-09 12:57:12,075] Trial 79 pruned. \n",
      "[I 2025-11-09 12:57:18,686] Trial 80 finished with value: 0.8328 and parameters: {'learning_rate': 0.020346631161198062, 'max_depth': 3, 'min_child_weight': 6, 'gamma': 1.755653062470956, 'subsample': 0.6372142054627074, 'colsample_bytree': 0.808542756061516, 'reg_alpha': 0.07840197923529454, 'reg_lambda': 0.1910975125188913}. Best is trial 68 with value: 0.8339000000000001.\n",
      "[I 2025-11-09 12:57:25,598] Trial 81 finished with value: 0.8336 and parameters: {'learning_rate': 0.020253789012354247, 'max_depth': 3, 'min_child_weight': 6, 'gamma': 1.7247042060748283, 'subsample': 0.6381775410382786, 'colsample_bytree': 0.8129163325066271, 'reg_alpha': 0.08338379143836941, 'reg_lambda': 0.2260293812914407}. Best is trial 68 with value: 0.8339000000000001.\n",
      "[I 2025-11-09 12:57:31,708] Trial 82 finished with value: 0.8334999999999999 and parameters: {'learning_rate': 0.02065497659141097, 'max_depth': 3, 'min_child_weight': 6, 'gamma': 2.171820558475034, 'subsample': 0.6379045277738679, 'colsample_bytree': 0.8089023188491817, 'reg_alpha': 0.02817177417743841, 'reg_lambda': 0.21260152261036863}. Best is trial 68 with value: 0.8339000000000001.\n",
      "[I 2025-11-09 12:57:32,942] Trial 83 pruned. \n",
      "[I 2025-11-09 12:57:39,963] Trial 84 finished with value: 0.8347 and parameters: {'learning_rate': 0.019161090151695974, 'max_depth': 3, 'min_child_weight': 6, 'gamma': 2.4267729113636345, 'subsample': 0.6391418336680764, 'colsample_bytree': 0.8034979811909722, 'reg_alpha': 0.04656745646903133, 'reg_lambda': 0.25114366021463247}. Best is trial 84 with value: 0.8347.\n",
      "[I 2025-11-09 12:57:42,934] Trial 85 pruned. \n",
      "[I 2025-11-09 12:57:45,366] Trial 86 pruned. \n",
      "[I 2025-11-09 12:57:52,261] Trial 87 finished with value: 0.8331 and parameters: {'learning_rate': 0.019728157047721545, 'max_depth': 3, 'min_child_weight': 6, 'gamma': 1.794021691465931, 'subsample': 0.6501578319292266, 'colsample_bytree': 0.8578215243226254, 'reg_alpha': 0.06525978361865356, 'reg_lambda': 0.3000681468367483}. Best is trial 84 with value: 0.8347.\n",
      "[I 2025-11-09 12:57:53,571] Trial 88 pruned. \n",
      "[I 2025-11-09 12:57:55,074] Trial 89 pruned. \n",
      "[I 2025-11-09 12:57:57,818] Trial 90 pruned. \n",
      "[I 2025-11-09 12:57:58,915] Trial 91 pruned. \n",
      "[I 2025-11-09 12:58:05,474] Trial 92 finished with value: 0.8331 and parameters: {'learning_rate': 0.020505738699512315, 'max_depth': 3, 'min_child_weight': 6, 'gamma': 2.1970972281188486, 'subsample': 0.6319958086749201, 'colsample_bytree': 0.7934378466621732, 'reg_alpha': 0.07111348042430239, 'reg_lambda': 0.22079716263661617}. Best is trial 84 with value: 0.8347.\n",
      "[I 2025-11-09 12:58:06,770] Trial 93 pruned. \n",
      "[I 2025-11-09 12:58:13,910] Trial 94 finished with value: 0.8321 and parameters: {'learning_rate': 0.017990514389381672, 'max_depth': 3, 'min_child_weight': 7, 'gamma': 2.6188327068142923, 'subsample': 0.6151920494039056, 'colsample_bytree': 0.7919033823836492, 'reg_alpha': 0.05799882497139763, 'reg_lambda': 0.45228614892675745}. Best is trial 84 with value: 0.8347.\n",
      "[I 2025-11-09 12:58:16,564] Trial 95 pruned. \n",
      "[I 2025-11-09 12:58:17,924] Trial 96 pruned. \n",
      "[I 2025-11-09 12:58:19,171] Trial 97 pruned. \n",
      "[I 2025-11-09 12:58:20,434] Trial 98 pruned. \n",
      "[I 2025-11-09 12:58:21,730] Trial 99 pruned. \n",
      "[I 2025-11-09 12:58:22,647] Trial 100 pruned. \n",
      "[I 2025-11-09 12:58:29,018] Trial 101 finished with value: 0.8329000000000001 and parameters: {'learning_rate': 0.020423948510915405, 'max_depth': 3, 'min_child_weight': 6, 'gamma': 1.7767250372021346, 'subsample': 0.6362524691564935, 'colsample_bytree': 0.841474419690464, 'reg_alpha': 0.11505561997955789, 'reg_lambda': 0.20441849371909898}. Best is trial 84 with value: 0.8347.\n",
      "[I 2025-11-09 12:58:32,064] Trial 102 pruned. \n",
      "[I 2025-11-09 12:58:38,958] Trial 103 finished with value: 0.8337 and parameters: {'learning_rate': 0.020195799394933125, 'max_depth': 3, 'min_child_weight': 6, 'gamma': 1.8248331069745851, 'subsample': 0.6297398076470696, 'colsample_bytree': 0.8408241437650462, 'reg_alpha': 0.16410595951869572, 'reg_lambda': 0.48023073414800377}. Best is trial 84 with value: 0.8347.\n",
      "[I 2025-11-09 12:58:41,924] Trial 104 pruned. \n",
      "[I 2025-11-09 12:58:43,186] Trial 105 pruned. \n",
      "[I 2025-11-09 12:58:52,026] Trial 106 finished with value: 0.8327 and parameters: {'learning_rate': 0.014997881056857596, 'max_depth': 3, 'min_child_weight': 6, 'gamma': 1.9599892817860187, 'subsample': 0.6294538002851271, 'colsample_bytree': 0.817920974039176, 'reg_alpha': 0.031947814600660034, 'reg_lambda': 0.27615105882223273}. Best is trial 84 with value: 0.8347.\n",
      "[I 2025-11-09 12:58:55,109] Trial 107 pruned. \n",
      "[I 2025-11-09 12:59:00,122] Trial 108 finished with value: 0.8338999999999999 and parameters: {'learning_rate': 0.026973075778642577, 'max_depth': 3, 'min_child_weight': 6, 'gamma': 2.5709931307797778, 'subsample': 0.6776329705006099, 'colsample_bytree': 0.8546737135451573, 'reg_alpha': 0.08560784814055454, 'reg_lambda': 0.16054882804001677}. Best is trial 84 with value: 0.8347.\n",
      "[I 2025-11-09 12:59:00,933] Trial 109 pruned. \n",
      "[I 2025-11-09 12:59:02,042] Trial 110 pruned. \n",
      "[I 2025-11-09 12:59:03,237] Trial 111 pruned. \n",
      "[I 2025-11-09 12:59:08,359] Trial 112 finished with value: 0.8333999999999999 and parameters: {'learning_rate': 0.026133811470456382, 'max_depth': 3, 'min_child_weight': 6, 'gamma': 2.1888857366298464, 'subsample': 0.6442990479816613, 'colsample_bytree': 0.8791622092360436, 'reg_alpha': 0.06944600208918923, 'reg_lambda': 0.3520158980201769}. Best is trial 84 with value: 0.8347.\n",
      "[I 2025-11-09 12:59:09,259] Trial 113 pruned. \n",
      "[I 2025-11-09 12:59:14,326] Trial 114 finished with value: 0.8332 and parameters: {'learning_rate': 0.029956535482084514, 'max_depth': 3, 'min_child_weight': 6, 'gamma': 2.760530128801915, 'subsample': 0.66421296990989, 'colsample_bytree': 0.8991661055123008, 'reg_alpha': 0.09598175437934377, 'reg_lambda': 0.7546930826896261}. Best is trial 84 with value: 0.8347.\n",
      "[I 2025-11-09 12:59:16,829] Trial 115 pruned. \n",
      "[I 2025-11-09 12:59:17,801] Trial 116 pruned. \n",
      "[I 2025-11-09 12:59:18,629] Trial 117 pruned. \n",
      "[I 2025-11-09 12:59:21,377] Trial 118 pruned. \n",
      "[I 2025-11-09 12:59:22,226] Trial 119 pruned. \n",
      "[I 2025-11-09 12:59:29,773] Trial 120 finished with value: 0.833 and parameters: {'learning_rate': 0.0185042286853457, 'max_depth': 3, 'min_child_weight': 6, 'gamma': 2.213360813294252, 'subsample': 0.6515931769064799, 'colsample_bytree': 0.87169405774416, 'reg_alpha': 0.06475218007370827, 'reg_lambda': 0.293226036234381}. Best is trial 84 with value: 0.8347.\n",
      "[I 2025-11-09 12:59:30,753] Trial 121 pruned. \n",
      "[I 2025-11-09 12:59:31,676] Trial 122 pruned. \n",
      "[I 2025-11-09 12:59:32,602] Trial 123 pruned. \n",
      "[I 2025-11-09 12:59:33,671] Trial 124 pruned. \n",
      "[I 2025-11-09 12:59:37,091] Trial 125 pruned. \n",
      "[I 2025-11-09 12:59:37,839] Trial 126 pruned. \n",
      "[I 2025-11-09 12:59:43,947] Trial 127 finished with value: 0.8324 and parameters: {'learning_rate': 0.024306814719042003, 'max_depth': 3, 'min_child_weight': 6, 'gamma': 2.0682793679623974, 'subsample': 0.6625854393695672, 'colsample_bytree': 0.8857206108142903, 'reg_alpha': 0.049238512671044565, 'reg_lambda': 0.31167849790262986}. Best is trial 84 with value: 0.8347.\n",
      "[I 2025-11-09 12:59:45,399] Trial 128 pruned. \n",
      "[I 2025-11-09 12:59:49,822] Trial 129 pruned. \n",
      "[I 2025-11-09 12:59:51,081] Trial 130 pruned. \n",
      "[I 2025-11-09 12:59:51,895] Trial 131 pruned. \n",
      "[I 2025-11-09 12:59:59,673] Trial 132 finished with value: 0.8343999999999999 and parameters: {'learning_rate': 0.018660593952625686, 'max_depth': 3, 'min_child_weight': 6, 'gamma': 2.1560553738317356, 'subsample': 0.6419102537615543, 'colsample_bytree': 0.8704619200592353, 'reg_alpha': 0.15099041792864173, 'reg_lambda': 0.20592277307296902}. Best is trial 84 with value: 0.8347.\n",
      "[I 2025-11-09 13:00:02,886] Trial 133 pruned. \n",
      "[I 2025-11-09 13:00:04,208] Trial 134 pruned. \n",
      "[I 2025-11-09 13:00:05,397] Trial 135 pruned. \n",
      "[I 2025-11-09 13:00:13,310] Trial 136 finished with value: 0.8327 and parameters: {'learning_rate': 0.01770633243415637, 'max_depth': 3, 'min_child_weight': 6, 'gamma': 2.3552696081447326, 'subsample': 0.6579350126543387, 'colsample_bytree': 0.8324093526470526, 'reg_alpha': 0.17943675148694177, 'reg_lambda': 0.09646190764014638}. Best is trial 84 with value: 0.8347.\n",
      "[I 2025-11-09 13:00:20,043] Trial 137 finished with value: 0.8339000000000001 and parameters: {'learning_rate': 0.021197008538692823, 'max_depth': 3, 'min_child_weight': 7, 'gamma': 1.94872163809199, 'subsample': 0.6394375127783686, 'colsample_bytree': 0.8492839570334625, 'reg_alpha': 0.11974425081021386, 'reg_lambda': 0.1505079895318274}. Best is trial 84 with value: 0.8347.\n",
      "[I 2025-11-09 13:00:22,824] Trial 138 pruned. \n",
      "[I 2025-11-09 13:00:24,540] Trial 139 pruned. \n",
      "[I 2025-11-09 13:00:25,605] Trial 140 pruned. \n",
      "[I 2025-11-09 13:00:27,968] Trial 141 pruned. \n",
      "[I 2025-11-09 13:00:29,137] Trial 142 pruned. \n",
      "[I 2025-11-09 13:00:35,548] Trial 143 finished with value: 0.8337 and parameters: {'learning_rate': 0.020391727924714376, 'max_depth': 3, 'min_child_weight': 6, 'gamma': 1.965053491149183, 'subsample': 0.628241433169765, 'colsample_bytree': 0.8144121664787419, 'reg_alpha': 0.01627496881097182, 'reg_lambda': 0.5210676389813854}. Best is trial 84 with value: 0.8347.\n",
      "[I 2025-11-09 13:00:42,122] Trial 144 finished with value: 0.8320000000000001 and parameters: {'learning_rate': 0.020065679877049096, 'max_depth': 3, 'min_child_weight': 6, 'gamma': 1.9735472186755023, 'subsample': 0.6262529699191416, 'colsample_bytree': 0.6107052519595895, 'reg_alpha': 0.017303418881597518, 'reg_lambda': 0.5639559608928176}. Best is trial 84 with value: 0.8347.\n",
      "[I 2025-11-09 13:00:43,486] Trial 145 pruned. \n",
      "[I 2025-11-09 13:00:44,816] Trial 146 pruned. \n",
      "[I 2025-11-09 13:00:46,010] Trial 147 pruned. \n",
      "[I 2025-11-09 13:00:47,467] Trial 148 pruned. \n",
      "[I 2025-11-09 13:00:49,066] Trial 149 pruned. \n",
      "\n",
      "=== RISULTATI OPTUNA ===\n",
      "Best Trial: 84\n",
      "Best Accuracy (CV media): 0.8347\n",
      "Best Params:\n",
      "  learning_rate: 0.019161090151695974\n",
      "  max_depth: 3\n",
      "  min_child_weight: 6\n",
      "  gamma: 2.4267729113636345\n",
      "  subsample: 0.6391418336680764\n",
      "  colsample_bytree: 0.8034979811909722\n",
      "  reg_alpha: 0.04656745646903133\n",
      "  reg_lambda: 0.25114366021463247\n"
     ]
    }
   ],
   "source": [
    "# === OPTUNA HYPERPARAMETER TUNING (FIXED) ===\n",
    "import optuna\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 1. Configurazione base\n",
    "N_TRIALS = 150\n",
    "TIMEOUT_SEC = 7200\n",
    "EARLY_STOPPING_ROUNDS = 50\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "print(f\"Avvio ottimizzazione Optuna per {N_TRIALS} trial (o {TIMEOUT_SEC} secondi)...\")\n",
    "\n",
    "def objective(trial):\n",
    "    # 2. Definizione dello spazio di ricerca\n",
    "    params = {\n",
    "        'objective': 'binary:logistic',\n",
    "        'tree_method': 'hist',\n",
    "        'eval_metric': 'logloss',\n",
    "        'use_label_encoder': False,\n",
    "        'n_jobs': -1,\n",
    "        'random_state': RANDOM_STATE,\n",
    "        # FIX: Early stopping ora va passato qui nel costruttore per le nuove versioni di XGBoost\n",
    "        'early_stopping_rounds': EARLY_STOPPING_ROUNDS,\n",
    "        # Iperparametri da ottimizzare\n",
    "        'n_estimators': 2000,\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.15, log=True),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 8),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
    "        'gamma': trial.suggest_float('gamma', 0.0, 5.0),\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 1e-3, 10.0, log=True),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 1e-3, 10.0, log=True),\n",
    "    }\n",
    "\n",
    "    # 3. Cross-Validation interna\n",
    "    cv_inner = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
    "    fold_scores = []\n",
    "\n",
    "    for train_idx, val_idx in cv_inner.split(X, y):\n",
    "        X_tr, X_val = X[train_idx], X[val_idx]\n",
    "        y_tr, y_val = y[train_idx], y[val_idx]\n",
    "\n",
    "        model = xgb.XGBClassifier(**params)\n",
    "\n",
    "        # Fit corretto: early_stopping_rounds è già in params (costruttore)\n",
    "        model.fit(\n",
    "            X_tr, y_tr,\n",
    "            eval_set=[(X_val, y_val)],\n",
    "            verbose=False\n",
    "        )\n",
    "\n",
    "        # Valutazione (usa il miglior numero di iterazioni trovato)\n",
    "        preds = model.predict(X_val)\n",
    "        acc = accuracy_score(y_val, preds)\n",
    "        fold_scores.append(acc)\n",
    "\n",
    "        trial.report(acc, step=len(fold_scores)-1)\n",
    "        if trial.should_prune():\n",
    "             raise optuna.TrialPruned()\n",
    "\n",
    "    return np.mean(fold_scores)\n",
    "\n",
    "# 4. Avvio ottimizzazione\n",
    "study = optuna.create_study(direction='maximize', pruner=optuna.pruners.MedianPruner())\n",
    "study.optimize(objective, n_trials=N_TRIALS, timeout=TIMEOUT_SEC, show_progress_bar=True)\n",
    "\n",
    "# 5. Risultati\n",
    "print(\"\\n=== RISULTATI OPTUNA ===\")\n",
    "print(f\"Best Trial: {study.best_trial.number}\")\n",
    "print(f\"Best Accuracy (CV media): {study.best_value:.4f}\")\n",
    "print(\"Best Params:\")\n",
    "for key, value in study.best_params.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "best_params_optuna = study.best_params\n",
    "best_params_optuna.update({\n",
    "    'objective': 'binary:logistic',\n",
    "    'tree_method': 'hist',\n",
    "    'eval_metric': 'logloss',\n",
    "    'use_label_encoder': False,\n",
    "    # Manteniamo n_estimators alto per il training finale,\n",
    "    # ma potresti voler usare il valore medio trovato con early stopping se lo salvassi.\n",
    "    # Per ora 1000 è un buon compromesso safe.\n",
    "    'n_estimators': 1000\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c663bc73",
   "metadata": {},
   "source": [
    "# Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cab0d514",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 10-Fold Cross-Validation (tutto il dataset) ===\n",
      "Parametri utilizzati: {'objective': 'binary:logistic', 'tree_method': 'hist', 'eval_metric': 'logloss', 'use_label_encoder': False, 'n_estimators': 1000, 'learning_rate': 0.019161090151695974, 'max_depth': 3, 'min_child_weight': 6, 'gamma': 2.4267729113636345, 'subsample': 0.6391418336680764, 'colsample_bytree': 0.8034979811909722, 'reg_alpha': 0.04656745646903133, 'reg_lambda': 0.25114366021463247}\n",
      "\n",
      "Fold 1: no ES, train=9000, val=1000, acc_val=83.40%, acc_val_opt=83.80% @thr=0.426, acc_train=87.77%, gap=4.37%\n",
      "Fold 2: no ES, train=9000, val=1000, acc_val=84.10%, acc_val_opt=84.50% @thr=0.536, acc_train=87.73%, gap=3.63%\n",
      "Fold 3: no ES, train=9000, val=1000, acc_val=84.00%, acc_val_opt=84.30% @thr=0.522, acc_train=87.66%, gap=3.66%\n",
      "Fold 4: no ES, train=9000, val=1000, acc_val=84.60%, acc_val_opt=85.20% @thr=0.569, acc_train=87.70%, gap=3.10%\n",
      "Fold 5: no ES, train=9000, val=1000, acc_val=85.70%, acc_val_opt=86.00% @thr=0.457, acc_train=87.50%, gap=1.80%\n",
      "Fold 6: no ES, train=9000, val=1000, acc_val=83.60%, acc_val_opt=84.20% @thr=0.513, acc_train=87.84%, gap=4.24%\n",
      "Fold 7: no ES, train=9000, val=1000, acc_val=83.60%, acc_val_opt=83.70% @thr=0.505, acc_train=87.76%, gap=4.16%\n",
      "Fold 8: no ES, train=9000, val=1000, acc_val=84.00%, acc_val_opt=84.30% @thr=0.490, acc_train=87.51%, gap=3.51%\n",
      "Fold 9: no ES, train=9000, val=1000, acc_val=84.10%, acc_val_opt=84.30% @thr=0.477, acc_train=87.68%, gap=3.58%\n",
      "Fold 10: no ES, train=9000, val=1000, acc_val=84.70%, acc_val_opt=85.00% @thr=0.392, acc_train=87.40%, gap=2.70%\n",
      "\n",
      "============================================================\n",
      "Risultati Cross-Validation\n",
      "============================================================\n",
      "  Fold 1: val_acc=83.40%, val_acc_opt=83.80% @thr=0.426, train_acc=87.77%, gap=4.37%\n",
      "  Fold 2: val_acc=84.10%, val_acc_opt=84.50% @thr=0.536, train_acc=87.73%, gap=3.63%\n",
      "  Fold 3: val_acc=84.00%, val_acc_opt=84.30% @thr=0.522, train_acc=87.66%, gap=3.66%\n",
      "  Fold 4: val_acc=84.60%, val_acc_opt=85.20% @thr=0.569, train_acc=87.70%, gap=3.10%\n",
      "  Fold 5: val_acc=85.70%, val_acc_opt=86.00% @thr=0.457, train_acc=87.50%, gap=1.80%\n",
      "  Fold 6: val_acc=83.60%, val_acc_opt=84.20% @thr=0.513, train_acc=87.84%, gap=4.24%\n",
      "  Fold 7: val_acc=83.60%, val_acc_opt=83.70% @thr=0.505, train_acc=87.76%, gap=4.16%\n",
      "  Fold 8: val_acc=84.00%, val_acc_opt=84.30% @thr=0.490, train_acc=87.51%, gap=3.51%\n",
      "  Fold 9: val_acc=84.10%, val_acc_opt=84.30% @thr=0.477, train_acc=87.68%, gap=3.58%\n",
      "  Fold 10: val_acc=84.70%, val_acc_opt=85.00% @thr=0.392, train_acc=87.40%, gap=2.70%\n",
      "\n",
      "Mean CV accuracy (0.5): 84.18%\n",
      "Mean CV accuracy (opt thr): 84.53%\n",
      "Mean train accuracy: 87.65%\n",
      "Mean gap (train - val): 3.47%\n",
      "Std CV accuracy:  0.64%\n",
      "Min/Max val acc:  83.40% / 85.70%\n",
      "\n",
      "Peggiore fold: #1 con acc_val=83.40% | acc_val_opt=83.80% | acc_train=87.77% | gap=4.37%\n"
     ]
    }
   ],
   "source": [
    "# === 10-Fold Cross-Validation con iperparametri FISSI ===\n",
    "# IMPORTANTE: Assegna qui i migliori iperparametri trovati dalla cella precedente\n",
    "# Oppure lascia questi di default (conservativi per ridurre overfitting)\n",
    "\n",
    "best_params = {\n",
    "    'objective': 'binary:logistic',\n",
    "    'tree_method': 'hist',\n",
    "    'eval_metric': 'logloss',\n",
    "    'use_label_encoder': False,\n",
    "    'n_estimators': 1000,\n",
    "    'learning_rate': 0.019161090151695974,\n",
    "    'max_depth': 3,\n",
    "    'min_child_weight': 6,\n",
    "    'gamma': 2.4267729113636345,\n",
    "    'subsample': 0.6391418336680764,\n",
    "    'colsample_bytree': 0.8034979811909722,\n",
    "    'reg_alpha': 0.04656745646903133,\n",
    "    'reg_lambda': 0.25114366021463247\n",
    "}\n",
    "\n",
    "print(\"=== 10-Fold Cross-Validation (tutto il dataset) ===\")\n",
    "print(f\"Parametri utilizzati: {best_params}\\n\")\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import xgboost as xgb\n",
    "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "outer_accuracies = []\n",
    "folds_info = []\n",
    "train_accuracies = []\n",
    "train_val_gaps = []\n",
    "outer_accuracies_opt = []\n",
    "\n",
    "EARLY_STOPPING_ROUNDS = 50\n",
    "\n",
    "def best_threshold_for_accuracy(y_true, proba, n_grid=201):\n",
    "    y_true = np.asarray(y_true).astype(int)\n",
    "    proba = np.asarray(proba).astype(float)\n",
    "    grid = np.unique(np.quantile(proba, np.linspace(0, 1, n_grid)))\n",
    "    best_thr, best_acc = 0.5, 0.0\n",
    "    for t in grid:\n",
    "        acc = ( ((proba >= t).astype(int) == y_true).mean() )\n",
    "        if (acc > best_acc) or (abs(acc - best_acc) < 1e-12 and abs(t - 0.5) < abs(best_thr - 0.5)):\n",
    "            best_acc, best_thr = float(acc), float(t)\n",
    "    return best_thr, best_acc\n",
    "\n",
    "def _fit_with_es(clf, X_tr, y_tr, X_val, y_val):\n",
    "    \"\"\"Fit con EarlyStopping via callback se supportato; fallback senza ES.\"\"\"\n",
    "    try:\n",
    "        cb = getattr(xgb.callback, 'EarlyStopping', None)\n",
    "        if cb is not None:\n",
    "            clf.fit(X_tr, y_tr, eval_set=[(X_val, y_val)], callbacks=[cb(rounds=EARLY_STOPPING_ROUNDS, save_best=True, maximize=False)], verbose=False)\n",
    "            return True\n",
    "    except TypeError:\n",
    "        pass\n",
    "    clf.fit(X_tr, y_tr, eval_set=[(X_val, y_val)], verbose=False)\n",
    "    return False\n",
    "\n",
    "def _predict_proba_best(clf, X, best_iter=None, best_ntree_limit=None):\n",
    "    \"\"\"Version-safe predict_proba using either iteration_range (new) or ntree_limit (old).\"\"\"\n",
    "    try:\n",
    "        if best_iter is not None:\n",
    "            return clf.predict_proba(X, iteration_range=(0, int(best_iter)+1))[:, 1]\n",
    "    except TypeError:\n",
    "        pass\n",
    "    try:\n",
    "        if best_ntree_limit is not None:\n",
    "            return clf.predict_proba(X, ntree_limit=int(best_ntree_limit))[:, 1]\n",
    "    except TypeError:\n",
    "        pass\n",
    "    return clf.predict_proba(X)[:, 1]\n",
    "\n",
    "fold_idx = 0\n",
    "for train_idx, val_idx in skf.split(X, y):  # MODIFICATO: usa X, y invece di X_train_val, y_train_val\n",
    "    fold_idx += 1\n",
    "    X_tr, X_val = X[train_idx], X[val_idx]  # MODIFICATO\n",
    "    y_tr, y_val = y[train_idx], y[val_idx]  # MODIFICATO\n",
    "\n",
    "    clf = XGBClassifier(**best_params)\n",
    "    used_es = _fit_with_es(clf, X_tr, y_tr, X_val, y_val)\n",
    "\n",
    "    best_iter = getattr(clf, 'best_iteration', None)\n",
    "    try:\n",
    "        booster = clf.get_booster()\n",
    "    except Exception:\n",
    "        booster = None\n",
    "    best_ntree_limit = getattr(booster, 'best_ntree_limit', None) if booster is not None else None\n",
    "\n",
    "    y_val_proba = _predict_proba_best(clf, X_val, best_iter, best_ntree_limit)\n",
    "    y_pred = (y_val_proba >= 0.5).astype(int)\n",
    "    acc = accuracy_score(y_val, y_pred)\n",
    "    outer_accuracies.append(acc)\n",
    "\n",
    "    y_tr_proba = _predict_proba_best(clf, X_tr, best_iter, best_ntree_limit)\n",
    "    y_tr_pred = (y_tr_proba >= 0.5).astype(int)\n",
    "    tr_acc = accuracy_score(y_tr, y_tr_pred)\n",
    "    gap = float(tr_acc - acc)\n",
    "    train_accuracies.append(tr_acc)\n",
    "    train_val_gaps.append(gap)\n",
    "\n",
    "    thr_acc, acc_opt = best_threshold_for_accuracy(y_val, y_val_proba, n_grid=301)\n",
    "    outer_accuracies_opt.append(acc_opt)\n",
    "\n",
    "    val_index_global = val_idx\n",
    "    train_index_global = train_idx\n",
    "\n",
    "    folds_info.append({\n",
    "        'fold': fold_idx,\n",
    "        'acc': float(acc),\n",
    "        'train_acc': float(tr_acc),\n",
    "        'gap_train_minus_val': float(gap),\n",
    "        'acc_opt': float(acc_opt),\n",
    "        'thr_acc': float(thr_acc),\n",
    "        'best_iteration': int(best_iter) if best_iter is not None else None,\n",
    "        'train_idx': train_idx,\n",
    "        'val_idx': val_idx,\n",
    "        'train_index_global': train_index_global,\n",
    "        'val_index_global': val_index_global,\n",
    "        'y_true': y_val.astype(int),\n",
    "        'y_pred': y_pred.astype(int),\n",
    "        'y_proba': y_val_proba.astype(float)\n",
    "    })\n",
    "\n",
    "    es_tag = 'with ES' if used_es else 'no ES'\n",
    "    print(f'Fold {fold_idx}: {es_tag}, train={len(y_tr)}, val={len(y_val)}, acc_val={acc*100:.2f}%, acc_val_opt={acc_opt*100:.2f}% @thr={thr_acc:.3f}, acc_train={tr_acc*100:.2f}%, gap={(gap)*100:.2f}%')\n",
    "\n",
    "print('\\n' + '='*60)\n",
    "print('Risultati Cross-Validation')\n",
    "print('='*60)\n",
    "for i, a in enumerate(outer_accuracies, 1):\n",
    "    print(f'  Fold {i}: val_acc={a*100:.2f}%, val_acc_opt={outer_accuracies_opt[i-1]*100:.2f}% @thr={folds_info[i-1][\"thr_acc\"]:.3f}, train_acc={train_accuracies[i-1]*100:.2f}%, gap={train_val_gaps[i-1]*100:.2f}%')\n",
    "print(f'\\nMean CV accuracy (0.5): {np.mean(outer_accuracies)*100:.2f}%')\n",
    "print(f'Mean CV accuracy (opt thr): {np.mean(outer_accuracies_opt)*100:.2f}%')\n",
    "print(f'Mean train accuracy: {np.mean(train_accuracies)*100:.2f}%')\n",
    "print(f'Mean gap (train - val): {np.mean(train_val_gaps)*100:.2f}%')\n",
    "print(f'Std CV accuracy:  {np.std(outer_accuracies)*100:.2f}%')\n",
    "print(f'Min/Max val acc:  {np.min(outer_accuracies)*100:.2f}% / {np.max(outer_accuracies)*100:.2f}%')\n",
    "\n",
    "WORST_FOLD_IDX = int(np.argmin(outer_accuracies))\n",
    "WORST_FOLD_NUM = int(folds_info[WORST_FOLD_IDX]['fold'])\n",
    "print(f\"\\nPeggiore fold: #{WORST_FOLD_NUM} con acc_val={outer_accuracies[WORST_FOLD_IDX]*100:.2f}% | acc_val_opt={outer_accuracies_opt[WORST_FOLD_IDX]*100:.2f}% | acc_train={train_accuracies[WORST_FOLD_IDX]*100:.2f}% | gap={train_val_gaps[WORST_FOLD_IDX]*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8873db82",
   "metadata": {},
   "source": [
    "# Make submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b87173",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Submission rapida post-CV ===\n",
      "✅ File di submission salvato in submission.csv\n",
      "   battle_id  player_won\n",
      "0          0           0\n",
      "1          1           1\n",
      "2          2           1\n",
      "3          3           1\n",
      "4          4           1\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Submission con modello trainato su tutto il dataset ===\")\n",
    "\n",
    "# MODIFICATO: train su tutto il dataset\n",
    "cv_submission_model = XGBClassifier(**best_params, use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
    "cv_submission_model.fit(X, y)  # MODIFICATO: usa X, y invece di X_train_val, y_train_val\n",
    "\n",
    "test_aligned = test_df.reindex(columns=FEATURES, fill_value=0)\n",
    "X_test_matrix = test_aligned.astype(float).to_numpy()\n",
    "test_predictions = cv_submission_model.predict(X_test_matrix).astype(int)\n",
    "\n",
    "submission_df = pd.DataFrame({\n",
    "    'battle_id': test_df['battle_id'].astype(np.int64),\n",
    "    'player_won': test_predictions.astype(np.int64)\n",
    "})\n",
    "\n",
    "submission_path = 'submission.csv'\n",
    "submission_df.to_csv(submission_path, index=False)\n",
    "print(f\"✅ File di submission salvato in {submission_path}\")\n",
    "print(f\"Modello trainato su {len(X)} samples (dataset completo)\")\n",
    "print(f\"Stima CV accuracy: {np.mean(outer_accuracies)*100:.2f}% ± {np.std(outer_accuracies)*100:.2f}%\")\n",
    "print(submission_df.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
