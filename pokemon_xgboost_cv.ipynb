{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9b7eeca",
   "metadata": {},
   "source": [
    "# Pokémon battles — XGBoost with 10-fold outer CV\n",
    "Notebook breve che esegue: feature engineering, split train/val/test, 10-fold outer CV con GridSearchCV interno, valutazione per fold, valutazione su holdout e generazione submission.csv."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae11f50",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3aebef2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caricamento dati...\n",
      "Train records: 10000, Test records: 5000\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from xgboost import XGBClassifier\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# --- Percorsi (modificare se necessario) ---\n",
    "COMPETITION_NAME = 'fds-pokemon-battles-prediction-2025'\n",
    "train_file_path = 'train.jsonl'\n",
    "test_file_path = 'test.jsonl'\n",
    "\n",
    "def load_jsonl(path):\n",
    "    data = []\n",
    "    with open(path, 'r') as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line))\n",
    "    return data\n",
    "\n",
    "print('Caricamento dati...')\n",
    "train_raw = load_jsonl(train_file_path)\n",
    "test_raw = load_jsonl(test_file_path)\n",
    "print(f'Train records: {len(train_raw)}, Test records: {len(test_raw)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea38e58a",
   "metadata": {},
   "source": [
    "# Features engeneering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8e15c30c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FE: 100%|██████████| 10000/10000 [00:12<00:00, 781.46it/s]\n",
      "FE: 100%|██████████| 5000/5000 [00:05<00:00, 977.99it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape train/test: (10000, 246) (5000, 245)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>battle_id</th>\n",
       "      <th>player_won</th>\n",
       "      <th>p1_base_hp_sum</th>\n",
       "      <th>p1_base_hp_mean</th>\n",
       "      <th>p1_base_hp_max</th>\n",
       "      <th>p1_base_hp_min</th>\n",
       "      <th>p1_base_hp_std</th>\n",
       "      <th>p1_base_atk_sum</th>\n",
       "      <th>p1_base_atk_mean</th>\n",
       "      <th>p1_base_atk_max</th>\n",
       "      <th>...</th>\n",
       "      <th>team_hp_sum_minus_p2lead_hp</th>\n",
       "      <th>team_spa_mean_minus_p2spa</th>\n",
       "      <th>speed_advantage</th>\n",
       "      <th>n_unique_types_diff</th>\n",
       "      <th>damage_per_turn_diff</th>\n",
       "      <th>last_pair</th>\n",
       "      <th>p1_vs_lead_avg_effectiveness</th>\n",
       "      <th>p1_vs_lead_max_effectiveness</th>\n",
       "      <th>p1_super_effective_options</th>\n",
       "      <th>p1_se_options_vs_lead_bulk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>695.0</td>\n",
       "      <td>115.833333</td>\n",
       "      <td>250.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>69.367179</td>\n",
       "      <td>435.0</td>\n",
       "      <td>72.500000</td>\n",
       "      <td>110.0</td>\n",
       "      <td>...</td>\n",
       "      <td>635.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>365.0</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.070393</td>\n",
       "      <td>starmie_VS_snorlax</td>\n",
       "      <td>1.083333</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.005405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>740.0</td>\n",
       "      <td>123.333333</td>\n",
       "      <td>250.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>64.204534</td>\n",
       "      <td>435.0</td>\n",
       "      <td>72.500000</td>\n",
       "      <td>110.0</td>\n",
       "      <td>...</td>\n",
       "      <td>685.0</td>\n",
       "      <td>-45.000000</td>\n",
       "      <td>250.0</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.012174</td>\n",
       "      <td>tauros_VS_alakazam</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>745.0</td>\n",
       "      <td>124.166667</td>\n",
       "      <td>250.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>64.382753</td>\n",
       "      <td>505.0</td>\n",
       "      <td>84.166667</td>\n",
       "      <td>130.0</td>\n",
       "      <td>...</td>\n",
       "      <td>495.0</td>\n",
       "      <td>-15.000000</td>\n",
       "      <td>345.0</td>\n",
       "      <td>6</td>\n",
       "      <td>-0.000690</td>\n",
       "      <td>snorlax_VS_gengar</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>730.0</td>\n",
       "      <td>121.666667</td>\n",
       "      <td>250.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>65.362239</td>\n",
       "      <td>465.0</td>\n",
       "      <td>77.500000</td>\n",
       "      <td>110.0</td>\n",
       "      <td>...</td>\n",
       "      <td>655.0</td>\n",
       "      <td>33.333333</td>\n",
       "      <td>345.0</td>\n",
       "      <td>6</td>\n",
       "      <td>-0.014574</td>\n",
       "      <td>snorlax_VS_zapdos</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>685.0</td>\n",
       "      <td>114.166667</td>\n",
       "      <td>250.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>70.794107</td>\n",
       "      <td>455.0</td>\n",
       "      <td>75.833333</td>\n",
       "      <td>110.0</td>\n",
       "      <td>...</td>\n",
       "      <td>625.0</td>\n",
       "      <td>-2.500000</td>\n",
       "      <td>320.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.006923</td>\n",
       "      <td>tauros_VS_chansey</td>\n",
       "      <td>1.083333</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.005405</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 246 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   battle_id  player_won  p1_base_hp_sum  p1_base_hp_mean  p1_base_hp_max  \\\n",
       "0          0           1           695.0       115.833333           250.0   \n",
       "1          1           1           740.0       123.333333           250.0   \n",
       "2          2           1           745.0       124.166667           250.0   \n",
       "3          3           1           730.0       121.666667           250.0   \n",
       "4          4           1           685.0       114.166667           250.0   \n",
       "\n",
       "   p1_base_hp_min  p1_base_hp_std  p1_base_atk_sum  p1_base_atk_mean  \\\n",
       "0            55.0       69.367179            435.0         72.500000   \n",
       "1            65.0       64.204534            435.0         72.500000   \n",
       "2            60.0       64.382753            505.0         84.166667   \n",
       "3            60.0       65.362239            465.0         77.500000   \n",
       "4            50.0       70.794107            455.0         75.833333   \n",
       "\n",
       "   p1_base_atk_max  ...  team_hp_sum_minus_p2lead_hp  \\\n",
       "0            110.0  ...                        635.0   \n",
       "1            110.0  ...                        685.0   \n",
       "2            130.0  ...                        495.0   \n",
       "3            110.0  ...                        655.0   \n",
       "4            110.0  ...                        625.0   \n",
       "\n",
       "   team_spa_mean_minus_p2spa  speed_advantage  n_unique_types_diff  \\\n",
       "0                   0.000000            365.0                    3   \n",
       "1                 -45.000000            250.0                    4   \n",
       "2                 -15.000000            345.0                    6   \n",
       "3                  33.333333            345.0                    6   \n",
       "4                  -2.500000            320.0                    4   \n",
       "\n",
       "   damage_per_turn_diff           last_pair  p1_vs_lead_avg_effectiveness  \\\n",
       "0             -0.070393  starmie_VS_snorlax                      1.083333   \n",
       "1             -0.012174  tauros_VS_alakazam                      1.000000   \n",
       "2             -0.000690   snorlax_VS_gengar                      1.000000   \n",
       "3             -0.014574   snorlax_VS_zapdos                      1.000000   \n",
       "4              0.006923   tauros_VS_chansey                      1.083333   \n",
       "\n",
       "   p1_vs_lead_max_effectiveness  p1_super_effective_options  \\\n",
       "0                           2.0                           1   \n",
       "1                           1.0                           0   \n",
       "2                           1.0                           0   \n",
       "3                           1.0                           0   \n",
       "4                           2.0                           1   \n",
       "\n",
       "   p1_se_options_vs_lead_bulk  \n",
       "0                    0.005405  \n",
       "1                    0.000000  \n",
       "2                    0.000000  \n",
       "3                    0.000000  \n",
       "4                    0.005405  \n",
       "\n",
       "[5 rows x 246 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import math\n",
    "from collections import Counter\n",
    "\n",
    "# === TYPE CHART (Gen 1) ===\n",
    "TYPE_CHART = {\n",
    "    'normal': {'rock': 0.5, 'ghost': 0},\n",
    "    'fire': {'fire': 0.5, 'water': 0.5, 'grass': 2, 'ice': 2, 'bug': 2, 'rock': 0.5, 'dragon': 0.5},\n",
    "    'water': {'fire': 2, 'water': 0.5, 'grass': 0.5, 'ground': 2, 'rock': 2, 'dragon': 0.5},\n",
    "    'grass': {'fire': 0.5, 'water': 2, 'grass': 0.5, 'poison': 0.5, 'ground': 2, 'flying': 0.5, 'bug': 0.5, 'rock': 2, 'dragon': 0.5},\n",
    "    'electric': {'water': 2, 'grass': 0.5, 'electric': 0.5, 'ground': 0, 'flying': 2, 'dragon': 0.5},\n",
    "    'ice': {'fire': 0.5, 'water': 0.5, 'grass': 2, 'ground': 2, 'flying': 2, 'dragon': 2},\n",
    "    'fighting': {'normal': 2, 'ice': 2, 'poison': 0.5, 'flying': 0.5, 'psychic': 0.5, 'bug': 0.5, 'rock': 2, 'ghost': 0},\n",
    "    'poison': {'grass': 2, 'poison': 0.5, 'ground': 0.5, 'bug': 2, 'rock': 0.5, 'ghost': 0.5},\n",
    "    'ground': {'fire': 2, 'grass': 0.5, 'electric': 2, 'poison': 2, 'flying': 0, 'bug': 0.5, 'rock': 2},\n",
    "    'flying': {'grass': 2, 'electric': 0.5, 'fighting': 2, 'bug': 2, 'rock': 0.5},\n",
    "    'psychic': {'fighting': 2, 'poison': 2, 'psychic': 0.5, 'ghost': 0},\n",
    "    'bug': {'fire': 0.5, 'grass': 2, 'fighting': 0.5, 'poison': 2, 'flying': 0.5, 'psychic': 2, 'ghost': 0.5},\n",
    "    'rock': {'fire': 2, 'ice': 2, 'fighting': 0.5, 'ground': 0.5, 'flying': 2, 'bug': 2},\n",
    "    'ghost': {'normal': 0, 'psychic': 0, 'ghost': 2},\n",
    "    'dragon': {'dragon': 2}\n",
    "}\n",
    "\n",
    "def get_effectiveness(attack_type: str, defense_types: list) -> float:\n",
    "    if not attack_type or not defense_types:\n",
    "        return 1.0\n",
    "    eff = 1.0\n",
    "    for d in defense_types:\n",
    "        eff *= TYPE_CHART.get(attack_type, {}).get(d, 1.0)\n",
    "    return eff\n",
    "\n",
    "def calculate_type_advantage(team1: list, team2_lead: dict) -> dict:\n",
    "    out = {'p1_vs_lead_avg_effectiveness': 0.0, 'p1_vs_lead_max_effectiveness': 0.0, 'p1_super_effective_options': 0}\n",
    "    if not team1 or not team2_lead:\n",
    "        return out\n",
    "    lead_types = [t.lower() for t in team2_lead.get('types', [])]\n",
    "    if not lead_types:\n",
    "        return out\n",
    "    effs = []\n",
    "    for p in team1:\n",
    "        p_types = [t.lower() for t in p.get('types', [])]\n",
    "        max_eff = 0.0\n",
    "        for pt in p_types:\n",
    "            max_eff = max(max_eff, get_effectiveness(pt, lead_types))\n",
    "        effs.append(max_eff)\n",
    "    if not effs:\n",
    "        return out\n",
    "    out['p1_vs_lead_avg_effectiveness'] = float(np.mean(effs))\n",
    "    out['p1_vs_lead_max_effectiveness'] = float(np.max(effs))\n",
    "    out['p1_super_effective_options'] = int(sum(1 for e in effs if e >= 2))\n",
    "    return out\n",
    "\n",
    "def _entropy(counter: Counter) -> float:\n",
    "    total = sum(counter.values())\n",
    "    if total == 0:\n",
    "        return 0.0\n",
    "    ent = 0.0\n",
    "    for v in counter.values():\n",
    "        p = v / total\n",
    "        if p > 0:\n",
    "            ent -= p * math.log(p, 2)\n",
    "    return ent\n",
    "\n",
    "def team_aggregate_features(team: list, prefix: str = 'p1_') -> dict:\n",
    "    stats = ['base_hp','base_atk','base_def','base_spa','base_spd','base_spe']\n",
    "    out = {}\n",
    "    vals = {s: [] for s in stats}\n",
    "    levels = []\n",
    "    types_counter = Counter()\n",
    "    names = []\n",
    "    for p in team:\n",
    "        names.append(p.get('name',''))\n",
    "        for s in stats:\n",
    "            vals[s].append(p.get(s, 0))\n",
    "        levels.append(p.get('level', 0))\n",
    "        for t in p.get('types', []):\n",
    "            types_counter[t.lower()] += 1\n",
    "    for s in stats:\n",
    "        arr = np.array(vals[s], dtype=float)\n",
    "        out[f'{prefix}{s}_sum'] = float(arr.sum())\n",
    "        out[f'{prefix}{s}_mean'] = float(arr.mean())\n",
    "        out[f'{prefix}{s}_max'] = float(arr.max())\n",
    "        out[f'{prefix}{s}_min'] = float(arr.min())\n",
    "        out[f'{prefix}{s}_std'] = float(arr.std())\n",
    "    level_arr = np.array(levels, dtype=float)\n",
    "    out[f'{prefix}level_mean'] = float(level_arr.mean()) if level_arr.size else 0.0\n",
    "    out[f'{prefix}level_sum'] = float(level_arr.sum()) if level_arr.size else 0.0\n",
    "    out[f'{prefix}n_unique_types'] = int(len(types_counter))\n",
    "    common_types = ['normal','fire','water','electric','grass','psychic','ice','dragon','rock','ground','flying']\n",
    "    for t in common_types:\n",
    "        out[f'{prefix}type_{t}_count'] = int(types_counter.get(t, 0))\n",
    "    out[f'{prefix}lead_name'] = names[0] if names else ''\n",
    "    out[f'{prefix}n_unique_names'] = int(len(set(names)))\n",
    "    out[f'{prefix}type_entropy'] = float(_entropy(types_counter))\n",
    "    spe_arr = np.array(vals['base_spe'], dtype=float)\n",
    "    out[f'{prefix}spe_p25'] = float(np.percentile(spe_arr, 25)) if spe_arr.size else 0.0\n",
    "    out[f'{prefix}spe_p50'] = float(np.percentile(spe_arr, 50)) if spe_arr.size else 0.0\n",
    "    out[f'{prefix}spe_p75'] = float(np.percentile(spe_arr, 75)) if spe_arr.size else 0.0\n",
    "    return out\n",
    "\n",
    "def lead_vs_lead_features(p1_lead: dict, p2_lead: dict) -> dict:\n",
    "    out = {}\n",
    "    stats = ['base_hp','base_atk','base_def','base_spa','base_spd','base_spe']\n",
    "    for s in stats:\n",
    "        out[f'lead_diff_{s}'] = float(p1_lead.get(s,0) - p2_lead.get(s,0))\n",
    "    out['lead_speed_advantage'] = float(p1_lead.get('base_spe',0) - p2_lead.get('base_spe',0))\n",
    "    p1_types = [t.lower() for t in p1_lead.get('types', [])]\n",
    "    p2_types = [t.lower() for t in p2_lead.get('types', [])]\n",
    "    max_eff = 0.0\n",
    "    for pt in p1_types:\n",
    "        max_eff = max(max_eff, get_effectiveness(pt, p2_types))\n",
    "    out['lead_p1_vs_p2_effectiveness'] = float(max_eff)\n",
    "    return out\n",
    "\n",
    "def lead_aggregate_features(pokemon: dict, prefix: str = 'p2_lead_') -> dict:\n",
    "    out = {}\n",
    "    stats = ['base_hp','base_atk','base_def','base_spa','base_spd','base_spe']\n",
    "    for s in stats:\n",
    "        out[f'{prefix}{s}'] = float(pokemon.get(s,0))\n",
    "    out[f'{prefix}level'] = int(pokemon.get('level',0))\n",
    "    types = [x.lower() for x in pokemon.get('types', [])]\n",
    "    common_types = ['normal','fire','water','electric','grass','psychic','ice','dragon','rock','ground','flying']\n",
    "    for t in common_types:\n",
    "        out[f'{prefix}type_{t}'] = int(t in types)\n",
    "    out[f'{prefix}name'] = pokemon.get('name','')\n",
    "    out[f'{prefix}n_unique_types'] = int(len(set(types)))\n",
    "    return out\n",
    "\n",
    "def summary_from_timeline(timeline: list, p1_team: list) -> dict:\n",
    "    out = {}\n",
    "    if not timeline:\n",
    "        return {'tl_p1_moves':0,'tl_p2_moves':0,'tl_p1_est_damage':0.0,'tl_p2_est_damage':0.0,'damage_diff':0.0}\n",
    "    p1_moves = p2_moves = 0\n",
    "    p1_damage = p2_damage = 0.0\n",
    "    p1_last_active = p2_last_active = ''\n",
    "    p1_last_hp = p2_last_hp = np.nan\n",
    "    p1_fainted = p2_fainted = 0\n",
    "    p1_fainted_names = set()\n",
    "    p2_fainted_names = set()\n",
    "    last_p1_hp = {}\n",
    "    last_p2_hp = {}\n",
    "    p1_comeback_kos = 0\n",
    "    p2_comeback_kos = 0\n",
    "    p1_inflicted_statuses = Counter()\n",
    "    p2_inflicted_statuses = Counter()\n",
    "    p1_pokemon_statuses = {}\n",
    "    p2_pokemon_statuses = {}\n",
    "    p1_move_type_counts = Counter()\n",
    "    p2_move_type_counts = Counter()\n",
    "    p1_damage_first2 = 0.0\n",
    "    p2_damage_first2 = 0.0\n",
    "\n",
    "    # NEW: per-turn damage accumulation, KO timing and early/late KO counters\n",
    "    p1_dmg_by_turn = {}  # damage inflitto da p1 (contro p2) per turno\n",
    "    p2_dmg_by_turn = {}  # damage inflitto da p2 (contro p1) per turno\n",
    "    seen_turns = set()\n",
    "    first_ko_turn_p1_taken = None   # primo KO subìto da p1 (p1_fainted++)\n",
    "    first_ko_turn_p1_inflicted = None  # primo KO inflitto da p1 (p2_fainted++)\n",
    "    early_threshold = 10\n",
    "    p1_kos_early = p1_kos_late = 0\n",
    "    p2_kos_early = p2_kos_late = 0\n",
    "    # NEW: counters per-tipo di mosse (special/physical/status), priority, critical\n",
    "    p1_special_moves = 0\n",
    "    p2_special_moves = 0\n",
    "    p1_status_moves = 0\n",
    "    p2_status_moves = 0\n",
    "    p1_priority_moves = 0\n",
    "    p2_priority_moves = 0\n",
    "    p1_critical_hits = 0\n",
    "    p2_critical_hits = 0\n",
    "\n",
    "    for turn in timeline[:30]:\n",
    "        prev_p1_fainted, prev_p2_fainted = p1_fainted, p2_fainted\n",
    "        p1_state = turn.get('p1_pokemon_state',{}) or {}\n",
    "        p2_state = turn.get('p2_pokemon_state',{}) or {}\n",
    "        tnum = turn.get('turn', None)\n",
    "        if tnum is None:\n",
    "            # fallback: usa lunghezza dei turni visti + 1\n",
    "            tnum = (len(seen_turns) + 1)\n",
    "        seen_turns.add(tnum)\n",
    "\n",
    "        if p1_state.get('name'):\n",
    "            p1_last_active = p1_state.get('name')\n",
    "        if p2_state.get('name'):\n",
    "            p2_last_active = p2_state.get('name')\n",
    "\n",
    "        if p1_state.get('fainted') and p1_state.get('name') not in p1_fainted_names:\n",
    "            p1_fainted += 1\n",
    "            p1_fainted_names.add(p1_state.get('name'))\n",
    "            if first_ko_turn_p1_taken is None:\n",
    "                first_ko_turn_p1_taken = tnum\n",
    "            if tnum <= early_threshold: p2_kos_early += 1\n",
    "            else: p2_kos_late += 1\n",
    "        if p2_state.get('fainted') and p2_state.get('name') not in p2_fainted_names:\n",
    "            p2_fainted += 1\n",
    "            p2_fainted_names.add(p2_state.get('name'))\n",
    "            if first_ko_turn_p1_inflicted is None:\n",
    "                first_ko_turn_p1_inflicted = tnum\n",
    "            if tnum <= early_threshold: p1_kos_early += 1\n",
    "            else: p1_kos_late += 1\n",
    "\n",
    "        p2_name, p2_hp = p2_state.get('name'), p2_state.get('hp_pct')\n",
    "        if p2_name and p2_hp is not None:\n",
    "            prev_hp = last_p2_hp.get(p2_name)\n",
    "            if prev_hp is not None:\n",
    "                delta = max(0.0, prev_hp - p2_hp)\n",
    "                p1_damage += delta\n",
    "                p1_dmg_by_turn[tnum] = p1_dmg_by_turn.get(tnum, 0.0) + delta\n",
    "                if turn.get('turn',999) <= 2:\n",
    "                    p1_damage_first2 += delta\n",
    "            last_p2_hp[p2_name] = p2_hp\n",
    "\n",
    "        p1_name, p1_hp = p1_state.get('name'), p1_state.get('hp_pct')\n",
    "        if p1_name and p1_hp is not None:\n",
    "            prev_hp = last_p1_hp.get(p1_name)\n",
    "            if prev_hp is not None:\n",
    "                delta = max(0.0, prev_hp - p1_hp)\n",
    "                p2_damage += delta\n",
    "                p2_dmg_by_turn[tnum] = p2_dmg_by_turn.get(tnum, 0.0) + delta\n",
    "                if turn.get('turn',999) <= 2:\n",
    "                    p2_damage_first2 += delta\n",
    "            last_p1_hp[p1_name] = p1_hp\n",
    "\n",
    "        damage_diff_so_far = p1_damage - p2_damage\n",
    "        if p2_fainted > prev_p2_fainted and damage_diff_so_far < -1.0:\n",
    "            p1_comeback_kos += 1\n",
    "        if p1_fainted > prev_p1_fainted and damage_diff_so_far > 1.0:\n",
    "            p2_comeback_kos += 1\n",
    "\n",
    "        p2_status = p2_state.get('status')\n",
    "        if p2_name and p2_status and p2_pokemon_statuses.get(p2_name) != p2_status:\n",
    "            p1_inflicted_statuses[p2_status] += 1\n",
    "            p2_pokemon_statuses[p2_name] = p2_status\n",
    "        p1_status = p1_state.get('status')\n",
    "        if p1_name and p1_status and p1_pokemon_statuses.get(p1_name) != p1_status:\n",
    "            p2_inflicted_statuses[p1_status] += 1\n",
    "            p1_pokemon_statuses[p1_name] = p1_status\n",
    "\n",
    "        p1_move = turn.get('p1_move_details') or {}\n",
    "        p2_move = turn.get('p2_move_details') or {}\n",
    "        if p1_move and p1_move.get('type'):\n",
    "            p1_move_type_counts[(p1_move.get('type') or '').lower()] += 1\n",
    "        if p2_move and p2_move.get('type'):\n",
    "            p2_move_type_counts[(p2_move.get('type') or '').lower()] += 1\n",
    "        if turn.get('p1_move_details'):\n",
    "            p1_moves += 1\n",
    "        if turn.get('p2_move_details'):\n",
    "            p2_moves += 1\n",
    "\n",
    "        # --- INIZIO: conta special/status/priority/critical se presenti nei dettagli della mossa ---\n",
    "        # Adatta le chiavi ('category','is_status','priority','is_critical') se i tuoi dati usano nomi diversi\n",
    "        if p1_move:\n",
    "            cat = (p1_move.get('category') or '').lower()\n",
    "            if cat == 'special':\n",
    "                p1_special_moves += 1\n",
    "            if p1_move.get('is_status') or cat == 'status':\n",
    "                p1_status_moves += 1\n",
    "            if (p1_move.get('priority') or 0) > 0:\n",
    "                p1_priority_moves += 1\n",
    "            if p1_move.get('is_critical') or p1_move.get('critical', False):\n",
    "                p1_critical_hits += 1\n",
    "        if p2_move:\n",
    "            cat = (p2_move.get('category') or '').lower()\n",
    "            if cat == 'special':\n",
    "                p2_special_moves += 1\n",
    "            if p2_move.get('is_status') or cat == 'status':\n",
    "                p2_status_moves += 1\n",
    "            if (p2_move.get('priority') or 0) > 0:\n",
    "                p2_priority_moves += 1\n",
    "            if p2_move.get('is_critical') or p2_move.get('critical', False):\n",
    "                p2_critical_hits += 1\n",
    "        # --- FINE: conta special/status/priority/critical ---\n",
    "\n",
    "        p1_last_hp = p1_state.get('hp_pct', np.nan)\n",
    "        p2_last_hp = p2_state.get('hp_pct', np.nan)\n",
    "\n",
    "    # ...existing code computing out[...] baseline metrics...\n",
    "    out['tl_p1_moves'] = int(p1_moves)\n",
    "    out['tl_p2_moves'] = int(p2_moves)\n",
    "    out['tl_p1_est_damage'] = float(p1_damage)\n",
    "    out['tl_p2_est_damage'] = float(p2_damage)\n",
    "    # NUOVE FEATURE: conteggio KO per squadra e rate normalizzati per turno\n",
    "    out['tl_p1_fainted'] = int(p1_fainted)\n",
    "    out['tl_p2_fainted'] = int(p2_fainted)\n",
    "    turns_count = max(1, len(seen_turns))\n",
    "    out['tl_p1_fainted_rate'] = float(out['tl_p1_fainted'] / turns_count)\n",
    "    out['tl_p2_fainted_rate'] = float(out['tl_p2_fainted'] / turns_count)\n",
    "    # fine nuovi features\n",
    "    out['damage_diff'] = float(p1_damage - p2_damage)\n",
    "    out['fainted_diff'] = int(p1_fainted - p2_fainted)\n",
    "    out['tl_p1_last_hp'] = float(p1_last_hp) if not np.isnan(p1_last_hp) else 0.0\n",
    "    out['tl_p2_last_hp'] = float(p2_last_hp) if not np.isnan(p2_last_hp) else 0.0\n",
    "    out['tl_p1_last_active'] = p1_last_active\n",
    "    out['tl_p2_last_active'] = p2_last_active\n",
    "    if p1_team:\n",
    "        p1_total_hp_sum = sum(p.get('base_hp',0) for p in p1_team)\n",
    "        p1_avg_def = np.mean([p.get('base_def',0) for p in p1_team] or [0])\n",
    "        p1_avg_spd = np.mean([p.get('base_spd',0) for p in p1_team] or [0])\n",
    "        out['tl_p2_damage_vs_p1_hp_pool'] = float(p2_damage / (p1_total_hp_sum + 1e-6))\n",
    "        out['tl_p1_defensive_endurance'] = float((p1_avg_def + p1_avg_spd) / (p2_damage + 1e-6))\n",
    "    out['tl_p1_comeback_kos'] = int(p1_comeback_kos)\n",
    "    out['tl_p2_comeback_kos'] = int(p2_comeback_kos)\n",
    "    out['tl_comeback_kos_diff'] = int(p1_comeback_kos - p2_comeback_kos)\n",
    "\n",
    "    common_statuses = ['brn','par','slp','frz','psn','tox']\n",
    "    for status in common_statuses:\n",
    "        out[f'tl_p1_inflicted_{status}_count'] = int(p1_inflicted_statuses.get(status,0))\n",
    "        out[f'tl_p2_inflicted_{status}_count'] = int(p2_inflicted_statuses.get(status,0))\n",
    "        out[f'tl_inflicted_{status}_diff'] = int(p1_inflicted_statuses.get(status,0) - p2_inflicted_statuses.get(status,0))\n",
    "\n",
    "    common_move_types = ['normal','fire','water','electric','grass','psychic','ice','dragon','rock','ground','flying','ghost','bug','poison','fighting']\n",
    "    for mt in common_move_types:\n",
    "        out[f'tl_p1_move_type_{mt}_count'] = int(p1_move_type_counts.get(mt,0))\n",
    "        out[f'tl_p2_move_type_{mt}_count'] = int(p2_move_type_counts.get(mt,0))\n",
    "        out[f'tl_move_type_{mt}_count_diff'] = int(p1_move_type_counts.get(mt,0) - p2_move_type_counts.get(mt,0))\n",
    "\n",
    "    out['tl_p1_damage_first2'] = float(p1_damage_first2)\n",
    "    out['tl_p2_damage_first2'] = float(p2_damage_first2)\n",
    "    out['tl_first2_damage_diff'] = float(p1_damage_first2 - p2_damage_first2)\n",
    "\n",
    "    # --- INIZIO: esporta contatori mosse special/status/priority/critical ---\n",
    "    out['tl_p1_special_moves'] = int(p1_special_moves)\n",
    "    out['tl_p2_special_moves'] = int(p2_special_moves)\n",
    "    out['tl_p1_status_moves'] = int(p1_status_moves)\n",
    "    out['tl_p2_status_moves'] = int(p2_status_moves)\n",
    "    out['tl_p1_priority_moves'] = int(p1_priority_moves)\n",
    "    out['tl_p2_priority_moves'] = int(p2_priority_moves)\n",
    "    out['tl_p1_critical_hits'] = int(p1_critical_hits)\n",
    "    out['tl_p2_critical_hits'] = int(p2_critical_hits)\n",
    "    # rate normalizzate per turno\n",
    "    turns_count = max(1, len(seen_turns))\n",
    "    out['tl_p1_special_moves_rate'] = float(p1_special_moves / turns_count)\n",
    "    out['tl_p2_special_moves_rate'] = float(p2_special_moves / turns_count)\n",
    "\n",
    "    # NEW: derived, normalized and late-game features\n",
    "    turns_count = max(1, len(seen_turns))\n",
    "    out['tl_turns_count'] = int(turns_count)\n",
    "    out['tl_p1_moves_rate'] = float(p1_moves / turns_count)\n",
    "    out['tl_p2_moves_rate'] = float(p2_moves / turns_count)\n",
    "    out['tl_p1_damage_per_turn'] = float(p1_damage / turns_count)\n",
    "    out['tl_p2_damage_per_turn'] = float(p2_damage / turns_count)\n",
    "    out['tl_damage_rate_diff'] = float(out['tl_p1_damage_per_turn'] - out['tl_p2_damage_per_turn'])\n",
    "\n",
    "    # last-5-turns damage window\n",
    "    if seen_turns:\n",
    "        recent_turns = sorted(seen_turns)[-5:]\n",
    "        p1_last5 = sum(p1_dmg_by_turn.get(t,0.0) for t in recent_turns)\n",
    "        p2_last5 = sum(p2_dmg_by_turn.get(t,0.0) for t in recent_turns)\n",
    "    else:\n",
    "        p1_last5 = p2_last5 = 0.0\n",
    "    out['tl_p1_damage_last5'] = float(p1_last5)\n",
    "    out['tl_p2_damage_last5'] = float(p2_last5)\n",
    "    out['tl_last5_damage_diff'] = float(p1_last5 - p2_last5)\n",
    "    # NEW: ratio danno ultimi 5 turni vs totale\n",
    "    out['tl_p1_last5_damage_ratio'] = float(p1_last5 / (p1_damage + 1e-6))\n",
    "    out['tl_p2_last5_damage_ratio'] = float(p2_last5 / (p2_damage + 1e-6))\n",
    "    out['tl_last5_damage_ratio_diff'] = float(out['tl_p1_last5_damage_ratio'] - out['tl_p2_last5_damage_ratio'])\n",
    "\n",
    "    # time-weighted damage advantage (peso crescente con il turno)\n",
    "    if seen_turns:\n",
    "        ts = sorted(seen_turns)\n",
    "        w = np.linspace(1.0, 2.0, num=len(ts))  # pesi crescenti\n",
    "        w = w / (w.sum() + 1e-9)\n",
    "        adv = [(p1_dmg_by_turn.get(t,0.0) - p2_dmg_by_turn.get(t,0.0)) for t in ts]\n",
    "        out['tl_weighted_damage_diff'] = float(np.dot(w, adv))\n",
    "    else:\n",
    "        out['tl_weighted_damage_diff'] = 0.0\n",
    "\n",
    "    # NEW: comeback indicator (cambio di segno dell'adv cumulativo)\n",
    "    if seen_turns:\n",
    "        ts = sorted(seen_turns)\n",
    "        cum = 0.0\n",
    "        signs = []\n",
    "        for t in ts:\n",
    "            cum += (p1_dmg_by_turn.get(t,0.0) - p2_dmg_by_turn.get(t,0.0))\n",
    "            s = 1 if cum > 1e-9 else (-1 if cum < -1e-9 else 0)\n",
    "            if s != 0:\n",
    "                if not signs or signs[-1] != s:\n",
    "                    signs.append(s)\n",
    "        sign_flips = max(0, len(signs) - 1)\n",
    "        comeback_flag = 1 if (len(signs) >= 2 and signs[0] != signs[-1]) else 0\n",
    "    else:\n",
    "        sign_flips = 0\n",
    "        comeback_flag = 0\n",
    "    out['tl_damage_adv_sign_flips'] = int(sign_flips)\n",
    "    out['tl_comeback_flag'] = int(comeback_flag)\n",
    "\n",
    "    # KO timing and early/late counts\n",
    "    out['tl_first_ko_turn_p1_inflicted'] = int(first_ko_turn_p1_inflicted or 0)\n",
    "    out['tl_first_ko_turn_p1_taken'] = int(first_ko_turn_p1_taken or 0)\n",
    "    out['tl_first_ko_turn_diff'] = int((first_ko_turn_p1_inflicted or 0) - (first_ko_turn_p1_taken or 0))\n",
    "    out['tl_kos_early_p1'] = int(p1_kos_early)\n",
    "    out['tl_kos_late_p1'] = int(p1_kos_late)\n",
    "    out['tl_kos_early_p2'] = int(p2_kos_early)\n",
    "    out['tl_kos_late_p2'] = int(p2_kos_late)\n",
    "\n",
    "    # normalized status rates per turn\n",
    "    for status in common_statuses:\n",
    "        c1 = p1_inflicted_statuses.get(status,0)\n",
    "        c2 = p2_inflicted_statuses.get(status,0)\n",
    "        out[f'tl_p1_inflicted_{status}_rate'] = float(c1 / turns_count)\n",
    "        out[f'tl_p2_inflicted_{status}_rate'] = float(c2 / turns_count)\n",
    "        out[f'tl_inflicted_{status}_rate_diff'] = float((c1 - c2) / turns_count)\n",
    "\n",
    "    return out\n",
    "\n",
    "def ability_features(team: list, prefix: str) -> dict:\n",
    "    immunity_abilities = {'levitate':0,'volt_absorb':0,'water_absorb':0,'flash_fire':0}\n",
    "    stat_drop_abilities = {'intimidate':0}\n",
    "    weather_abilities = {'drought':0,'drizzle':0,'sand_stream':0}\n",
    "    out = {}\n",
    "    for pokemon in team:\n",
    "        ability = (pokemon.get('ability','') or '').lower().replace(' ','_')\n",
    "        if ability in immunity_abilities:\n",
    "            immunity_abilities[ability] += 1\n",
    "        if ability in stat_drop_abilities:\n",
    "            stat_drop_abilities[ability] += 1\n",
    "        if ability in weather_abilities:\n",
    "            weather_abilities[ability] += 1\n",
    "    for ability,count in immunity_abilities.items():\n",
    "        out[f'{prefix}ability_{ability}_count'] = int(count)\n",
    "    for ability,count in stat_drop_abilities.items():\n",
    "        out[f'{prefix}ability_{ability}_count'] = int(count)\n",
    "    for ability,count in weather_abilities.items():\n",
    "        out[f'{prefix}ability_{ability}_count'] = int(count)\n",
    "    out[f'{prefix}total_immunity_abilities'] = int(sum(immunity_abilities.values()))\n",
    "    out[f'{prefix}total_stat_drop_abilities'] = int(sum(stat_drop_abilities.values()))\n",
    "    return out\n",
    "\n",
    "def prepare_record_features(record: dict, max_turns: int = 30) -> dict:\n",
    "    out = {}\n",
    "    out['battle_id'] = record.get('battle_id')\n",
    "    if 'player_won' in record:\n",
    "        out['player_won'] = int(bool(record.get('player_won')))\n",
    "    p1_team = record.get('p1_team_details', [])\n",
    "    out.update(team_aggregate_features(p1_team, prefix='p1_'))\n",
    "    p2_lead = record.get('p2_lead_details', {})\n",
    "    out.update(lead_aggregate_features(p2_lead, prefix='p2_lead_'))\n",
    "    out.update(ability_features(p1_team, prefix='p1_'))\n",
    "    p1_lead = p1_team[0] if p1_team else {}\n",
    "    out.update(lead_vs_lead_features(p1_lead, p2_lead))\n",
    "    out.update(ability_features([p2_lead], prefix='p2_lead_'))\n",
    "    out['p1_intimidate_vs_lead'] = 1 if out.get('p1_ability_intimidate_count',0) > 0 else 0\n",
    "    tl = record.get('battle_timeline', [])\n",
    "    out.update(summary_from_timeline(tl[:max_turns], p1_team))\n",
    "    out['team_hp_sum_minus_p2lead_hp'] = out.get('p1_base_hp_sum', 0) - out.get('p2_lead_base_hp', 0)\n",
    "    out['team_spa_mean_minus_p2spa'] = out.get('p1_base_spa_mean', 0) - out.get('p2_lead_base_spa', 0)\n",
    "    out['speed_advantage'] = out.get('p1_base_spe_sum', 0) - out.get('p2_lead_base_spe', 0)\n",
    "    out['n_unique_types_diff'] = out.get('p1_n_unique_types', 0) - out.get('p2_lead_n_unique_types', 1)\n",
    "    p1_moves = max(out.get('tl_p1_moves',1),1)\n",
    "    p2_moves = max(out.get('tl_p2_moves',1),1)\n",
    "    out['damage_per_turn_diff'] = (out.get('tl_p1_est_damage',0.0)/p1_moves) - (out.get('tl_p2_est_damage',0.0)/p2_moves)\n",
    "    out['last_pair'] = f\"{out.get('tl_p1_last_active','')}_VS_{out.get('tl_p2_last_active','')}\"\n",
    "    out.update(calculate_type_advantage(p1_team, p2_lead))\n",
    "    p2_lead_bulk = out.get('p2_lead_base_def',1) + out.get('p2_lead_base_spd',1)\n",
    "    out['p1_se_options_vs_lead_bulk'] = out.get('p1_super_effective_options',0) / (p2_lead_bulk + 1e-6)\n",
    "    p2_team = record.get('p2_team_details', [])\n",
    "    if p2_team:\n",
    "        out.update(team_aggregate_features(p2_team, prefix='p2_'))\n",
    "        out['team_hp_sum_diff'] = out.get('p1_base_hp_sum',0) - out.get('p2_base_hp_sum',0)\n",
    "        out['team_spa_mean_diff'] = out.get('p1_base_spa_mean',0) - out.get('p2_base_spa_mean',0)\n",
    "        out['team_spe_mean_diff'] = out.get('p1_base_spe_mean',0) - out.get('p2_base_spe_mean',0)\n",
    "        out['n_unique_types_team_diff'] = out.get('p1_n_unique_types',0) - out.get('p2_n_unique_types',0)\n",
    "    return out\n",
    "\n",
    "def create_features_from_raw(data: list) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    for b in tqdm(data, desc='FE'):\n",
    "        try:\n",
    "            feat = prepare_record_features(b, max_turns=30)\n",
    "            if 'battle_id' not in feat:\n",
    "                feat['battle_id'] = b.get('battle_id')\n",
    "            rows.append(feat)\n",
    "        except Exception as e:\n",
    "            rows.append({'battle_id': b.get('battle_id'), 'error': 1})\n",
    "    df = pd.DataFrame(rows)\n",
    "    if 'player_won' in df.columns:\n",
    "        df['player_won'] = df['player_won'].astype(int)\n",
    "    return df.fillna(0)\n",
    "\n",
    "train_df = create_features_from_raw(train_raw)\n",
    "test_df = create_features_from_raw(test_raw)\n",
    "print('Feature shape train/test:', train_df.shape, test_df.shape)\n",
    "display(train_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6903c13a",
   "metadata": {},
   "source": [
    "# Search best features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "626ecdb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TreeExplainer non funzionante o formato non previsto, fallback a shap.Explainer: could not convert string to float: '[5.06E-1]'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PermutationExplainer explainer: 2001it [04:40,  7.11it/s]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shap_arr.shape: (2000, 229) X_sample.shape: (2000, 229) X_all.shape: (10000, 229)\n",
      "Top-100 features SHAP salvate in top100_shap_features.csv\n",
      "                           feature  shap_mean_abs\n",
      "0          tl_weighted_damage_diff       0.152290\n",
      "1                      tl_p2_moves       0.054426\n",
      "2            tl_inflicted_par_diff       0.050478\n",
      "3        tl_p2_inflicted_slp_count       0.037319\n",
      "4                      tl_p1_moves       0.027659\n",
      "5        tl_p1_inflicted_slp_count       0.024133\n",
      "6                    tl_p1_last_hp       0.023037\n",
      "7                      damage_diff       0.018435\n",
      "8  tl_move_type_psychic_count_diff       0.016813\n",
      "9             tl_last5_damage_diff       0.016260\n"
     ]
    }
   ],
   "source": [
    "# Cell: estrai top-100 features con SHAP (robusta)\n",
    "import shap\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Candidate features: tutte le colonne numeriche tranne id/target\n",
    "candidate_features = [c for c in train_df.columns if c not in ('battle_id','player_won') and train_df[c].dtype != 'object']\n",
    "X_all = train_df[candidate_features].astype(float).fillna(0.0)\n",
    "y_all = train_df['player_won'].astype(int).values\n",
    "\n",
    "# Per velocità, campiona fino a N per training + spiegazioni\n",
    "RND = np.random.RandomState(42)\n",
    "N_SAMPLE = min(2000, len(X_all))\n",
    "sample_idx = RND.choice(len(X_all), size=N_SAMPLE, replace=False)\n",
    "X_sample = X_all.iloc[sample_idx]\n",
    "y_sample = y_all[sample_idx]\n",
    "\n",
    "# Allena un modello leggero (veloce) usato solo per SHAP\n",
    "shap_clf = XGBClassifier(n_estimators=300, max_depth=3, learning_rate=0.05,\n",
    "                         use_label_encoder=False, eval_metric='logloss',\n",
    "                         random_state=42, n_jobs=4, tree_method='hist')\n",
    "shap_clf.fit(X_sample, y_sample)\n",
    "\n",
    "# Calcola SHAP con fallback robusto\n",
    "shap_arr = None\n",
    "explainer = None\n",
    "ev = None\n",
    "\n",
    "def _to_shap_array(shap_vals, n_samples, n_features):\n",
    "    arr = np.array(shap_vals)\n",
    "    if arr.ndim == 3:\n",
    "        # Possibili layout: (n_samples, n_classes, n_features) o (n_classes, n_samples, n_features)\n",
    "        if arr.shape[0] == n_samples and arr.shape[2] == n_features:\n",
    "            class_idx = 1 if arr.shape[1] > 1 else 0\n",
    "            return arr[:, class_idx, :]\n",
    "        if arr.shape[1] == n_samples and arr.shape[2] == n_features:\n",
    "            class_idx = 1 if arr.shape[0] > 1 else 0\n",
    "            return arr[class_idx, :, :]\n",
    "        if arr.shape[0] == n_samples and arr.shape[1] == n_features:\n",
    "            class_idx = 1 if arr.shape[2] > 1 else 0\n",
    "            return arr[:, :, class_idx]\n",
    "        raise RuntimeError(f\"Formato 3D SHAP non riconosciuto: {arr.shape}\")\n",
    "    elif arr.ndim == 2:\n",
    "        # Atteso (n_samples, n_features) oppure trasposto\n",
    "        if arr.shape[0] == n_samples and arr.shape[1] == n_features:\n",
    "            return arr\n",
    "        if arr.shape[1] == n_samples and arr.shape[0] == n_features:\n",
    "            return arr.T\n",
    "        raise RuntimeError(f\"Formato 2D SHAP non riconosciuto: {arr.shape}\")\n",
    "    else:\n",
    "        raise RuntimeError(f\"Formato SHAP inatteso: {arr.shape}\")\n",
    "\n",
    "n_samples = X_sample.shape[0]\n",
    "n_features = X_sample.shape[1]\n",
    "\n",
    "try:\n",
    "    explainer = shap.TreeExplainer(shap_clf)\n",
    "    shap_vals = explainer.shap_values(X_sample)\n",
    "    shap_arr = _to_shap_array(shap_vals, n_samples, n_features)\n",
    "except Exception as e:\n",
    "    print(\"TreeExplainer non funzionante o formato non previsto, fallback a shap.Explainer:\", e)\n",
    "    explainer = shap.Explainer(shap_clf.predict_proba, X_sample)\n",
    "    ev = explainer(X_sample)\n",
    "    vals = ev.values\n",
    "    shap_arr = _to_shap_array(vals, n_samples, n_features)\n",
    "\n",
    "# Debug shapes se serve\n",
    "print(\"shap_arr.shape:\", getattr(shap_arr, \"shape\", None), \"X_sample.shape:\", getattr(X_sample, \"shape\", None), \"X_all.shape:\", getattr(X_all, \"shape\", None))\n",
    "\n",
    "# Allinea nomi feature e calcola importanza\n",
    "shap_imp = np.abs(shap_arr).mean(axis=0)\n",
    "\n",
    "# Recupera nomi feature nell'ordine usato per SHAP\n",
    "feat_names = None\n",
    "try:\n",
    "    if explainer is not None and getattr(explainer, 'feature_names', None) is not None:\n",
    "        feat_names = list(explainer.feature_names)\n",
    "except Exception:\n",
    "    feat_names = None\n",
    "\n",
    "if feat_names is None and ev is not None:\n",
    "    try:\n",
    "        if getattr(ev, 'feature_names', None) is not None:\n",
    "            feat_names = list(ev.feature_names)\n",
    "    except Exception:\n",
    "        feat_names = None\n",
    "\n",
    "if feat_names is None:\n",
    "    try:\n",
    "        feat_names = list(X_sample.columns)\n",
    "    except Exception:\n",
    "        feat_names = None\n",
    "\n",
    "if feat_names is None:\n",
    "    feat_names = list(X_all.columns)\n",
    "\n",
    "if len(shap_imp) != len(feat_names):\n",
    "    raise ValueError(f\"Incoerenza lunghezze: len(shap_imp)={len(shap_imp)}, len(feat_names)={len(feat_names)}. Controlla X_sample/colonne.\")\n",
    "\n",
    "imp_df = pd.DataFrame({'feature': feat_names, 'shap_mean_abs': shap_imp})\n",
    "imp_df = imp_df.sort_values('shap_mean_abs', ascending=False).reset_index(drop=True)\n",
    "\n",
    "TOP_K = min(100, len(imp_df))\n",
    "top100 = imp_df['feature'].iloc[:TOP_K].tolist()\n",
    "\n",
    "imp_df.to_csv('shap_feature_importances_all.csv', index=False)\n",
    "pd.DataFrame({'feature': top100}).to_csv('top100_shap_features.csv', index=False)\n",
    "\n",
    "print(f\"Top-{TOP_K} features SHAP salvate in top100_shap_features.csv\")\n",
    "print(imp_df.head(10))\n",
    "\n",
    "SHAP_TOP100 = top100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf62000",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2cede4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num FEATURES numeriche rilevate (ALL): 239\n",
      "Num FEATURES effettive usate (FEATURES): 239\n",
      "Num TOP100 caricate: 100\n",
      "Preprocessing (no transformers) completato.\n",
      "train_val size: 8000 holdout size: 2000\n",
      "Preprocessed feature count: 239\n"
     ]
    }
   ],
   "source": [
    "# ====== Preprocessing (senza transformer sklearn) =========\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# base exclusions\n",
    "exclude_cols = ['battle_id', 'player_won']\n",
    "string_cols = train_df.select_dtypes(include=['object']).columns.tolist()\n",
    "exclude_cols.extend(string_cols)\n",
    "\n",
    "# tutte le colonne numeriche candidate\n",
    "ALL_NUMERIC_FEATURES = [c for c in train_df.columns if c not in exclude_cols]\n",
    "\n",
    "# flag per usare top features se necessario\n",
    "use_top_features = False\n",
    "\n",
    "# carica TOP100 se presente (comportamento invariato)\n",
    "top100_path = r'top100_shap_features.csv'\n",
    "try:\n",
    "    top100_df = pd.read_csv(top100_path)\n",
    "    TOP100 = [str(x).strip() for x in top100_df['feature'].tolist()]\n",
    "except Exception:\n",
    "    TOP100 = []\n",
    "\n",
    "# --- INIZIO: filtro dalle keep_features_list se richiesto ---\n",
    "features_filter = True  # imposta True per applicare il filtro, False per comportamento attuale\n",
    "keep_list_path = 'keep_features_list.txt'\n",
    "\n",
    "if features_filter:\n",
    "    try:\n",
    "        import os\n",
    "        if os.path.exists(keep_list_path):\n",
    "            keep_df = pd.read_csv(keep_list_path, header=None)\n",
    "            keep_list = [str(x).strip() for x in keep_df.iloc[:, 0].tolist()]\n",
    "            # mantieni solo feature numeriche valide presenti in ALL_NUMERIC_FEATURES\n",
    "            filtered = [f for f in ALL_NUMERIC_FEATURES if f in keep_list]\n",
    "            if filtered:\n",
    "                # sovrascrive FEATURES più avanti: qui memorizziamo in temp\n",
    "                FEATURES_FROM_KEEP = filtered\n",
    "                print(f\"features_filter=ON: trovato {len(filtered)} feature valide in {keep_list_path}\")\n",
    "            else:\n",
    "                FEATURES_FROM_KEEP = None\n",
    "                print(f\"features_filter=ON: nessuna feature di {keep_list_path} presente in ALL_NUMERIC_FEATURES\")\n",
    "        else:\n",
    "            FEATURES_FROM_KEEP = None\n",
    "            print(f\"features_filter=ON ma file {keep_list_path} non trovato. Nessun filtro applicato.\")\n",
    "    except Exception as e:\n",
    "        FEATURES_FROM_KEEP = None\n",
    "        print(\"Errore caricando keep_features_list.txt, nessun filtro applicato:\", e)\n",
    "else:\n",
    "    FEATURES_FROM_KEEP = None\n",
    "# --- FINE: filtro dalle keep_features_list ---\n",
    "\n",
    "if use_top_features and TOP100:\n",
    "    FEATURES = [f for f in TOP100 if f in ALL_NUMERIC_FEATURES]\n",
    "elif features_filter:\n",
    "    FEATURES = FEATURES_FROM_KEEP\n",
    "else:\n",
    "    FEATURES = ALL_NUMERIC_FEATURES\n",
    "\n",
    "print(f'Num FEATURES numeriche rilevate (ALL): {len(ALL_NUMERIC_FEATURES)}')\n",
    "print(f'Num FEATURES effettive usate (FEATURES): {len(FEATURES)}')\n",
    "print(f'Num TOP100 caricate: {len(TOP100)}')\n",
    "\n",
    "# costruisco DataFrame numerico raw\n",
    "num_df = train_df[FEATURES].astype(float).replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "# Imputazione semplice: usiamo la mediana per ogni feature calcolata sul train\n",
    "medians = num_df.median()\n",
    "train_imputed = num_df.fillna(medians)\n",
    "\n",
    "# NON eseguo alcuno scaling: lascio i valori nella loro scala naturale\n",
    "train_preproc_df = train_imputed.copy()\n",
    "\n",
    "# target\n",
    "y = train_df['player_won'].astype(int).values\n",
    "\n",
    "# split holdout (20%) - mantengo comportamento originale\n",
    "X = train_preproc_df.values\n",
    "X_train_val, X_holdout, y_train_val, y_holdout, idx_train_val, idx_holdout = train_test_split(\n",
    "    X, y, train_df.index.values, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "print('Preprocessing (no transformers) completato.')\n",
    "print('train_val size:', X_train_val.shape[0], 'holdout size:', X_holdout.shape[0])\n",
    "print('Preprocessed feature count:', len(FEATURES))\n",
    "\n",
    "# Allinea e imputa test_df usando le mediane del train (coerente con l'imputazione sopra)\n",
    "test_aligned = test_df.reindex(columns=FEATURES, fill_value=np.nan).astype(float).replace([np.inf, -np.inf], np.nan)\n",
    "test_imputed = test_aligned.fillna(medians)\n",
    "test_preproc_df = pd.DataFrame(test_imputed.values, columns=FEATURES, index=test_df.index)\n",
    "\n",
    "# Variabili pronte per le celle successive:\n",
    "# FEATURES, X, y, X_train_val, X_holdout, y_train_val, y_holdout, test_preproc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "916a0a8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analisi feature: 239 features\n",
      "Feature selection summary salvata in feature_selection_candidates.csv\n",
      "Esempio TOP 30 (to_drop prima):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>missing_pct</th>\n",
       "      <th>unique</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "      <th>mi</th>\n",
       "      <th>max_abs_corr</th>\n",
       "      <th>psi</th>\n",
       "      <th>drop_missing</th>\n",
       "      <th>drop_constant</th>\n",
       "      <th>drop_low_std</th>\n",
       "      <th>drop_low_mi</th>\n",
       "      <th>unstable_psi</th>\n",
       "      <th>high_corr_flag</th>\n",
       "      <th>drop_high_corr</th>\n",
       "      <th>to_drop</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>damage_diff</th>\n",
       "      <td>0.0</td>\n",
       "      <td>5886</td>\n",
       "      <td>1.469337</td>\n",
       "      <td>-5.345115</td>\n",
       "      <td>4.157353</td>\n",
       "      <td>0.185328</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.006958</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tl_damage_rate_diff</th>\n",
       "      <td>0.0</td>\n",
       "      <td>6209</td>\n",
       "      <td>0.048978</td>\n",
       "      <td>-0.178170</td>\n",
       "      <td>0.138578</td>\n",
       "      <td>0.182122</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.006958</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tl_p2_est_damage</th>\n",
       "      <td>0.0</td>\n",
       "      <td>4536</td>\n",
       "      <td>1.159621</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.800000</td>\n",
       "      <td>0.112424</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.060739</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tl_p1_moves</th>\n",
       "      <td>0.0</td>\n",
       "      <td>26</td>\n",
       "      <td>3.073948</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>0.082345</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.003664</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tl_p2_moves</th>\n",
       "      <td>0.0</td>\n",
       "      <td>27</td>\n",
       "      <td>3.064175</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>0.065962</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.002273</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tl_p1_est_damage</th>\n",
       "      <td>0.0</td>\n",
       "      <td>4343</td>\n",
       "      <td>0.879283</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.817803</td>\n",
       "      <td>0.062344</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.038250</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tl_inflicted_slp_rate_diff</th>\n",
       "      <td>0.0</td>\n",
       "      <td>14</td>\n",
       "      <td>0.044272</td>\n",
       "      <td>-0.200000</td>\n",
       "      <td>0.233333</td>\n",
       "      <td>0.061028</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.493303</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tl_inflicted_frz_rate_diff</th>\n",
       "      <td>0.0</td>\n",
       "      <td>6</td>\n",
       "      <td>0.018307</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.043304</td>\n",
       "      <td>1.0</td>\n",
       "      <td>22.683193</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tl_p1_inflicted_slp_count</th>\n",
       "      <td>0.0</td>\n",
       "      <td>8</td>\n",
       "      <td>0.874723</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>0.042108</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.593849</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tl_inflicted_par_diff</th>\n",
       "      <td>0.0</td>\n",
       "      <td>13</td>\n",
       "      <td>1.453273</td>\n",
       "      <td>-6.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.038329</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.532303</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tl_p2_inflicted_slp_count</th>\n",
       "      <td>0.0</td>\n",
       "      <td>7</td>\n",
       "      <td>0.871812</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.028427</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.001029</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tl_p1_inflicted_frz_count</th>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.373515</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.027006</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000450</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tl_p1_inflicted_par_count</th>\n",
       "      <td>0.0</td>\n",
       "      <td>7</td>\n",
       "      <td>1.022386</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.025493</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.002054</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tl_p2_inflicted_frz_rate</th>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.012360</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.024466</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.573013</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p1_base_spa_std</th>\n",
       "      <td>0.0</td>\n",
       "      <td>520</td>\n",
       "      <td>3.612545</td>\n",
       "      <td>8.498366</td>\n",
       "      <td>35.939764</td>\n",
       "      <td>0.013583</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.196123</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tl_p2_special_moves</th>\n",
       "      <td>0.0</td>\n",
       "      <td>27</td>\n",
       "      <td>4.841304</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>0.010941</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.057407</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tl_p1_special_moves</th>\n",
       "      <td>0.0</td>\n",
       "      <td>29</td>\n",
       "      <td>4.735952</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>0.010300</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.088879</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tl_p2_inflicted_par_count</th>\n",
       "      <td>0.0</td>\n",
       "      <td>7</td>\n",
       "      <td>1.028143</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.007975</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.001687</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p1_n_unique_names</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.007895</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p1_ability_flash_fire_count</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007304</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tl_p1_critical_hits</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006865</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tl_kos_late_p1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006863</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p1_base_spe_sum</th>\n",
       "      <td>0.0</td>\n",
       "      <td>73</td>\n",
       "      <td>48.700342</td>\n",
       "      <td>280.000000</td>\n",
       "      <td>665.000000</td>\n",
       "      <td>0.006836</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.227207</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p1_base_spa_min</th>\n",
       "      <td>0.0</td>\n",
       "      <td>9</td>\n",
       "      <td>9.062260</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>105.000000</td>\n",
       "      <td>0.006779</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.034719</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p2_lead_base_spd</th>\n",
       "      <td>0.0</td>\n",
       "      <td>13</td>\n",
       "      <td>19.301077</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>135.000000</td>\n",
       "      <td>0.006478</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.003301</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tl_p2_inflicted_psn_count</th>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.102567</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.006325</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.147568</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tl_p2_move_type_dragon_count</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006309</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p1_base_atk_sum</th>\n",
       "      <td>0.0</td>\n",
       "      <td>119</td>\n",
       "      <td>42.711644</td>\n",
       "      <td>335.000000</td>\n",
       "      <td>664.000000</td>\n",
       "      <td>0.006150</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.041365</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tl_p2_priority_moves</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tl_first_ko_turn_p1_taken</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005580</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              missing_pct  unique        std         min  \\\n",
       "feature                                                                    \n",
       "damage_diff                           0.0    5886   1.469337   -5.345115   \n",
       "tl_damage_rate_diff                   0.0    6209   0.048978   -0.178170   \n",
       "tl_p2_est_damage                      0.0    4536   1.159621    0.000000   \n",
       "tl_p1_moves                           0.0      26   3.073948    0.000000   \n",
       "tl_p2_moves                           0.0      27   3.064175    0.000000   \n",
       "tl_p1_est_damage                      0.0    4343   0.879283    0.000000   \n",
       "tl_inflicted_slp_rate_diff            0.0      14   0.044272   -0.200000   \n",
       "tl_inflicted_frz_rate_diff            0.0       6   0.018307   -0.100000   \n",
       "tl_p1_inflicted_slp_count             0.0       8   0.874723    0.000000   \n",
       "tl_inflicted_par_diff                 0.0      13   1.453273   -6.000000   \n",
       "tl_p2_inflicted_slp_count             0.0       7   0.871812    0.000000   \n",
       "tl_p1_inflicted_frz_count             0.0       3   0.373515    0.000000   \n",
       "tl_p1_inflicted_par_count             0.0       7   1.022386    0.000000   \n",
       "tl_p2_inflicted_frz_rate              0.0       4   0.012360    0.000000   \n",
       "p1_base_spa_std                       0.0     520   3.612545    8.498366   \n",
       "tl_p2_special_moves                   0.0      27   4.841304    0.000000   \n",
       "tl_p1_special_moves                   0.0      29   4.735952    0.000000   \n",
       "tl_p2_inflicted_par_count             0.0       7   1.028143    0.000000   \n",
       "p1_n_unique_names                     0.0       1   0.000000    6.000000   \n",
       "p1_ability_flash_fire_count           0.0       1   0.000000    0.000000   \n",
       "tl_p1_critical_hits                   0.0       1   0.000000    0.000000   \n",
       "tl_kos_late_p1                        0.0       1   0.000000    0.000000   \n",
       "p1_base_spe_sum                       0.0      73  48.700342  280.000000   \n",
       "p1_base_spa_min                       0.0       9   9.062260   45.000000   \n",
       "p2_lead_base_spd                      0.0      13  19.301077   45.000000   \n",
       "tl_p2_inflicted_psn_count             0.0       3   0.102567    0.000000   \n",
       "tl_p2_move_type_dragon_count          0.0       1   0.000000    0.000000   \n",
       "p1_base_atk_sum                       0.0     119  42.711644  335.000000   \n",
       "tl_p2_priority_moves                  0.0       1   0.000000    0.000000   \n",
       "tl_first_ko_turn_p1_taken             0.0       1   0.000000    0.000000   \n",
       "\n",
       "                                     max        mi  max_abs_corr        psi  \\\n",
       "feature                                                                       \n",
       "damage_diff                     4.157353  0.185328           1.0   0.006958   \n",
       "tl_damage_rate_diff             0.138578  0.182122           1.0   0.006958   \n",
       "tl_p2_est_damage                6.800000  0.112424           1.0   0.060739   \n",
       "tl_p1_moves                    30.000000  0.082345           1.0   0.003664   \n",
       "tl_p2_moves                    30.000000  0.065962           1.0   0.002273   \n",
       "tl_p1_est_damage                5.817803  0.062344           1.0   0.038250   \n",
       "tl_inflicted_slp_rate_diff      0.233333  0.061028           1.0   0.493303   \n",
       "tl_inflicted_frz_rate_diff      0.066667  0.043304           1.0  22.683193   \n",
       "tl_p1_inflicted_slp_count       7.000000  0.042108           1.0   3.593849   \n",
       "tl_inflicted_par_diff           6.000000  0.038329           1.0   0.532303   \n",
       "tl_p2_inflicted_slp_count       6.000000  0.028427           1.0   0.001029   \n",
       "tl_p1_inflicted_frz_count       2.000000  0.027006           1.0   0.000450   \n",
       "tl_p1_inflicted_par_count       6.000000  0.025493           1.0   0.002054   \n",
       "tl_p2_inflicted_frz_rate        0.100000  0.024466           1.0   3.573013   \n",
       "p1_base_spa_std                35.939764  0.013583           1.0   0.196123   \n",
       "tl_p2_special_moves            26.000000  0.010941           1.0   0.057407   \n",
       "tl_p1_special_moves            29.000000  0.010300           1.0   0.088879   \n",
       "tl_p2_inflicted_par_count       6.000000  0.007975           1.0   0.001687   \n",
       "p1_n_unique_names               6.000000  0.007895           0.0   0.000000   \n",
       "p1_ability_flash_fire_count     0.000000  0.007304           0.0   0.000000   \n",
       "tl_p1_critical_hits             0.000000  0.006865           0.0   0.000000   \n",
       "tl_kos_late_p1                  0.000000  0.006863           0.0   0.000000   \n",
       "p1_base_spe_sum               665.000000  0.006836           1.0   0.227207   \n",
       "p1_base_spa_min               105.000000  0.006779           1.0   0.034719   \n",
       "p2_lead_base_spd              135.000000  0.006478           1.0   0.003301   \n",
       "tl_p2_inflicted_psn_count       2.000000  0.006325           1.0   0.147568   \n",
       "tl_p2_move_type_dragon_count    0.000000  0.006309           0.0   0.000000   \n",
       "p1_base_atk_sum               664.000000  0.006150           1.0   0.041365   \n",
       "tl_p2_priority_moves            0.000000  0.005905           0.0   0.000000   \n",
       "tl_first_ko_turn_p1_taken       0.000000  0.005580           0.0   0.000000   \n",
       "\n",
       "                              drop_missing  drop_constant  drop_low_std  \\\n",
       "feature                                                                   \n",
       "damage_diff                          False          False         False   \n",
       "tl_damage_rate_diff                  False          False         False   \n",
       "tl_p2_est_damage                     False          False         False   \n",
       "tl_p1_moves                          False          False         False   \n",
       "tl_p2_moves                          False          False         False   \n",
       "tl_p1_est_damage                     False          False         False   \n",
       "tl_inflicted_slp_rate_diff           False          False         False   \n",
       "tl_inflicted_frz_rate_diff           False          False         False   \n",
       "tl_p1_inflicted_slp_count            False          False         False   \n",
       "tl_inflicted_par_diff                False          False         False   \n",
       "tl_p2_inflicted_slp_count            False          False         False   \n",
       "tl_p1_inflicted_frz_count            False          False         False   \n",
       "tl_p1_inflicted_par_count            False          False         False   \n",
       "tl_p2_inflicted_frz_rate             False          False         False   \n",
       "p1_base_spa_std                      False          False         False   \n",
       "tl_p2_special_moves                  False          False         False   \n",
       "tl_p1_special_moves                  False          False         False   \n",
       "tl_p2_inflicted_par_count            False          False         False   \n",
       "p1_n_unique_names                    False           True          True   \n",
       "p1_ability_flash_fire_count          False           True          True   \n",
       "tl_p1_critical_hits                  False           True          True   \n",
       "tl_kos_late_p1                       False           True          True   \n",
       "p1_base_spe_sum                      False          False         False   \n",
       "p1_base_spa_min                      False          False         False   \n",
       "p2_lead_base_spd                     False          False         False   \n",
       "tl_p2_inflicted_psn_count            False          False         False   \n",
       "tl_p2_move_type_dragon_count         False           True          True   \n",
       "p1_base_atk_sum                      False          False         False   \n",
       "tl_p2_priority_moves                 False           True          True   \n",
       "tl_first_ko_turn_p1_taken            False           True          True   \n",
       "\n",
       "                              drop_low_mi  unstable_psi  high_corr_flag  \\\n",
       "feature                                                                   \n",
       "damage_diff                         False         False            True   \n",
       "tl_damage_rate_diff                 False         False            True   \n",
       "tl_p2_est_damage                    False         False            True   \n",
       "tl_p1_moves                         False         False            True   \n",
       "tl_p2_moves                         False         False            True   \n",
       "tl_p1_est_damage                    False         False            True   \n",
       "tl_inflicted_slp_rate_diff          False         False            True   \n",
       "tl_inflicted_frz_rate_diff          False          True            True   \n",
       "tl_p1_inflicted_slp_count           False          True            True   \n",
       "tl_inflicted_par_diff               False          True            True   \n",
       "tl_p2_inflicted_slp_count           False         False            True   \n",
       "tl_p1_inflicted_frz_count           False         False            True   \n",
       "tl_p1_inflicted_par_count           False         False            True   \n",
       "tl_p2_inflicted_frz_rate            False          True            True   \n",
       "p1_base_spa_std                     False         False            True   \n",
       "tl_p2_special_moves                 False         False            True   \n",
       "tl_p1_special_moves                 False         False            True   \n",
       "tl_p2_inflicted_par_count           False         False            True   \n",
       "p1_n_unique_names                   False         False           False   \n",
       "p1_ability_flash_fire_count         False         False           False   \n",
       "tl_p1_critical_hits                 False         False           False   \n",
       "tl_kos_late_p1                      False         False           False   \n",
       "p1_base_spe_sum                     False         False            True   \n",
       "p1_base_spa_min                     False         False            True   \n",
       "p2_lead_base_spd                    False         False            True   \n",
       "tl_p2_inflicted_psn_count           False         False            True   \n",
       "tl_p2_move_type_dragon_count        False         False           False   \n",
       "p1_base_atk_sum                     False         False            True   \n",
       "tl_p2_priority_moves                False         False           False   \n",
       "tl_first_ko_turn_p1_taken           False         False           False   \n",
       "\n",
       "                             drop_high_corr  to_drop  \n",
       "feature                                               \n",
       "damage_diff                            True     True  \n",
       "tl_damage_rate_diff                    True     True  \n",
       "tl_p2_est_damage                       True     True  \n",
       "tl_p1_moves                            True     True  \n",
       "tl_p2_moves                            True     True  \n",
       "tl_p1_est_damage                       True     True  \n",
       "tl_inflicted_slp_rate_diff             True     True  \n",
       "tl_inflicted_frz_rate_diff             True     True  \n",
       "tl_p1_inflicted_slp_count              True     True  \n",
       "tl_inflicted_par_diff                  True     True  \n",
       "tl_p2_inflicted_slp_count              True     True  \n",
       "tl_p1_inflicted_frz_count              True     True  \n",
       "tl_p1_inflicted_par_count              True     True  \n",
       "tl_p2_inflicted_frz_rate               True     True  \n",
       "p1_base_spa_std                        True     True  \n",
       "tl_p2_special_moves                    True     True  \n",
       "tl_p1_special_moves                    True     True  \n",
       "tl_p2_inflicted_par_count              True     True  \n",
       "p1_n_unique_names                       NaN     True  \n",
       "p1_ability_flash_fire_count             NaN     True  \n",
       "tl_p1_critical_hits                     NaN     True  \n",
       "tl_kos_late_p1                          NaN     True  \n",
       "p1_base_spe_sum                        True     True  \n",
       "p1_base_spa_min                        True     True  \n",
       "p2_lead_base_spd                       True     True  \n",
       "tl_p2_inflicted_psn_count              True     True  \n",
       "tl_p2_move_type_dragon_count            NaN     True  \n",
       "p1_base_atk_sum                        True     True  \n",
       "tl_p2_priority_moves                    NaN     True  \n",
       "tl_first_ko_turn_p1_taken               NaN     True  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEEP: 106  DROP: 133\n",
      "Auto-salvato 'keep_features_list.txt' con 106 feature (una per riga).\n"
     ]
    }
   ],
   "source": [
    "# --- NEW CELL: feature selection candidates (statistics, MI, corr, PSI) ---\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "\n",
    "def feature_basic_stats(df, features):\n",
    "    rows = []\n",
    "    for f in features:\n",
    "        col = df[f]\n",
    "        rows.append({\n",
    "            'feature': f,\n",
    "            'missing_pct': float(col.isna().mean()),\n",
    "            'unique': int(col.nunique(dropna=True)),\n",
    "            'std': float(col.std()),\n",
    "            'min': float(col.min()) if np.isfinite(col.min()) else np.nan,\n",
    "            'max': float(col.max()) if np.isfinite(col.max()) else np.nan\n",
    "        })\n",
    "    return pd.DataFrame(rows).set_index('feature')\n",
    "\n",
    "def psi(expected, actual, buckets=10):\n",
    "    # simple PSI by quantile buckets\n",
    "    def _hist(arr):\n",
    "        try:\n",
    "            labels = pd.qcut(arr.rank(method='first'), buckets, labels=False)\n",
    "        except Exception:\n",
    "            labels = pd.cut(arr, buckets, labels=False, duplicates='drop')\n",
    "            labels = np.nan_to_num(labels, nan=0).astype(int)\n",
    "        cnt = np.bincount(labels, minlength=buckets).astype(float)\n",
    "        return cnt / max(1.0, cnt.sum())\n",
    "    e = _hist(expected.fillna(0).values)\n",
    "    a = _hist(actual.fillna(0).values)\n",
    "    eps = 1e-6\n",
    "    return float(np.sum((e - a) * np.log((e + eps) / (a + eps))))\n",
    "\n",
    "# run only if FEATURES is defined\n",
    "try:\n",
    "    feats = list(FEATURES)\n",
    "except NameError:\n",
    "    feats = [c for c in train_df.columns if c not in ('battle_id','player_won') and train_df[c].dtype != 'object']\n",
    "\n",
    "print(f\"Analisi feature: {len(feats)} features\")\n",
    "\n",
    "# basic stats\n",
    "stats_df = feature_basic_stats(train_df, feats)\n",
    "\n",
    "# mutual information\n",
    "X = train_df[feats].fillna(0).astype(float)\n",
    "y = train_df['player_won'].astype(int).values\n",
    "mi = mutual_info_classif(X, y, discrete_features=False, random_state=42)\n",
    "mi_ser = pd.Series(mi, index=feats, name='mi')\n",
    "\n",
    "# max pairwise correlation (train)\n",
    "corr = X.corr().abs()\n",
    "max_corr = corr.where(~np.eye(len(corr),dtype=bool)).max(axis=0).fillna(0)\n",
    "max_corr.name = 'max_abs_corr'\n",
    "\n",
    "# PSI vs test (if test_df present)\n",
    "psilist = []\n",
    "for f in feats:\n",
    "    if f in test_df.columns:\n",
    "        psilist.append(psi(train_df[f].fillna(0), test_df[f].fillna(0), buckets=10))\n",
    "    else:\n",
    "        psilist.append(np.nan)\n",
    "psi_ser = pd.Series(psilist, index=feats, name='psi')\n",
    "\n",
    "# assemble summary\n",
    "summary = stats_df.join(mi_ser).join(max_corr).join(psi_ser)\n",
    "summary['drop_missing'] = summary['missing_pct'] > 0.5\n",
    "summary['drop_constant'] = summary['unique'] <= 1\n",
    "summary['drop_low_std'] = summary['std'].abs() < 1e-8\n",
    "mi_thr = summary['mi'].quantile(0.05) if summary['mi'].notna().any() else 0.0\n",
    "summary['drop_low_mi'] = summary['mi'] <= mi_thr\n",
    "summary['unstable_psi'] = summary['psi'] > 0.5\n",
    "summary['high_corr_flag'] = summary['max_abs_corr'] > 0.95\n",
    "\n",
    "# decide drops: combine rules but keep decision explainable\n",
    "drop_reasons = []\n",
    "drop_set = set()\n",
    "for f, row in summary.iterrows():\n",
    "    reasons = []\n",
    "    if row['drop_missing']:\n",
    "        reasons.append('missing>0.5')\n",
    "    if row['drop_constant']:\n",
    "        reasons.append('constant')\n",
    "    if row['drop_low_std']:\n",
    "        reasons.append('std~0')\n",
    "    if row['drop_low_mi']:\n",
    "        reasons.append('low_mi')\n",
    "    # high corr: mark but decide to drop the one with lower MI within correlated pairs later\n",
    "    if reasons:\n",
    "        drop_reasons.append((f, ';'.join(reasons)))\n",
    "        drop_set.add(f)\n",
    "\n",
    "# resolve high-corr groups (keep feature with higher MI)\n",
    "if summary['high_corr_flag'].any():\n",
    "    cor_mat = corr\n",
    "    visited = set()\n",
    "    for i, fi in enumerate(feats):\n",
    "        if fi in visited:\n",
    "            continue\n",
    "        # find strongly correlated partners\n",
    "        partners = [fj for fj in feats if (fj != fi and cor_mat.at[fi,fj] > 0.95)]\n",
    "        if partners:\n",
    "            group = [fi] + partners\n",
    "            visited.update(group)\n",
    "            # pick best by MI\n",
    "            group_mi = summary.loc[group, 'mi'].fillna(-1.0)\n",
    "            keep = group_mi.idxmax()\n",
    "            for g in group:\n",
    "                if g != keep:\n",
    "                    drop_set.add(g)\n",
    "                    summary.at[g, 'drop_high_corr'] = True\n",
    "            summary.at[keep, 'drop_high_corr'] = False\n",
    "\n",
    "summary['to_drop'] = summary.index.isin(drop_set)\n",
    "summary = summary.sort_values(['to_drop','mi'], ascending=[False, False])\n",
    "\n",
    "# save results\n",
    "out_path = 'feature_selection_candidates.csv'\n",
    "summary.reset_index().to_csv(out_path, index=False)\n",
    "print(f\"Feature selection summary salvata in {out_path}\")\n",
    "print(\"Esempio TOP 30 (to_drop prima):\")\n",
    "display(summary.head(30))\n",
    "\n",
    "# convenience lists\n",
    "KEEP = [c for c in summary.index.tolist() if not summary.at[c,'to_drop']]\n",
    "DROP = [c for c in summary.index.tolist() if summary.at[c,'to_drop']]\n",
    "print(f\"KEEP: {len(KEEP)}  DROP: {len(DROP)}\")\n",
    "# ... you can later modify thresholds and re-run this cell to update candidates ...\n",
    "\n",
    "# --- INIZIO: scrivi keep_features_list.txt dalle feature KEEP generate qui ---\n",
    "keep_path = 'keep_features_list.txt'\n",
    "seen = set()\n",
    "cleaned_keep = []\n",
    "for f in KEEP:\n",
    "    ff = str(f).strip()\n",
    "    if not ff or ff in seen:\n",
    "        continue\n",
    "    seen.add(ff)\n",
    "    cleaned_keep.append(ff)\n",
    "\n",
    "if cleaned_keep:\n",
    "    with open(keep_path, 'w', encoding='utf-8') as fh:\n",
    "        for feat in cleaned_keep:\n",
    "            fh.write(feat + '\\n')\n",
    "    print(f\"Auto-salvato '{keep_path}' con {len(cleaned_keep)} feature (una per riga).\")\n",
    "else:\n",
    "    # se KEEP è vuoto, rimuovere file esistente per evitare confusione\n",
    "    try:\n",
    "        import os\n",
    "        if os.path.exists(keep_path):\n",
    "            os.remove(keep_path)\n",
    "            print(f\"KEEP vuoto: rimosso eventuale '{keep_path}' esistente.\")\n",
    "    except Exception:\n",
    "        pass\n",
    "# --- FINE: scrivi keep_features_list.txt ---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb05f0e",
   "metadata": {},
   "source": [
    "# Hyperparameter serch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "38364108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Time-boxed GridSearchCV (<= ~2 ore) ===\n",
      "scale_pos_weight auto≈1.00 -> grid=[1.0, 1.25]\n",
      "Warmup per stimare t_fit...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 53\u001b[39m\n\u001b[32m     43\u001b[39m t0 = time.time()\n\u001b[32m     44\u001b[39m gs_warm = GridSearchCV(\n\u001b[32m     45\u001b[39m     base_clf,\n\u001b[32m     46\u001b[39m     param_grid=[{k:[v] \u001b[38;5;28;01mfor\u001b[39;00m k,v \u001b[38;5;129;01min\u001b[39;00m d.items()} \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m warm_params],\n\u001b[32m   (...)\u001b[39m\u001b[32m     51\u001b[39m     refit=\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m     52\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m \u001b[43mgs_warm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_val\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     54\u001b[39m elapsed_warm = time.time() - t0\n\u001b[32m     55\u001b[39m fits_warm = \u001b[38;5;28mlen\u001b[39m(warm_params) * cv_inner.get_n_splits()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1365\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1358\u001b[39m     estimator._validate_params()\n\u001b[32m   1360\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1361\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1362\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1363\u001b[39m     )\n\u001b[32m   1364\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1365\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\model_selection\\_search.py:1051\u001b[39m, in \u001b[36mBaseSearchCV.fit\u001b[39m\u001b[34m(self, X, y, **params)\u001b[39m\n\u001b[32m   1045\u001b[39m     results = \u001b[38;5;28mself\u001b[39m._format_results(\n\u001b[32m   1046\u001b[39m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[32m   1047\u001b[39m     )\n\u001b[32m   1049\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[32m-> \u001b[39m\u001b[32m1051\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1053\u001b[39m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[32m   1054\u001b[39m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[32m   1055\u001b[39m first_test_score = all_out[\u001b[32m0\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mtest_scores\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\model_selection\\_search.py:1605\u001b[39m, in \u001b[36mGridSearchCV._run_search\u001b[39m\u001b[34m(self, evaluate_candidates)\u001b[39m\n\u001b[32m   1603\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[32m   1604\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1605\u001b[39m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\model_selection\\_search.py:997\u001b[39m, in \u001b[36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[39m\u001b[34m(candidate_params, cv, more_results)\u001b[39m\n\u001b[32m    989\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.verbose > \u001b[32m0\u001b[39m:\n\u001b[32m    990\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[32m    991\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[33m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[33m candidates,\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    992\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m fits\u001b[39m\u001b[33m\"\u001b[39m.format(\n\u001b[32m    993\u001b[39m             n_splits, n_candidates, n_candidates * n_splits\n\u001b[32m    994\u001b[39m         )\n\u001b[32m    995\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m997\u001b[39m out = \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    998\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    999\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1000\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1001\u001b[39m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1002\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1003\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1004\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1005\u001b[39m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1006\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1007\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1008\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1009\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1010\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1011\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mrouted_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplitter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1012\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1013\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1015\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) < \u001b[32m1\u001b[39m:\n\u001b[32m   1016\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1017\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mNo fits were performed. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1018\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mWas the CV iterator empty? \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1019\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mWere there no candidates?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1020\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\parallel.py:82\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m     73\u001b[39m warning_filters = warnings.filters\n\u001b[32m     74\u001b[39m iterable_with_config_and_warning_filters = (\n\u001b[32m     75\u001b[39m     (\n\u001b[32m     76\u001b[39m         _with_config_and_warning_filters(delayed_func, config, warning_filters),\n\u001b[32m   (...)\u001b[39m\u001b[32m     80\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[32m     81\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config_and_warning_filters\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\joblib\\parallel.py:2072\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   2066\u001b[39m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[32m   2067\u001b[39m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[32m   2068\u001b[39m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[32m   2069\u001b[39m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[32m   2070\u001b[39m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[32m-> \u001b[39m\u001b[32m2072\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\joblib\\parallel.py:1682\u001b[39m, in \u001b[36mParallel._get_outputs\u001b[39m\u001b[34m(self, iterator, pre_dispatch)\u001b[39m\n\u001b[32m   1679\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[32m   1681\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backend.retrieval_context():\n\u001b[32m-> \u001b[39m\u001b[32m1682\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m._retrieve()\n\u001b[32m   1684\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[32m   1685\u001b[39m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[32m   1686\u001b[39m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[32m   1687\u001b[39m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[32m   1688\u001b[39m     \u001b[38;5;28mself\u001b[39m._exception = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\joblib\\parallel.py:1800\u001b[39m, in \u001b[36mParallel._retrieve\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1789\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_ordered:\n\u001b[32m   1790\u001b[39m     \u001b[38;5;66;03m# Case ordered: wait for completion (or error) of the next job\u001b[39;00m\n\u001b[32m   1791\u001b[39m     \u001b[38;5;66;03m# that have been dispatched and not retrieved yet. If no job\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1795\u001b[39m     \u001b[38;5;66;03m# control only have to be done on the amount of time the next\u001b[39;00m\n\u001b[32m   1796\u001b[39m     \u001b[38;5;66;03m# dispatched job is pending.\u001b[39;00m\n\u001b[32m   1797\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (nb_jobs == \u001b[32m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   1798\u001b[39m         \u001b[38;5;28mself\u001b[39m._jobs[\u001b[32m0\u001b[39m].get_status(timeout=\u001b[38;5;28mself\u001b[39m.timeout) == TASK_PENDING\n\u001b[32m   1799\u001b[39m     ):\n\u001b[32m-> \u001b[39m\u001b[32m1800\u001b[39m         time.sleep(\u001b[32m0.01\u001b[39m)\n\u001b[32m   1801\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m   1803\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m nb_jobs == \u001b[32m0\u001b[39m:\n\u001b[32m   1804\u001b[39m     \u001b[38;5;66;03m# Case unordered: jobs are added to the list of jobs to\u001b[39;00m\n\u001b[32m   1805\u001b[39m     \u001b[38;5;66;03m# retrieve `self._jobs` only once completed or in error, which\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1811\u001b[39m     \u001b[38;5;66;03m# timeouts before any other dispatched job has completed and\u001b[39;00m\n\u001b[32m   1812\u001b[39m     \u001b[38;5;66;03m# been added to `self._jobs` to be retrieved.\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# === Grid Search time-boxed (<= ~2 ore) ===\n",
    "print(\"=== Time-boxed GridSearchCV (<= ~2 ore) ===\")\n",
    "import time, os\n",
    "import numpy as np, pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV, ParameterGrid\n",
    "try:\n",
    "    import joblib\n",
    "    CPU_COUNT = joblib.cpu_count()\n",
    "except Exception:\n",
    "    CPU_COUNT = os.cpu_count() or 4\n",
    "\n",
    "cv_inner = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Base estimator\n",
    "base_clf = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42, n_jobs=1, tree_method='hist')\n",
    "\n",
    "# Calcola uno scale_pos_weight automatico e cerca attorno ad esso\n",
    "pos_rate = float(y_train_val.mean())\n",
    "spw_auto = float((1.0 - pos_rate) / max(pos_rate, 1e-9))\n",
    "spw_grid = sorted({1.0, max(1.0, spw_auto*0.75), max(1.0, spw_auto), max(1.0, spw_auto*1.25)})\n",
    "print(f'scale_pos_weight auto≈{spw_auto:.2f} -> grid={spw_grid}')\n",
    "\n",
    "# Griglia COARSE (regolarizzata) — include scale_pos_weight\n",
    "grid_coarse = {\n",
    "    'n_estimators':      [300, 500, 700],\n",
    "    'max_depth':         [3, 4],\n",
    "    'min_child_weight':  [3, 5, 7],\n",
    "    'learning_rate':     [0.03, 0.05, 0.07],\n",
    "    'subsample':         [0.7, 0.8, 0.9],\n",
    "    'colsample_bytree':  [0.7, 0.8, 0.9],\n",
    "    'gamma':             [0.1, 0.2, 0.3],\n",
    "    'reg_alpha':         [0.05, 0.1, 0.2],\n",
    "    'reg_lambda':        [2.0, 3.0, 4.0],\n",
    "    'scale_pos_weight':  spw_grid\n",
    "}\n",
    "\n",
    "# Stima tempo per-fit (warmup) — CORRETTO param_grid\n",
    "warm_params = [\n",
    "    {'n_estimators': 500, 'max_depth': 3, 'min_child_weight': 5, 'learning_rate': 0.05, 'subsample': 0.8, 'colsample_bytree': 0.8, 'gamma': 0.2, 'reg_alpha': 0.1, 'reg_lambda': 3.0, 'scale_pos_weight': max(1.0, spw_auto)},\n",
    "    {'n_estimators': 700, 'max_depth': 4, 'min_child_weight': 5, 'learning_rate': 0.03, 'subsample': 0.8, 'colsample_bytree': 0.8, 'gamma': 0.2, 'reg_alpha': 0.1, 'reg_lambda': 3.0, 'scale_pos_weight': max(1.0, spw_auto*1.25)}\n",
    "]\n",
    "print(\"Warmup per stimare t_fit...\")\n",
    "t0 = time.time()\n",
    "gs_warm = GridSearchCV(\n",
    "    base_clf,\n",
    "    param_grid=[{k:[v] for k,v in d.items()} for d in warm_params],\n",
    "    scoring='balanced_accuracy',\n",
    "    cv=cv_inner,\n",
    "    n_jobs=-1,\n",
    "    verbose=0,\n",
    "    refit=False\n",
    ")\n",
    "gs_warm.fit(X_train_val, y_train_val)\n",
    "elapsed_warm = time.time() - t0\n",
    "fits_warm = len(warm_params) * cv_inner.get_n_splits()\n",
    "t_fit_per_fold = max(0.01, elapsed_warm / fits_warm)\n",
    "print(f\"Warmup: {elapsed_warm:.2f}s per {fits_warm} fit -> ~{t_fit_per_fold:.3f}s/fit\")\n",
    "\n",
    "# Budget totale ~2 ore con margine sicurezza\n",
    "TARGET_SECONDS = int(2*3600*0.9)\n",
    "speedup = max(1, min(CPU_COUNT, cv_inner.get_n_splits()))\n",
    "max_combos = int((TARGET_SECONDS * speedup) / (t_fit_per_fold * cv_inner.get_n_splits()))\n",
    "max_combos = int(max(48, min(max_combos, 2000)))\n",
    "print(f\"CPU={CPU_COUNT}, speedup~{speedup}, max_combos≈{max_combos}\")\n",
    "\n",
    "# Costruisci tutte le combinazioni e campiona fino a max_combos\n",
    "all_points = list(ParameterGrid(grid_coarse))\n",
    "total = len(all_points)\n",
    "print(f\"Candidate totali nella griglia: {total}\")\n",
    "rng = np.random.default_rng(42)\n",
    "if total > max_combos:\n",
    "    idx = rng.choice(total, size=max_combos, replace=False)\n",
    "    sampled = [all_points[i] for i in idx]\n",
    "else:\n",
    "    sampled = all_points\n",
    "print(f\"Config selezionate: {len(sampled)}\")\n",
    "\n",
    "# Converte in lista di 'micro-grid' (1 punto ciascuno) — CORRETTO\n",
    "param_grid_list = [{k:[v] for k,v in pt.items()} for pt in sampled]\n",
    "\n",
    "print(\"Esecuzione GridSearch time-boxed...\")\n",
    "t1 = time.time()\n",
    "gs = GridSearchCV(\n",
    "    estimator=base_clf,\n",
    "    param_grid=param_grid_list,\n",
    "    scoring='balanced_accuracy',\n",
    "    cv=cv_inner,\n",
    "    n_jobs=-1,\n",
    "    verbose=2,\n",
    "    refit=True\n",
    ")\n",
    "gs.fit(X_train_val, y_train_val)\n",
    "elapsed = time.time() - t1\n",
    "\n",
    "results_df = pd.DataFrame(gs.cv_results_).sort_values('rank_test_score')\n",
    "csv_path = 'hp_search_results_timeboxed_grid.csv'\n",
    "results_df.to_csv(csv_path, index=False)\n",
    "best_params = gs.best_params_\n",
    "\n",
    "print(f\"\\n✅ Salvato {csv_path} ({len(results_df)} righe)\")\n",
    "print(\"Migliori iperparametri:\")\n",
    "for k, v in best_params.items():\n",
    "    print(f\"  {k}: {v}\")\n",
    "print(f\"Best CV (balanced_accuracy): {gs.best_score_:.4f}\")\n",
    "print(f\"Tempo GridSearch: {elapsed/60:.1f} min (budget ~{TARGET_SECONDS/60:.0f} min)\")\n",
    "print(\"Ora puoi usare 'best_params' nelle celle successive.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "239a614a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-05 12:43:27,773] A new study created in memory with name: no-name-7b3890b9-1b90-49af-afec-35f1ddf8b3a1\n",
      "[I 2025-11-05 12:43:40,921] Trial 0 finished with value: 0.8899728124999999 and parameters: {'n_estimators': 400, 'learning_rate': 0.06359848890377333, 'max_depth': 5, 'min_child_weight': 32, 'gamma': 0.7800932022121826, 'subsample': 0.6389986300840507, 'colsample_bytree': 0.5203292642588698, 'colsample_bylevel': 0.9464704583099741, 'colsample_bynode': 0.8404460046972835, 'reg_alpha': 2.1242177333881367, 'reg_lambda': 1.0838581269344745, 'max_delta_step': 2, 'scale_pos_weight': 1.4994655844802531}. Best is trial 0 with value: 0.8899728124999999.\n",
      "[I 2025-11-05 12:43:50,655] Trial 1 finished with value: 0.8814624999999999 and parameters: {'n_estimators': 300, 'learning_rate': 0.014244917010070048, 'max_depth': 3, 'min_child_weight': 18, 'gamma': 2.6237821581611893, 'subsample': 0.7079862546605289, 'colsample_bytree': 0.6019301990693147, 'colsample_bylevel': 0.8447411578889518, 'colsample_bynode': 0.6557975442608167, 'reg_alpha': 0.8764339456056545, 'reg_lambda': 4.192159350410974, 'max_delta_step': 1, 'scale_pos_weight': 1.4711055768358081}. Best is trial 0 with value: 0.8899728124999999.\n",
      "[I 2025-11-05 12:44:00,923] Trial 2 finished with value: 0.8904215624999999 and parameters: {'n_estimators': 300, 'learning_rate': 0.027200602007803203, 'max_depth': 4, 'min_child_weight': 7, 'gamma': 3.0377242595071916, 'subsample': 0.6426310309218228, 'colsample_bytree': 0.5227680575448478, 'colsample_bylevel': 0.9795542149013333, 'colsample_bynode': 0.9862528132298237, 'reg_alpha': 2.4251920443493833, 'reg_lambda': 3.292529363110524, 'max_delta_step': 0, 'scale_pos_weight': 1.4105398159072942}. Best is trial 2 with value: 0.8904215624999999.\n",
      "[I 2025-11-05 12:44:15,314] Trial 3 finished with value: 0.8889803125 and parameters: {'n_estimators': 450, 'learning_rate': 0.012680438529434264, 'max_depth': 4, 'min_child_weight': 6, 'gamma': 4.546602010393911, 'subsample': 0.6646949954000042, 'colsample_bytree': 0.7318827995238937, 'colsample_bylevel': 0.7246844304357644, 'colsample_bynode': 0.8080272084711243, 'reg_alpha': 1.640130838029839, 'reg_lambda': 2.0609249413202355, 'max_delta_step': 2, 'scale_pos_weight': 1.4650796940166688}. Best is trial 2 with value: 0.8904215624999999.\n",
      "[I 2025-11-05 12:44:37,752] Trial 4 finished with value: 0.8895881249999998 and parameters: {'n_estimators': 800, 'learning_rate': 0.05704490221326816, 'max_depth': 4, 'min_child_weight': 47, 'gamma': 0.4424625102595975, 'subsample': 0.6489957156047863, 'colsample_bytree': 0.5158295511186883, 'colsample_bylevel': 0.7301321323053057, 'colsample_bynode': 0.7554709158757928, 'reg_alpha': 0.8140470953216877, 'reg_lambda': 25.58586885593424, 'max_delta_step': 1, 'scale_pos_weight': 1.1685607058124285}. Best is trial 2 with value: 0.8904215624999999.\n",
      "[I 2025-11-05 12:44:51,799] Trial 5 pruned. \n",
      "[I 2025-11-05 12:44:54,945] Trial 6 pruned. \n",
      "[I 2025-11-05 12:45:13,120] Trial 7 finished with value: 0.8914428124999999 and parameters: {'n_estimators': 650, 'learning_rate': 0.04394788237371214, 'max_depth': 4, 'min_child_weight': 40, 'gamma': 2.4689779818219537, 'subsample': 0.7306832073454985, 'colsample_bytree': 0.6496393564254923, 'colsample_bylevel': 0.610167650697638, 'colsample_bynode': 0.6431565707973218, 'reg_alpha': 0.09428755706020275, 'reg_lambda': 12.057081573389116, 'max_delta_step': 0, 'scale_pos_weight': 1.3051424146988218}. Best is trial 7 with value: 0.8914428124999999.\n",
      "[I 2025-11-05 12:45:34,481] Trial 8 finished with value: 0.8902340625 and parameters: {'n_estimators': 750, 'learning_rate': 0.016243378873014174, 'max_depth': 4, 'min_child_weight': 39, 'gamma': 1.1439908274581123, 'subsample': 0.6192449774571982, 'colsample_bytree': 0.6014130085198188, 'colsample_bylevel': 0.6644885149016018, 'colsample_bynode': 0.9718790609370292, 'reg_alpha': 2.424361138693251, 'reg_lambda': 11.91609571377583, 'max_delta_step': 2, 'scale_pos_weight': 1.4822032461394687}. Best is trial 7 with value: 0.8914428124999999.\n",
      "[I 2025-11-05 12:45:36,744] Trial 9 pruned. \n",
      "[I 2025-11-05 12:45:53,264] Trial 10 finished with value: 0.8930262499999999 and parameters: {'n_estimators': 650, 'learning_rate': 0.03621460667906725, 'max_depth': 3, 'min_child_weight': 23, 'gamma': 3.575848060983975, 'subsample': 0.7725617683922863, 'colsample_bytree': 0.8359197610808968, 'colsample_bylevel': 0.8399780282203828, 'colsample_bynode': 0.6138778243691246, 'reg_alpha': 0.03693350869481238, 'reg_lambda': 8.012964171977487, 'max_delta_step': 0, 'scale_pos_weight': 1.2774067518529872}. Best is trial 10 with value: 0.8930262499999999.\n",
      "[I 2025-11-05 12:46:10,016] Trial 11 finished with value: 0.8932084375 and parameters: {'n_estimators': 650, 'learning_rate': 0.03609152354601192, 'max_depth': 3, 'min_child_weight': 24, 'gamma': 3.446050026958348, 'subsample': 0.7769391038602059, 'colsample_bytree': 0.8321288871233464, 'colsample_bylevel': 0.8330765754474546, 'colsample_bynode': 0.6029825536106241, 'reg_alpha': 0.05651966540264667, 'reg_lambda': 7.788584756550063, 'max_delta_step': 0, 'scale_pos_weight': 1.2673013879712527}. Best is trial 11 with value: 0.8932084375.\n",
      "[I 2025-11-05 12:46:25,600] Trial 12 finished with value: 0.8939143750000001 and parameters: {'n_estimators': 600, 'learning_rate': 0.02934129671834544, 'max_depth': 3, 'min_child_weight': 23, 'gamma': 3.6545565115170797, 'subsample': 0.8108249501896831, 'colsample_bytree': 0.8227472198269544, 'colsample_bylevel': 0.8435396559579935, 'colsample_bynode': 0.6192937628510786, 'reg_alpha': 0.05683345303117703, 'reg_lambda': 6.907291694737957, 'max_delta_step': 0, 'scale_pos_weight': 1.2006377537941204}. Best is trial 12 with value: 0.8939143750000001.\n",
      "[I 2025-11-05 12:46:41,574] Trial 13 finished with value: 0.8923978125000002 and parameters: {'n_estimators': 600, 'learning_rate': 0.02235503969497528, 'max_depth': 3, 'min_child_weight': 23, 'gamma': 3.5250496913914557, 'subsample': 0.8277669950925424, 'colsample_bytree': 0.8469921395635928, 'colsample_bylevel': 0.914258870688524, 'colsample_bynode': 0.7151637197860257, 'reg_alpha': 0.6517135715299867, 'reg_lambda': 5.656389133852796, 'max_delta_step': 0, 'scale_pos_weight': 1.1782087014063056}. Best is trial 12 with value: 0.8939143750000001.\n",
      "[I 2025-11-05 12:46:59,084] Trial 14 finished with value: 0.8932434375 and parameters: {'n_estimators': 700, 'learning_rate': 0.029118099686782074, 'max_depth': 3, 'min_child_weight': 15, 'gamma': 3.8622632836384843, 'subsample': 0.8449116249203072, 'colsample_bytree': 0.7673115882201984, 'colsample_bylevel': 0.8014976938698741, 'colsample_bynode': 0.7078630961533803, 'reg_alpha': 0.5229022967450228, 'reg_lambda': 8.203022589815285, 'max_delta_step': 0, 'scale_pos_weight': 1.176813109310622}. Best is trial 12 with value: 0.8939143750000001.\n",
      "[I 2025-11-05 12:47:18,307] Trial 15 finished with value: 0.8936534374999999 and parameters: {'n_estimators': 750, 'learning_rate': 0.02333507574011068, 'max_depth': 3, 'min_child_weight': 15, 'gamma': 4.0699685278891575, 'subsample': 0.8498232184761471, 'colsample_bytree': 0.7643219543287534, 'colsample_bylevel': 0.8919611362282347, 'colsample_bynode': 0.7056281465849481, 'reg_alpha': 0.5073140710604351, 'reg_lambda': 2.407062300758826, 'max_delta_step': 1, 'scale_pos_weight': 1.1566158847351409}. Best is trial 12 with value: 0.8939143750000001.\n",
      "[I 2025-11-05 12:47:38,544] Trial 16 finished with value: 0.8936225 and parameters: {'n_estimators': 800, 'learning_rate': 0.01988623663421359, 'max_depth': 3, 'min_child_weight': 15, 'gamma': 4.123161127609903, 'subsample': 0.8109690014397164, 'colsample_bytree': 0.7662526409142928, 'colsample_bylevel': 0.8845189747479743, 'colsample_bynode': 0.686621281279895, 'reg_alpha': 1.273241314744862, 'reg_lambda': 1.9413951114922017, 'max_delta_step': 1, 'scale_pos_weight': 1.1031657780540285}. Best is trial 12 with value: 0.8939143750000001.\n",
      "[I 2025-11-05 12:47:41,987] Trial 17 pruned. \n",
      "[I 2025-11-05 12:47:55,391] Trial 18 finished with value: 0.8937268750000001 and parameters: {'n_estimators': 500, 'learning_rate': 0.03337304053779644, 'max_depth': 3, 'min_child_weight': 16, 'gamma': 3.0202710171572553, 'subsample': 0.8424012159562776, 'colsample_bytree': 0.7915832809719712, 'colsample_bylevel': 0.9957845022128459, 'colsample_bynode': 0.6680494764994823, 'reg_alpha': 1.115041146547746, 'reg_lambda': 1.2396619568959901, 'max_delta_step': 1, 'scale_pos_weight': 1.219163172185842}. Best is trial 12 with value: 0.8939143750000001.\n",
      "[I 2025-11-05 12:47:57,287] Trial 19 pruned. \n",
      "[I 2025-11-05 12:48:09,706] Trial 20 pruned. \n",
      "[I 2025-11-05 12:48:24,050] Trial 21 finished with value: 0.893251875 and parameters: {'n_estimators': 550, 'learning_rate': 0.024804715212188817, 'max_depth': 3, 'min_child_weight': 12, 'gamma': 4.1699413831422065, 'subsample': 0.8415958783593067, 'colsample_bytree': 0.7868210354151003, 'colsample_bylevel': 0.9170401701979944, 'colsample_bynode': 0.6259251289843883, 'reg_alpha': 1.0732813077102579, 'reg_lambda': 1.449561147358561, 'max_delta_step': 1, 'scale_pos_weight': 1.2228151807509302}. Best is trial 12 with value: 0.8939143750000001.\n",
      "[I 2025-11-05 12:48:28,140] Trial 22 pruned. \n",
      "[I 2025-11-05 12:48:31,119] Trial 23 pruned. \n",
      "[I 2025-11-05 12:48:35,456] Trial 24 pruned. \n",
      "[I 2025-11-05 12:48:50,847] Trial 25 finished with value: 0.89326 and parameters: {'n_estimators': 600, 'learning_rate': 0.025081068091490386, 'max_depth': 3, 'min_child_weight': 11, 'gamma': 4.698707456902114, 'subsample': 0.7902524469331204, 'colsample_bytree': 0.7007877447071472, 'colsample_bylevel': 0.7888938145305916, 'colsample_bynode': 0.7290138144344682, 'reg_alpha': 0.7862766615615293, 'reg_lambda': 1.4478329807621437, 'max_delta_step': 1, 'scale_pos_weight': 1.2266320297841389}. Best is trial 12 with value: 0.8939143750000001.\n",
      "[I 2025-11-05 12:49:05,705] Trial 26 finished with value: 0.8934746875 and parameters: {'n_estimators': 600, 'learning_rate': 0.0445077906429425, 'max_depth': 3, 'min_child_weight': 26, 'gamma': 4.116321504212068, 'subsample': 0.8293383859659403, 'colsample_bytree': 0.811982306688609, 'colsample_bylevel': 0.9617474686955357, 'colsample_bynode': 0.6307147776381895, 'reg_alpha': 0.9840788992091264, 'reg_lambda': 3.0002984275445637, 'max_delta_step': 2, 'scale_pos_weight': 1.3545549584670427}. Best is trial 12 with value: 0.8939143750000001.\n",
      "[I 2025-11-05 12:49:09,090] Trial 27 pruned. \n",
      "[I 2025-11-05 12:49:13,176] Trial 28 pruned. \n",
      "[I 2025-11-05 12:49:16,486] Trial 29 pruned. \n",
      "[I 2025-11-05 12:49:20,703] Trial 30 pruned. \n",
      "[I 2025-11-05 12:49:40,453] Trial 31 finished with value: 0.8931103125 and parameters: {'n_estimators': 800, 'learning_rate': 0.020090822667443747, 'max_depth': 3, 'min_child_weight': 15, 'gamma': 4.254601931604697, 'subsample': 0.8121378902114399, 'colsample_bytree': 0.7731988908059118, 'colsample_bylevel': 0.8797503679733679, 'colsample_bynode': 0.6887830600475653, 'reg_alpha': 1.1566149412496358, 'reg_lambda': 2.050760519871266, 'max_delta_step': 1, 'scale_pos_weight': 1.108633144329908}. Best is trial 12 with value: 0.8939143750000001.\n",
      "[I 2025-11-05 12:49:59,953] Trial 32 finished with value: 0.8931737500000001 and parameters: {'n_estimators': 800, 'learning_rate': 0.019588752612929076, 'max_depth': 3, 'min_child_weight': 17, 'gamma': 4.983491470265199, 'subsample': 0.8127591456563427, 'colsample_bytree': 0.7492619338701607, 'colsample_bylevel': 0.821566087733893, 'colsample_bynode': 0.6502268248259241, 'reg_alpha': 1.379068213038868, 'reg_lambda': 1.49969974204443, 'max_delta_step': 1, 'scale_pos_weight': 1.0977306653506882}. Best is trial 12 with value: 0.8939143750000001.\n",
      "[I 2025-11-05 12:50:18,456] Trial 33 finished with value: 0.8940962499999999 and parameters: {'n_estimators': 750, 'learning_rate': 0.02656147962890573, 'max_depth': 3, 'min_child_weight': 13, 'gamma': 4.027785938643997, 'subsample': 0.8315455506959151, 'colsample_bytree': 0.7865304451244489, 'colsample_bylevel': 0.9110704155809861, 'colsample_bynode': 0.7465807634651531, 'reg_alpha': 0.7409238132356848, 'reg_lambda': 2.3035848178251936, 'max_delta_step': 1, 'scale_pos_weight': 1.0142898492369425}. Best is trial 33 with value: 0.8940962499999999.\n",
      "[I 2025-11-05 12:50:36,425] Trial 34 finished with value: 0.89334125 and parameters: {'n_estimators': 700, 'learning_rate': 0.026461411215630132, 'max_depth': 3, 'min_child_weight': 8, 'gamma': 2.8740489170390893, 'subsample': 0.8321365748171026, 'colsample_bytree': 0.849379555954579, 'colsample_bylevel': 0.9029423180845149, 'colsample_bynode': 0.7379272922676032, 'reg_alpha': 0.7111691681899186, 'reg_lambda': 2.7062260072771993, 'max_delta_step': 1, 'scale_pos_weight': 1.0168655634953694}. Best is trial 33 with value: 0.8940962499999999.\n",
      "[I 2025-11-05 12:50:47,234] Trial 35 pruned. \n",
      "[I 2025-11-05 12:50:49,997] Trial 36 pruned. \n",
      "[I 2025-11-05 12:50:52,730] Trial 37 pruned. \n",
      "[I 2025-11-05 12:50:57,430] Trial 38 pruned. \n",
      "[I 2025-11-05 12:51:01,247] Trial 39 pruned. \n",
      "[I 2025-11-05 12:51:13,634] Trial 40 finished with value: 0.8938271875 and parameters: {'n_estimators': 450, 'learning_rate': 0.039256748833062516, 'max_depth': 3, 'min_child_weight': 13, 'gamma': 0.5054283881645358, 'subsample': 0.7117485791037156, 'colsample_bytree': 0.7514238269399777, 'colsample_bylevel': 0.967546328079302, 'colsample_bynode': 0.8226657726491857, 'reg_alpha': 0.4222909162320094, 'reg_lambda': 2.4348017937720168, 'max_delta_step': 0, 'scale_pos_weight': 1.195275576732358}. Best is trial 33 with value: 0.8940962499999999.\n",
      "[I 2025-11-05 12:51:16,625] Trial 41 pruned. \n",
      "[I 2025-11-05 12:51:25,495] Trial 42 pruned. \n",
      "[I 2025-11-05 12:51:28,854] Trial 43 pruned. \n",
      "[I 2025-11-05 12:51:32,291] Trial 44 pruned. \n",
      "[I 2025-11-05 12:51:35,462] Trial 45 pruned. \n",
      "[I 2025-11-05 12:51:39,457] Trial 46 pruned. \n",
      "[I 2025-11-05 12:51:41,965] Trial 47 pruned. \n",
      "[I 2025-11-05 12:51:43,972] Trial 48 pruned. \n",
      "[I 2025-11-05 12:51:47,804] Trial 49 pruned. \n",
      "[I 2025-11-05 12:51:52,107] Trial 50 pruned. \n",
      "[I 2025-11-05 12:52:12,104] Trial 51 finished with value: 0.8933784375 and parameters: {'n_estimators': 800, 'learning_rate': 0.02011974989993696, 'max_depth': 3, 'min_child_weight': 15, 'gamma': 4.0566431619299586, 'subsample': 0.8097764454165289, 'colsample_bytree': 0.775355373649941, 'colsample_bylevel': 0.8714351830805231, 'colsample_bynode': 0.6844628296044766, 'reg_alpha': 1.3420260784781957, 'reg_lambda': 2.1539076592841244, 'max_delta_step': 1, 'scale_pos_weight': 1.0870655758756806}. Best is trial 33 with value: 0.8940962499999999.\n",
      "[I 2025-11-05 12:52:20,599] Trial 52 pruned. \n",
      "[I 2025-11-05 12:52:24,615] Trial 53 pruned. \n",
      "[I 2025-11-05 12:52:28,757] Trial 54 pruned. \n",
      "[I 2025-11-05 12:52:48,378] Trial 55 finished with value: 0.8940289062500002 and parameters: {'n_estimators': 800, 'learning_rate': 0.02457187229745634, 'max_depth': 3, 'min_child_weight': 20, 'gamma': 3.378905033197819, 'subsample': 0.843806583650363, 'colsample_bytree': 0.8192486318977625, 'colsample_bylevel': 0.7580018504136046, 'colsample_bynode': 0.6999023295723831, 'reg_alpha': 0.4824301042789363, 'reg_lambda': 1.2912510066646425, 'max_delta_step': 1, 'scale_pos_weight': 1.208014226090807}. Best is trial 33 with value: 0.8940962499999999.\n",
      "[I 2025-11-05 12:52:51,727] Trial 56 pruned. \n",
      "[I 2025-11-05 12:52:55,386] Trial 57 pruned. \n",
      "[I 2025-11-05 12:53:13,653] Trial 58 finished with value: 0.89344515625 and parameters: {'n_estimators': 750, 'learning_rate': 0.024098428505461164, 'max_depth': 3, 'min_child_weight': 22, 'gamma': 2.708213003324819, 'subsample': 0.8285254934154684, 'colsample_bytree': 0.5845247620423695, 'colsample_bylevel': 0.7075815192977962, 'colsample_bynode': 0.663851460960065, 'reg_alpha': 0.27069855400913045, 'reg_lambda': 4.440566186316649, 'max_delta_step': 0, 'scale_pos_weight': 1.5747812752401509}. Best is trial 33 with value: 0.8940962499999999.\n",
      "[I 2025-11-05 12:53:29,306] Trial 59 finished with value: 0.8931546874999998 and parameters: {'n_estimators': 600, 'learning_rate': 0.034869931377359734, 'max_depth': 3, 'min_child_weight': 19, 'gamma': 2.3627359635824208, 'subsample': 0.8204077322199929, 'colsample_bytree': 0.810095341200115, 'colsample_bylevel': 0.8109828627589756, 'colsample_bynode': 0.7501605844412452, 'reg_alpha': 2.9757786665430315, 'reg_lambda': 6.821046025521374, 'max_delta_step': 1, 'scale_pos_weight': 1.2772584442943833}. Best is trial 33 with value: 0.8940962499999999.\n",
      "[I 2025-11-05 12:53:32,555] Trial 60 pruned. \n",
      "[I 2025-11-05 12:53:52,306] Trial 61 finished with value: 0.8937971875 and parameters: {'n_estimators': 800, 'learning_rate': 0.022668387180612377, 'max_depth': 3, 'min_child_weight': 14, 'gamma': 3.9863891914547476, 'subsample': 0.8072577420423532, 'colsample_bytree': 0.7592082896826626, 'colsample_bylevel': 0.7689327430164702, 'colsample_bynode': 0.7054070059351949, 'reg_alpha': 1.0684765746558713, 'reg_lambda': 1.941475975461245, 'max_delta_step': 1, 'scale_pos_weight': 1.137261012630952}. Best is trial 33 with value: 0.8940962499999999.\n",
      "[I 2025-11-05 12:54:11,988] Trial 62 finished with value: 0.8936504687500001 and parameters: {'n_estimators': 800, 'learning_rate': 0.023064689958321342, 'max_depth': 3, 'min_child_weight': 12, 'gamma': 3.983456257406144, 'subsample': 0.8071229209371186, 'colsample_bytree': 0.7536588827227181, 'colsample_bylevel': 0.7705695110809586, 'colsample_bynode': 0.698561163129296, 'reg_alpha': 0.7679832992045228, 'reg_lambda': 2.369154811213748, 'max_delta_step': 1, 'scale_pos_weight': 1.1851914635146512}. Best is trial 33 with value: 0.8940962499999999.\n",
      "[I 2025-11-05 12:54:30,507] Trial 63 finished with value: 0.893760625 and parameters: {'n_estimators': 750, 'learning_rate': 0.03102135539411819, 'max_depth': 3, 'min_child_weight': 17, 'gamma': 3.270640351777158, 'subsample': 0.8427545236429937, 'colsample_bytree': 0.7781473075817573, 'colsample_bylevel': 0.7652143583946412, 'colsample_bynode': 0.7048559805737147, 'reg_alpha': 0.9803228593718255, 'reg_lambda': 15.19446342112996, 'max_delta_step': 1, 'scale_pos_weight': 1.1361638997078303}. Best is trial 33 with value: 0.8940962499999999.\n",
      "[I 2025-11-05 12:54:49,662] Trial 64 finished with value: 0.8935135937500001 and parameters: {'n_estimators': 800, 'learning_rate': 0.041773434548374046, 'max_depth': 3, 'min_child_weight': 18, 'gamma': 3.0897033727495113, 'subsample': 0.8394016881134789, 'colsample_bytree': 0.7789285370489625, 'colsample_bylevel': 0.7471793797088445, 'colsample_bynode': 0.7107465588915051, 'reg_alpha': 0.9704028351723151, 'reg_lambda': 21.885898635367894, 'max_delta_step': 1, 'scale_pos_weight': 1.2058533356525072}. Best is trial 33 with value: 0.8940962499999999.\n",
      "[I 2025-11-05 12:54:53,858] Trial 65 pruned. \n",
      "[I 2025-11-05 12:54:58,250] Trial 66 pruned. \n",
      "[I 2025-11-05 12:55:01,526] Trial 67 pruned. \n",
      "[I 2025-11-05 12:55:08,952] Trial 68 pruned. \n",
      "[I 2025-11-05 12:55:15,888] Trial 69 pruned. \n",
      "[I 2025-11-05 12:55:18,560] Trial 70 pruned. \n",
      "[I 2025-11-05 12:55:26,320] Trial 71 pruned. \n",
      "[I 2025-11-05 12:55:48,310] Trial 72 finished with value: 0.8937565625000001 and parameters: {'n_estimators': 800, 'learning_rate': 0.021190254265066953, 'max_depth': 3, 'min_child_weight': 19, 'gamma': 3.7709472658447094, 'subsample': 0.8330278044670553, 'colsample_bytree': 0.7675689042428369, 'colsample_bylevel': 0.7779389049663419, 'colsample_bynode': 0.6621657780322395, 'reg_alpha': 0.8684300627931604, 'reg_lambda': 1.929792513315876, 'max_delta_step': 1, 'scale_pos_weight': 1.1014672299089132}. Best is trial 33 with value: 0.8940962499999999.\n",
      "[I 2025-11-05 12:56:09,835] Trial 73 finished with value: 0.8936825 and parameters: {'n_estimators': 800, 'learning_rate': 0.02101285232799247, 'max_depth': 3, 'min_child_weight': 19, 'gamma': 3.7741838878836433, 'subsample': 0.8336190877208107, 'colsample_bytree': 0.7815461922446171, 'colsample_bylevel': 0.7863493408938859, 'colsample_bynode': 0.6409063365064052, 'reg_alpha': 0.8557285254869145, 'reg_lambda': 1.9606539892006591, 'max_delta_step': 1, 'scale_pos_weight': 1.0927330353084643}. Best is trial 33 with value: 0.8940962499999999.\n",
      "[I 2025-11-05 12:56:13,394] Trial 74 pruned. \n",
      "[I 2025-11-05 12:56:16,781] Trial 75 pruned. \n",
      "[I 2025-11-05 12:56:21,507] Trial 76 pruned. \n",
      "[I 2025-11-05 12:56:25,984] Trial 77 pruned. \n",
      "[I 2025-11-05 12:56:30,523] Trial 78 pruned. \n",
      "[I 2025-11-05 12:56:35,575] Trial 79 pruned. \n",
      "[I 2025-11-05 12:56:40,147] Trial 80 pruned. \n",
      "[I 2025-11-05 12:56:44,799] Trial 81 pruned. \n",
      "[I 2025-11-05 12:56:49,757] Trial 82 pruned. \n",
      "[I 2025-11-05 12:57:11,741] Trial 83 finished with value: 0.8936390624999999 and parameters: {'n_estimators': 800, 'learning_rate': 0.02123249032098519, 'max_depth': 3, 'min_child_weight': 17, 'gamma': 3.552724011804089, 'subsample': 0.8253169762625, 'colsample_bytree': 0.807599451553469, 'colsample_bylevel': 0.7774573881551948, 'colsample_bynode': 0.6741488574257922, 'reg_alpha': 0.5742710915119629, 'reg_lambda': 2.252701106529912, 'max_delta_step': 1, 'scale_pos_weight': 1.0899524453256217}. Best is trial 33 with value: 0.8940962499999999.\n",
      "[I 2025-11-05 12:57:31,587] Trial 84 finished with value: 0.8937340624999999 and parameters: {'n_estimators': 750, 'learning_rate': 0.03463712614001544, 'max_depth': 3, 'min_child_weight': 20, 'gamma': 3.845404767931065, 'subsample': 0.8341395111774601, 'colsample_bytree': 0.7821627476842269, 'colsample_bylevel': 0.738125713251399, 'colsample_bynode': 0.6485256430230643, 'reg_alpha': 1.2272974321693133, 'reg_lambda': 1.561306344459776, 'max_delta_step': 2, 'scale_pos_weight': 1.0202519431204538}. Best is trial 33 with value: 0.8940962499999999.\n",
      "[I 2025-11-05 12:57:36,468] Trial 85 pruned. \n",
      "[I 2025-11-05 12:57:41,185] Trial 86 pruned. \n",
      "[I 2025-11-05 12:57:44,698] Trial 87 pruned. \n",
      "[I 2025-11-05 12:57:50,100] Trial 88 pruned. \n",
      "[I 2025-11-05 12:58:05,824] Trial 89 finished with value: 0.893541875 and parameters: {'n_estimators': 500, 'learning_rate': 0.04073897424988422, 'max_depth': 3, 'min_child_weight': 18, 'gamma': 3.3918673052220676, 'subsample': 0.8276862981033385, 'colsample_bytree': 0.7838380953600764, 'colsample_bylevel': 0.7339434063499268, 'colsample_bynode': 0.6967241894697584, 'reg_alpha': 0.4199734499677104, 'reg_lambda': 1.11183537000367, 'max_delta_step': 2, 'scale_pos_weight': 1.20451073772994}. Best is trial 33 with value: 0.8940962499999999.\n",
      "[I 2025-11-05 12:58:10,584] Trial 90 pruned. \n",
      "[I 2025-11-05 12:58:20,269] Trial 91 pruned. \n",
      "[I 2025-11-05 12:58:42,049] Trial 92 finished with value: 0.8939323437500001 and parameters: {'n_estimators': 750, 'learning_rate': 0.02883165420227283, 'max_depth': 3, 'min_child_weight': 14, 'gamma': 3.5812876687880824, 'subsample': 0.8223549408182476, 'colsample_bytree': 0.7925957014969082, 'colsample_bylevel': 0.7950546286926586, 'colsample_bynode': 0.6118555496936804, 'reg_alpha': 0.0827166734876153, 'reg_lambda': 1.4102371636168038, 'max_delta_step': 1, 'scale_pos_weight': 1.0015200917497877}. Best is trial 33 with value: 0.8940962499999999.\n",
      "[I 2025-11-05 12:59:03,426] Trial 93 finished with value: 0.893895 and parameters: {'n_estimators': 750, 'learning_rate': 0.030832834640417285, 'max_depth': 3, 'min_child_weight': 17, 'gamma': 3.1556468245092075, 'subsample': 0.8223594373882441, 'colsample_bytree': 0.7917054808976044, 'colsample_bylevel': 0.8047128651743859, 'colsample_bynode': 0.6161147241938544, 'reg_alpha': 0.17423010676404235, 'reg_lambda': 1.4110312401506397, 'max_delta_step': 1, 'scale_pos_weight': 1.0450052024508036}. Best is trial 33 with value: 0.8940962499999999.\n",
      "[I 2025-11-05 12:59:24,730] Trial 94 finished with value: 0.893909375 and parameters: {'n_estimators': 750, 'learning_rate': 0.028585726507681226, 'max_depth': 3, 'min_child_weight': 14, 'gamma': 3.59814063492087, 'subsample': 0.8232352766837839, 'colsample_bytree': 0.7665119543457903, 'colsample_bylevel': 0.7989618539069312, 'colsample_bynode': 0.607629129933158, 'reg_alpha': 0.09578828915892681, 'reg_lambda': 1.4110877506565056, 'max_delta_step': 0, 'scale_pos_weight': 1.0351231902509819}. Best is trial 33 with value: 0.8940962499999999.\n",
      "[I 2025-11-05 12:59:29,857] Trial 95 pruned. \n",
      "[I 2025-11-05 12:59:52,304] Trial 96 finished with value: 0.8938635937499999 and parameters: {'n_estimators': 750, 'learning_rate': 0.026914558087840677, 'max_depth': 3, 'min_child_weight': 12, 'gamma': 3.5982694433767115, 'subsample': 0.8073588960699047, 'colsample_bytree': 0.7502654380439582, 'colsample_bylevel': 0.8047677917711956, 'colsample_bynode': 0.6166755954123243, 'reg_alpha': 0.07403161868183677, 'reg_lambda': 1.7341880970157273, 'max_delta_step': 0, 'scale_pos_weight': 1.0001759723327561}. Best is trial 33 with value: 0.8940962499999999.\n",
      "[I 2025-11-05 13:00:13,704] Trial 97 finished with value: 0.8940906250000001 and parameters: {'n_estimators': 750, 'learning_rate': 0.027007883144146407, 'max_depth': 3, 'min_child_weight': 12, 'gamma': 3.597056920417509, 'subsample': 0.7870469599777559, 'colsample_bytree': 0.7506456818619893, 'colsample_bylevel': 0.8068171375470559, 'colsample_bynode': 0.6101793058385407, 'reg_alpha': 0.08422005437911391, 'reg_lambda': 1.2939302732459061, 'max_delta_step': 0, 'scale_pos_weight': 1.003505516997007}. Best is trial 33 with value: 0.8940962499999999.\n",
      "[I 2025-11-05 13:00:22,063] Trial 98 pruned. \n",
      "[I 2025-11-05 13:00:43,235] Trial 99 finished with value: 0.89423 and parameters: {'n_estimators': 750, 'learning_rate': 0.026955864869419904, 'max_depth': 3, 'min_child_weight': 11, 'gamma': 3.629872187374523, 'subsample': 0.7806892624438801, 'colsample_bytree': 0.6957925541165181, 'colsample_bylevel': 0.7994308933140254, 'colsample_bynode': 0.6129554460871888, 'reg_alpha': 0.1828942098960428, 'reg_lambda': 1.3129485137813097, 'max_delta_step': 0, 'scale_pos_weight': 1.033143094183888}. Best is trial 99 with value: 0.89423.\n",
      "[I 2025-11-05 13:00:48,070] Trial 100 pruned. \n",
      "[I 2025-11-05 13:01:08,939] Trial 101 finished with value: 0.893985625 and parameters: {'n_estimators': 750, 'learning_rate': 0.026841953796222364, 'max_depth': 3, 'min_child_weight': 11, 'gamma': 3.645732177515099, 'subsample': 0.7741887854054649, 'colsample_bytree': 0.6613320455869065, 'colsample_bylevel': 0.7967133864078182, 'colsample_bynode': 0.6174795709428701, 'reg_alpha': 0.09807502800937173, 'reg_lambda': 1.0784601502329487, 'max_delta_step': 0, 'scale_pos_weight': 1.008945873483868}. Best is trial 99 with value: 0.89423.\n",
      "[I 2025-11-05 13:01:30,511] Trial 102 finished with value: 0.8937340625000001 and parameters: {'n_estimators': 750, 'learning_rate': 0.02675417660151948, 'max_depth': 3, 'min_child_weight': 11, 'gamma': 3.459476901707648, 'subsample': 0.7702211140417945, 'colsample_bytree': 0.6577263434938598, 'colsample_bylevel': 0.7965315585711198, 'colsample_bynode': 0.6235074684518829, 'reg_alpha': 0.09619874804392493, 'reg_lambda': 1.0682176749213899, 'max_delta_step': 0, 'scale_pos_weight': 1.0000187544251615}. Best is trial 99 with value: 0.89423.\n",
      "[I 2025-11-05 13:01:50,592] Trial 103 finished with value: 0.8942303125000001 and parameters: {'n_estimators': 700, 'learning_rate': 0.028886354260163036, 'max_depth': 3, 'min_child_weight': 12, 'gamma': 3.3226278127757247, 'subsample': 0.7636282255017623, 'colsample_bytree': 0.630353262357179, 'colsample_bylevel': 0.8136839867132009, 'colsample_bynode': 0.6131475138186301, 'reg_alpha': 0.2905381304147245, 'reg_lambda': 1.302068496507773, 'max_delta_step': 0, 'scale_pos_weight': 1.0341636616062087}. Best is trial 103 with value: 0.8942303125000001.\n",
      "[I 2025-11-05 13:02:10,385] Trial 104 finished with value: 0.8935909375 and parameters: {'n_estimators': 700, 'learning_rate': 0.029214744445794957, 'max_depth': 3, 'min_child_weight': 8, 'gamma': 3.1166950283450188, 'subsample': 0.7740851131744376, 'colsample_bytree': 0.615144845794804, 'colsample_bylevel': 0.8373608853514877, 'colsample_bynode': 0.6146733041356128, 'reg_alpha': 0.2922447638530747, 'reg_lambda': 1.2838311688997615, 'max_delta_step': 0, 'scale_pos_weight': 1.0150437689979193}. Best is trial 103 with value: 0.8942303125000001.\n",
      "[I 2025-11-05 13:02:18,793] Trial 105 pruned. \n",
      "[I 2025-11-05 13:02:23,192] Trial 106 pruned. \n",
      "[I 2025-11-05 13:02:27,557] Trial 107 pruned. \n",
      "[I 2025-11-05 13:02:36,053] Trial 108 pruned. \n",
      "[I 2025-11-05 13:02:40,929] Trial 109 pruned. \n",
      "[I 2025-11-05 13:02:45,712] Trial 110 pruned. \n",
      "[I 2025-11-05 13:03:08,004] Trial 111 finished with value: 0.8942217187499999 and parameters: {'n_estimators': 750, 'learning_rate': 0.02685447852747093, 'max_depth': 3, 'min_child_weight': 13, 'gamma': 1.3447571765569726, 'subsample': 0.7144445251315167, 'colsample_bytree': 0.6426302191398031, 'colsample_bylevel': 0.8046556720759455, 'colsample_bynode': 0.6346306838484719, 'reg_alpha': 0.27244028091340394, 'reg_lambda': 1.0232473526864458, 'max_delta_step': 0, 'scale_pos_weight': 1.052313146844527}. Best is trial 103 with value: 0.8942303125000001.\n",
      "[I 2025-11-05 13:03:29,714] Trial 112 finished with value: 0.8943596875 and parameters: {'n_estimators': 750, 'learning_rate': 0.027183146095176653, 'max_depth': 3, 'min_child_weight': 13, 'gamma': 0.9630685697150754, 'subsample': 0.7657069144790073, 'colsample_bytree': 0.6375299152482371, 'colsample_bylevel': 0.8463663005130012, 'colsample_bynode': 0.6351510570517238, 'reg_alpha': 0.22745268657546328, 'reg_lambda': 1.0627184653362756, 'max_delta_step': 0, 'scale_pos_weight': 1.0605038673452052}. Best is trial 112 with value: 0.8943596875.\n",
      "[I 2025-11-05 13:03:38,136] Trial 113 pruned. \n",
      "[I 2025-11-05 13:03:43,409] Trial 114 pruned. \n",
      "[I 2025-11-05 13:04:05,206] Trial 115 finished with value: 0.8937834375 and parameters: {'n_estimators': 700, 'learning_rate': 0.028803049609181727, 'max_depth': 3, 'min_child_weight': 14, 'gamma': 3.1808533588849883, 'subsample': 0.7307656953249128, 'colsample_bytree': 0.6246266987249457, 'colsample_bylevel': 0.8316798934727376, 'colsample_bynode': 0.6022268979460308, 'reg_alpha': 0.1334052175315263, 'reg_lambda': 1.0109846338254378, 'max_delta_step': 0, 'scale_pos_weight': 1.0366873872424953}. Best is trial 112 with value: 0.8943596875.\n",
      "[I 2025-11-05 13:04:28,668] Trial 116 finished with value: 0.8938699999999999 and parameters: {'n_estimators': 750, 'learning_rate': 0.025392527881158894, 'max_depth': 3, 'min_child_weight': 16, 'gamma': 1.30360858405784, 'subsample': 0.7701159799894637, 'colsample_bytree': 0.6710706406108731, 'colsample_bylevel': 0.8750054658551485, 'colsample_bynode': 0.6376347809631674, 'reg_alpha': 0.31670914042982107, 'reg_lambda': 1.1498807914730262, 'max_delta_step': 0, 'scale_pos_weight': 1.0480252203280025}. Best is trial 112 with value: 0.8943596875.\n",
      "[I 2025-11-05 13:04:34,508] Trial 117 pruned. \n",
      "[I 2025-11-05 13:04:40,382] Trial 118 pruned. \n",
      "[I 2025-11-05 13:04:49,676] Trial 119 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best ROC AUC (cv): 0.8944\n",
      "Best params:\n",
      "  n_estimators: 750\n",
      "  learning_rate: 0.027183146095176653\n",
      "  max_depth: 3\n",
      "  min_child_weight: 13\n",
      "  gamma: 0.9630685697150754\n",
      "  subsample: 0.7657069144790073\n",
      "  colsample_bytree: 0.6375299152482371\n",
      "  colsample_bylevel: 0.8463663005130012\n",
      "  colsample_bynode: 0.6351510570517238\n",
      "  reg_alpha: 0.22745268657546328\n",
      "  reg_lambda: 1.0627184653362756\n",
      "  max_delta_step: 0\n",
      "  scale_pos_weight: 1.0605038673452052\n",
      "✅ Salvato optuna_trials_xgb.csv\n",
      "\n",
      "best_params pronti per la 10-Fold CV:\n",
      "{'n_estimators': 750, 'learning_rate': 0.027183146095176653, 'max_depth': 3, 'min_child_weight': 13, 'gamma': 0.9630685697150754, 'subsample': 0.7657069144790073, 'colsample_bytree': 0.6375299152482371, 'colsample_bylevel': 0.8463663005130012, 'colsample_bynode': 0.6351510570517238, 'reg_alpha': 0.22745268657546328, 'reg_lambda': 1.0627184653362756, 'max_delta_step': 0, 'scale_pos_weight': 1.0605038673452052, 'booster': 'gbtree'}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from xgboost import XGBClassifier\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Usa lo stesso split interno del GridSearch\n",
    "cv_inner = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Calibra il range di scale_pos_weight attorno al valore automatico\n",
    "pos_rate = float(y_train_val.mean())\n",
    "spw_auto = float((1.0 - pos_rate) / max(pos_rate, 1e-9))\n",
    "spw_low = max(1.0, spw_auto * 0.6)\n",
    "spw_high = min(10.0, spw_auto * 1.6)\n",
    "\n",
    "EARLY_STOPPING_ROUNDS = 50\n",
    "\n",
    "def _predict_proba_best(clf, X):\n",
    "    # Compat con diverse versioni XGBoost/SKLearn wrapper\n",
    "    best_iter = getattr(clf, \"best_iteration\", None)\n",
    "    try:\n",
    "        if best_iter is not None:\n",
    "            return clf.predict_proba(X, iteration_range=(0, int(best_iter)+1))[:, 1]\n",
    "    except TypeError:\n",
    "        pass\n",
    "    try:\n",
    "        booster = clf.get_booster()\n",
    "        best_ntree_limit = getattr(booster, \"best_ntree_limit\", None)\n",
    "        if best_ntree_limit is not None:\n",
    "            return clf.predict_proba(X, ntree_limit=int(best_ntree_limit))[:, 1]\n",
    "    except Exception:\n",
    "        pass\n",
    "    return clf.predict_proba(X)[:, 1]\n",
    "\n",
    "def objective(trial: optuna.Trial) -> float:\n",
    "    # Search space più conservativo per ridurre overfit\n",
    "    params = {\n",
    "        \"booster\": \"gbtree\",\n",
    "        \"tree_method\": \"hist\",\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 200, 800, step=50),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.07, log=True),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 5),\n",
    "        \"min_child_weight\": trial.suggest_int(\"min_child_weight\", 5, 50),\n",
    "        \"gamma\": trial.suggest_float(\"gamma\", 0.0, 5.0),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.6, 0.85),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 0.85),\n",
    "        \"colsample_bylevel\": trial.suggest_float(\"colsample_bylevel\", 0.6, 1.0),\n",
    "        \"colsample_bynode\": trial.suggest_float(\"colsample_bynode\", 0.6, 1.0),\n",
    "        \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 0.0, 3.0),\n",
    "        \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 1.0, 50.0, log=True),\n",
    "        \"max_delta_step\": trial.suggest_int(\"max_delta_step\", 0, 2),\n",
    "        \"scale_pos_weight\": trial.suggest_float(\"scale_pos_weight\", spw_low, spw_high),\n",
    "    }\n",
    "\n",
    "    fold_scores = []\n",
    "    for fold_idx, (tr_idx, va_idx) in enumerate(cv_inner.split(X_train_val, y_train_val), start=1):\n",
    "        X_tr, X_va = X_train_val[tr_idx], X_train_val[va_idx]\n",
    "        y_tr, y_va = y_train_val[tr_idx], y_train_val[va_idx]\n",
    "\n",
    "        clf = XGBClassifier(\n",
    "            **params,\n",
    "            use_label_encoder=False,\n",
    "            eval_metric=\"logloss\",\n",
    "            random_state=42,\n",
    "            n_jobs=1\n",
    "        )\n",
    "\n",
    "        # Early stopping per fold (compatibile con wrapper sklearn)\n",
    "        try:\n",
    "            clf.fit(X_tr, y_tr, eval_set=[(X_va, y_va)], early_stopping_rounds=EARLY_STOPPING_ROUNDS, verbose=False)\n",
    "        except TypeError:\n",
    "            # fallback: versione di XGBoost che non accetta early_stopping_rounds\n",
    "            clf.fit(X_tr, y_tr, eval_set=[(X_va, y_va)], verbose=False)\n",
    "\n",
    "        proba = _predict_proba_best(clf, X_va)\n",
    "        # usa ROC AUC sul proba (più stabile e meno dipendente dalla soglia)\n",
    "        try:\n",
    "            score = roc_auc_score(y_va, proba)\n",
    "        except Exception:\n",
    "            # fallback: se AUC non calcolabile (es. tutte le classi uguali), scorri con balanced_accuracy\n",
    "            from sklearn.metrics import balanced_accuracy_score\n",
    "            preds = (proba >= 0.5).astype(int)\n",
    "            score = balanced_accuracy_score(y_va, preds)\n",
    "\n",
    "        fold_scores.append(score)\n",
    "\n",
    "        # Pruning basato sulla media parziale\n",
    "        trial.report(float(np.mean(fold_scores)), step=fold_idx)\n",
    "        if trial.should_prune():\n",
    "            raise optuna.TrialPruned()\n",
    "\n",
    "    # restituisci media AUC (più alta = meglio)\n",
    "    return float(np.mean(fold_scores))\n",
    "\n",
    "# Parametri di ricerca\n",
    "N_TRIALS = 120    # regola in base al tempo\n",
    "TIMEOUT = None    # in secondi, es. 7200 per ~2h\n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\", sampler=TPESampler(seed=42))\n",
    "study.optimize(objective, n_trials=N_TRIALS, timeout=TIMEOUT, gc_after_trial=True)\n",
    "\n",
    "print(f\"Best ROC AUC (cv): {study.best_value:.4f}\")\n",
    "print(\"Best params:\")\n",
    "for k, v in study.best_params.items():\n",
    "    print(f\"  {k}: {v}\")\n",
    "\n",
    "# Esporta risultati\n",
    "trials_df = study.trials_dataframe()\n",
    "trials_df.to_csv(\"optuna_trials_xgb.csv\", index=False)\n",
    "print(\"✅ Salvato optuna_trials_xgb.csv\")\n",
    "\n",
    "# Prepara best_params per le celle successive (CV/holdout/submission)\n",
    "best_params = dict(study.best_params)\n",
    "best_params.update({\n",
    "    \"booster\": \"gbtree\",\n",
    "    # valori sicuri aggiuntivi se vuoi forzarli\n",
    "})\n",
    "print(\"\\nbest_params pronti per la 10-Fold CV:\")\n",
    "print(best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4ff77b",
   "metadata": {},
   "source": [
    "# Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "cab0d514",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 10-Fold Cross-Validation (9 train + 1 validation) ===\n",
      "Parametri utilizzati: {'booster': 'gbtree', 'tree_method': 'hist', 'max_bin': 256, 'learning_rate': 0.035, 'n_estimators': 900, 'max_depth': 3, 'min_child_weight': 9, 'gamma': 0.5, 'subsample': 0.7, 'colsample_bytree': 0.7, 'colsample_bynode': 0.7, 'colsample_bylevel': 0.8, 'reg_alpha': 0.4, 'reg_lambda': 10.0, 'max_delta_step': 1}\n",
      "\n",
      "Fold 1: no ES, train=7200, val=800, acc_val=81.12%, acc_val_opt=81.88% @thr=0.564, acc_train=86.64%, gap=5.51%\n",
      "Fold 2: no ES, train=7200, val=800, acc_val=81.38%, acc_val_opt=82.12% @thr=0.409, acc_train=86.56%, gap=5.18%\n",
      "Fold 3: no ES, train=7200, val=800, acc_val=82.12%, acc_val_opt=82.62% @thr=0.486, acc_train=86.49%, gap=4.36%\n",
      "Fold 4: no ES, train=7200, val=800, acc_val=83.75%, acc_val_opt=84.12% @thr=0.409, acc_train=86.49%, gap=2.74%\n",
      "Fold 5: no ES, train=7200, val=800, acc_val=84.50%, acc_val_opt=84.88% @thr=0.507, acc_train=86.21%, gap=1.71%\n",
      "Fold 6: no ES, train=7200, val=800, acc_val=81.50%, acc_val_opt=81.88% @thr=0.519, acc_train=86.65%, gap=5.15%\n",
      "Fold 7: no ES, train=7200, val=800, acc_val=84.12%, acc_val_opt=84.38% @thr=0.435, acc_train=86.31%, gap=2.18%\n",
      "Fold 8: no ES, train=7200, val=800, acc_val=83.62%, acc_val_opt=83.50% @thr=0.502, acc_train=86.40%, gap=2.78%\n",
      "Fold 9: no ES, train=7200, val=800, acc_val=82.50%, acc_val_opt=82.88% @thr=0.433, acc_train=86.79%, gap=4.29%\n",
      "Fold 10: no ES, train=7200, val=800, acc_val=81.50%, acc_val_opt=82.00% @thr=0.456, acc_train=86.82%, gap=5.32%\n",
      "\n",
      "============================================================\n",
      "Risultati Cross-Validation\n",
      "============================================================\n",
      "  Fold 1: val_acc=81.12%, val_acc_opt=81.88% @thr=0.564, train_acc=86.64%, gap=5.51%\n",
      "  Fold 2: val_acc=81.38%, val_acc_opt=82.12% @thr=0.409, train_acc=86.56%, gap=5.18%\n",
      "  Fold 3: val_acc=82.12%, val_acc_opt=82.62% @thr=0.486, train_acc=86.49%, gap=4.36%\n",
      "  Fold 4: val_acc=83.75%, val_acc_opt=84.12% @thr=0.409, train_acc=86.49%, gap=2.74%\n",
      "  Fold 5: val_acc=84.50%, val_acc_opt=84.88% @thr=0.507, train_acc=86.21%, gap=1.71%\n",
      "  Fold 6: val_acc=81.50%, val_acc_opt=81.88% @thr=0.519, train_acc=86.65%, gap=5.15%\n",
      "  Fold 7: val_acc=84.12%, val_acc_opt=84.38% @thr=0.435, train_acc=86.31%, gap=2.18%\n",
      "  Fold 8: val_acc=83.62%, val_acc_opt=83.50% @thr=0.502, train_acc=86.40%, gap=2.78%\n",
      "  Fold 9: val_acc=82.50%, val_acc_opt=82.88% @thr=0.433, train_acc=86.79%, gap=4.29%\n",
      "  Fold 10: val_acc=81.50%, val_acc_opt=82.00% @thr=0.456, train_acc=86.82%, gap=5.32%\n",
      "\n",
      "Mean CV accuracy (0.5): 82.61%\n",
      "Mean CV accuracy (opt thr): 83.03%\n",
      "Mean train accuracy: 86.53%\n",
      "Mean gap (train - val): 3.92%\n",
      "Std CV accuracy:  1.21%\n",
      "Min/Max val acc:  81.12% / 84.50%\n",
      "\n",
      "Peggiore fold: #1 con acc_val=81.12% | acc_val_opt=81.88% | acc_train=86.64% | gap=5.51%\n"
     ]
    }
   ],
   "source": [
    "# === 10-Fold Cross-Validation con iperparametri FISSI ===\n",
    "# IMPORTANTE: Assegna qui i migliori iperparametri trovati dalla cella precedente\n",
    "# Oppure lascia questi di default (conservativi per ridurre overfitting)\n",
    "\n",
    "best_params = {\n",
    "    'booster': 'gbtree',\n",
    "    'tree_method': 'hist',\n",
    "    'max_bin': 256,          # istogrammi più grossolani = meno varianza e più veloce\n",
    "    'learning_rate': 0.035,  # leggermente più alto con meno alberi\n",
    "    'n_estimators': 900,     # meno alberi per ridurre overfitting\n",
    "    'max_depth': 3,\n",
    "    'min_child_weight': 9,   # nodi con più peso => meno overfit\n",
    "    'gamma': 0.5,            # penalizza split deboli\n",
    "    'subsample': 0.7,\n",
    "    'colsample_bytree': 0.7,\n",
    "    'colsample_bynode': 0.7,\n",
    "    'colsample_bylevel': 0.8,\n",
    "    'reg_alpha': 0.4,        # L1\n",
    "    'reg_lambda': 10.0,      # L2 più alta\n",
    "    'max_delta_step': 1\n",
    "}\n",
    "\n",
    "print(\"=== 10-Fold Cross-Validation (9 train + 1 validation) ===\")\n",
    "print(f\"Parametri utilizzati: {best_params}\\n\")\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import xgboost as xgb  # per callback EarlyStopping se disponibile\n",
    "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "outer_accuracies = []\n",
    "folds_info = []\n",
    "train_accuracies = []\n",
    "train_val_gaps = []\n",
    "outer_accuracies_opt = []\n",
    "\n",
    "EARLY_STOPPING_ROUNDS = 50\n",
    "\n",
    "def best_threshold_for_accuracy(y_true, proba, n_grid=201):\n",
    "    y_true = np.asarray(y_true).astype(int)\n",
    "    proba = np.asarray(proba).astype(float)\n",
    "    grid = np.unique(np.quantile(proba, np.linspace(0, 1, n_grid)))\n",
    "    best_thr, best_acc = 0.5, 0.0\n",
    "    for t in grid:\n",
    "        acc = ( ((proba >= t).astype(int) == y_true).mean() )\n",
    "        if (acc > best_acc) or (abs(acc - best_acc) < 1e-12 and abs(t - 0.5) < abs(best_thr - 0.5)):\n",
    "            best_acc, best_thr = float(acc), float(t)\n",
    "    return best_thr, best_acc\n",
    "\n",
    "def _fit_with_es(clf, X_tr, y_tr, X_val, y_val):\n",
    "    \"\"\"Fit con EarlyStopping via callback se supportato; fallback senza ES.\"\"\"\n",
    "    try:\n",
    "        cb = getattr(xgb.callback, 'EarlyStopping', None)\n",
    "        if cb is not None:\n",
    "            clf.fit(X_tr, y_tr, eval_set=[(X_val, y_val)], callbacks=[cb(rounds=EARLY_STOPPING_ROUNDS, save_best=True, maximize=False)], verbose=False)\n",
    "            return True\n",
    "    except TypeError:\n",
    "        pass\n",
    "    clf.fit(X_tr, y_tr, eval_set=[(X_val, y_val)], verbose=False)\n",
    "    return False\n",
    "\n",
    "def _predict_proba_best(clf, X, best_iter=None, best_ntree_limit=None):\n",
    "    \"\"\"Version-safe predict_proba using either iteration_range (new) or ntree_limit (old).\"\"\"\n",
    "    try:\n",
    "        if best_iter is not None:\n",
    "            return clf.predict_proba(X, iteration_range=(0, int(best_iter)+1))[:, 1]\n",
    "    except TypeError:\n",
    "        pass\n",
    "    try:\n",
    "        if best_ntree_limit is not None:\n",
    "            return clf.predict_proba(X, ntree_limit=int(best_ntree_limit))[:, 1]\n",
    "    except TypeError:\n",
    "        pass\n",
    "    return clf.predict_proba(X)[:, 1]\n",
    "\n",
    "fold_idx = 0\n",
    "for train_idx, val_idx in skf.split(X_train_val, y_train_val):\n",
    "    fold_idx += 1\n",
    "    X_tr, X_val = X_train_val[train_idx], X_train_val[val_idx]\n",
    "    y_tr, y_val = y_train_val[train_idx], y_train_val[val_idx]\n",
    "\n",
    "    clf = XGBClassifier(**best_params, use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
    "    used_es = _fit_with_es(clf, X_tr, y_tr, X_val, y_val)\n",
    "\n",
    "    best_iter = getattr(clf, 'best_iteration', None)\n",
    "    try:\n",
    "        booster = clf.get_booster()\n",
    "    except Exception:\n",
    "        booster = None\n",
    "    best_ntree_limit = getattr(booster, 'best_ntree_limit', None) if booster is not None else None\n",
    "\n",
    "    y_val_proba = _predict_proba_best(clf, X_val, best_iter, best_ntree_limit)\n",
    "    y_pred = (y_val_proba >= 0.5).astype(int)\n",
    "    acc = accuracy_score(y_val, y_pred)\n",
    "    outer_accuracies.append(acc)\n",
    "\n",
    "    y_tr_proba = _predict_proba_best(clf, X_tr, best_iter, best_ntree_limit)\n",
    "    y_tr_pred = (y_tr_proba >= 0.5).astype(int)\n",
    "    tr_acc = accuracy_score(y_tr, y_tr_pred)\n",
    "    gap = float(tr_acc - acc)\n",
    "    train_accuracies.append(tr_acc)\n",
    "    train_val_gaps.append(gap)\n",
    "\n",
    "    thr_acc, acc_opt = best_threshold_for_accuracy(y_val, y_val_proba, n_grid=301)\n",
    "    outer_accuracies_opt.append(acc_opt)\n",
    "\n",
    "    val_index_global = idx_train_val[val_idx]\n",
    "    train_index_global = idx_train_val[train_idx]\n",
    "\n",
    "    folds_info.append({\n",
    "        'fold': fold_idx,\n",
    "        'acc': float(acc),\n",
    "        'train_acc': float(tr_acc),\n",
    "        'gap_train_minus_val': float(gap),\n",
    "        'acc_opt': float(acc_opt),\n",
    "        'thr_acc': float(thr_acc),\n",
    "        'best_iteration': int(best_iter) if best_iter is not None else None,\n",
    "        'train_idx': train_idx,\n",
    "        'val_idx': val_idx,\n",
    "        'train_index_global': train_index_global,\n",
    "        'val_index_global': val_index_global,\n",
    "        'y_true': y_val.astype(int),\n",
    "        'y_pred': y_pred.astype(int),\n",
    "        'y_proba': y_val_proba.astype(float)\n",
    "    })\n",
    "\n",
    "    es_tag = 'with ES' if used_es else 'no ES'\n",
    "    print(f'Fold {fold_idx}: {es_tag}, train={len(y_tr)}, val={len(y_val)}, acc_val={acc*100:.2f}%, acc_val_opt={acc_opt*100:.2f}% @thr={thr_acc:.3f}, acc_train={tr_acc*100:.2f}%, gap={(gap)*100:.2f}%')\n",
    "\n",
    "print('\\n' + '='*60)\n",
    "print('Risultati Cross-Validation')\n",
    "print('='*60)\n",
    "for i, a in enumerate(outer_accuracies, 1):\n",
    "    print(f'  Fold {i}: val_acc={a*100:.2f}%, val_acc_opt={outer_accuracies_opt[i-1]*100:.2f}% @thr={folds_info[i-1][\"thr_acc\"]:.3f}, train_acc={train_accuracies[i-1]*100:.2f}%, gap={train_val_gaps[i-1]*100:.2f}%')\n",
    "print(f'\\nMean CV accuracy (0.5): {np.mean(outer_accuracies)*100:.2f}%')\n",
    "print(f'Mean CV accuracy (opt thr): {np.mean(outer_accuracies_opt)*100:.2f}%')\n",
    "print(f'Mean train accuracy: {np.mean(train_accuracies)*100:.2f}%')\n",
    "print(f'Mean gap (train - val): {np.mean(train_val_gaps)*100:.2f}%')\n",
    "print(f'Std CV accuracy:  {np.std(outer_accuracies)*100:.2f}%')\n",
    "print(f'Min/Max val acc:  {np.min(outer_accuracies)*100:.2f}% / {np.max(outer_accuracies)*100:.2f}%')\n",
    "\n",
    "WORST_FOLD_IDX = int(np.argmin(outer_accuracies))\n",
    "WORST_FOLD_NUM = int(folds_info[WORST_FOLD_IDX]['fold'])\n",
    "print(f\"\\nPeggiore fold: #{WORST_FOLD_NUM} con acc_val={outer_accuracies[WORST_FOLD_IDX]*100:.2f}% | acc_val_opt={outer_accuracies_opt[WORST_FOLD_IDX]*100:.2f}% | acc_train={train_accuracies[WORST_FOLD_IDX]*100:.2f}% | gap={train_val_gaps[WORST_FOLD_IDX]*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aef63ee",
   "metadata": {},
   "source": [
    "# Holdout validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c175e44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start holdout evaluation\n",
      "Using best_params from notebook\n",
      "Holdout size: 2000\n",
      "Accuracy: 0.8245, Balanced Accuracy: 0.8245, ROC AUC: 0.8937\n",
      "Confusion matrix:\n",
      " [[830 170]\n",
      " [181 819]]\n",
      "\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8210    0.8300    0.8255      1000\n",
      "           1     0.8281    0.8190    0.8235      1000\n",
      "\n",
      "    accuracy                         0.8245      2000\n",
      "   macro avg     0.8245    0.8245    0.8245      2000\n",
      "weighted avg     0.8245    0.8245    0.8245      2000\n",
      "\n",
      "Holdout predictions saved to holdout_predictions.csv (rows=2000)\n"
     ]
    }
   ],
   "source": [
    "# === Valutazione su holdout ===\n",
    "from sklearn.metrics import balanced_accuracy_score, roc_auc_score, confusion_matrix, classification_report\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "print('Start holdout evaluation')\n",
    "\n",
    "# Usa best_params se esiste, fallback a parametri di base\n",
    "try:\n",
    "    params = dict(best_params)\n",
    "    print('Using best_params from notebook')\n",
    "except Exception:\n",
    "    params = {}\n",
    "    print('best_params non trovato: uso parametri di default')\n",
    "\n",
    "clf = XGBClassifier(**params, use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
    "clf.fit(X_train_val, y_train_val)\n",
    "\n",
    "# predizioni e probabilità su holdout\n",
    "try:\n",
    "    proba = clf.predict_proba(X_holdout)[:, 1]\n",
    "except Exception:\n",
    "    # fallback: predict then map to 0/1 probs\n",
    "    preds_tmp = clf.predict(X_holdout)\n",
    "    proba = preds_tmp.astype(float)\n",
    "\n",
    "pred = (proba >= 0.5).astype(int)\n",
    "\n",
    "# metriche\n",
    "acc = (pred == y_holdout).mean()\n",
    "bacc = balanced_accuracy_score(y_holdout, pred)\n",
    "try:\n",
    "    roc = roc_auc_score(y_holdout, proba)\n",
    "except Exception:\n",
    "    roc = float('nan')\n",
    "\n",
    "cm = confusion_matrix(y_holdout, pred)\n",
    "cr = classification_report(y_holdout, pred, digits=4)\n",
    "\n",
    "print(f'Holdout size: {len(y_holdout)}')\n",
    "print(f'Accuracy: {acc:.4f}, Balanced Accuracy: {bacc:.4f}, ROC AUC: {roc:.4f}')\n",
    "print('Confusion matrix:\\n', cm)\n",
    "print('\\nClassification report:\\n', cr)\n",
    "\n",
    "# Salva predizioni holdout per ispezione (include battle_id se disponibili)\n",
    "try:\n",
    "    holdout_idx = idx_holdout\n",
    "    holdout_ids = train_df.loc[holdout_idx, 'battle_id'] if 'battle_id' in train_df.columns else pd.Series(holdout_idx, index=holdout_idx)\n",
    "    out_df = pd.DataFrame({'battle_id': holdout_ids.values, 'y_true': y_holdout, 'y_pred': pred, 'y_proba': proba})\n",
    "except Exception:\n",
    "    out_df = pd.DataFrame({'y_true': y_holdout, 'y_pred': pred, 'y_proba': proba})\n",
    "\n",
    "out_path = 'holdout_predictions.csv'\n",
    "out_df.to_csv(out_path, index=False)\n",
    "print(f'Holdout predictions saved to {out_path} (rows={len(out_df)})')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8873db82",
   "metadata": {},
   "source": [
    "# Make submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3b87173",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Submission rapida post-CV ===\n",
      "✅ File di submission salvato in submission.csv\n",
      "   battle_id  player_won\n",
      "0          0           0\n",
      "1          1           1\n",
      "2          2           1\n",
      "3          3           1\n",
      "4          4           1\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Submission rapida post-CV ===\")\n",
    "cv_submission_model = XGBClassifier(**best_params, use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
    "cv_submission_model.fit(X_train_val, y_train_val)\n",
    "\n",
    "test_aligned = test_df.reindex(columns=FEATURES, fill_value=0)\n",
    "X_test_matrix = test_aligned.astype(float).to_numpy()\n",
    "test_predictions = cv_submission_model.predict(X_test_matrix).astype(int)\n",
    "\n",
    "submission_df = pd.DataFrame({\n",
    "    'battle_id': test_df['battle_id'].astype(np.int64),\n",
    "    'player_won': test_predictions.astype(np.int64)\n",
    "})\n",
    "\n",
    "submission_path = 'submission.csv'\n",
    "submission_df.to_csv(submission_path, index=False)\n",
    "print(f\"✅ File di submission salvato in {submission_path}\")\n",
    "print(submission_df.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
