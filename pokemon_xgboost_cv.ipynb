{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9b7eeca",
   "metadata": {},
   "source": [
    "# Pokémon battles — XGBoost with 10-fold outer CV\n",
    "Notebook breve che esegue: feature engineering, split train/val/test, 10-fold outer CV con GridSearchCV interno, valutazione per fold, valutazione su holdout e generazione submission.csv."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae11f50",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3aebef2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caricamento dati...\n",
      "Train records: 10000, Test records: 5000\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from xgboost import XGBClassifier\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# --- Percorsi (modificare se necessario) ---\n",
    "COMPETITION_NAME = 'fds-pokemon-battles-prediction-2025'\n",
    "train_file_path = 'train.jsonl'\n",
    "test_file_path = 'test.jsonl'\n",
    "\n",
    "def load_jsonl(path):\n",
    "    data = []\n",
    "    with open(path, 'r') as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line))\n",
    "    return data\n",
    "\n",
    "print('Caricamento dati...')\n",
    "train_raw = load_jsonl(train_file_path)\n",
    "test_raw = load_jsonl(test_file_path)\n",
    "print(f'Train records: {len(train_raw)}, Test records: {len(test_raw)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea38e58a",
   "metadata": {},
   "source": [
    "# Features engeneering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e15c30c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FE:   0%|          | 0/10000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FE: 100%|██████████| 10000/10000 [00:19<00:00, 502.39it/s]\n",
      "FE: 100%|██████████| 5000/5000 [00:07<00:00, 673.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape train/test: (10000, 236) (5000, 235)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>battle_id</th>\n",
       "      <th>player_won</th>\n",
       "      <th>p1_base_hp_sum</th>\n",
       "      <th>p1_base_hp_mean</th>\n",
       "      <th>p1_base_hp_max</th>\n",
       "      <th>p1_base_hp_min</th>\n",
       "      <th>p1_base_hp_std</th>\n",
       "      <th>p1_base_atk_sum</th>\n",
       "      <th>p1_base_atk_mean</th>\n",
       "      <th>p1_base_atk_max</th>\n",
       "      <th>...</th>\n",
       "      <th>team_hp_sum_minus_p2lead_hp</th>\n",
       "      <th>team_spa_mean_minus_p2spa</th>\n",
       "      <th>speed_advantage</th>\n",
       "      <th>n_unique_types_diff</th>\n",
       "      <th>damage_per_turn_diff</th>\n",
       "      <th>last_pair</th>\n",
       "      <th>p1_vs_lead_avg_effectiveness</th>\n",
       "      <th>p1_vs_lead_max_effectiveness</th>\n",
       "      <th>p1_super_effective_options</th>\n",
       "      <th>p1_se_options_vs_lead_bulk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>695.0</td>\n",
       "      <td>115.833333</td>\n",
       "      <td>250.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>69.367179</td>\n",
       "      <td>435.0</td>\n",
       "      <td>72.500000</td>\n",
       "      <td>110.0</td>\n",
       "      <td>...</td>\n",
       "      <td>635.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>365.0</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.070393</td>\n",
       "      <td>starmie_VS_snorlax</td>\n",
       "      <td>1.083333</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.005405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>740.0</td>\n",
       "      <td>123.333333</td>\n",
       "      <td>250.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>64.204534</td>\n",
       "      <td>435.0</td>\n",
       "      <td>72.500000</td>\n",
       "      <td>110.0</td>\n",
       "      <td>...</td>\n",
       "      <td>685.0</td>\n",
       "      <td>-45.000000</td>\n",
       "      <td>250.0</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.012174</td>\n",
       "      <td>tauros_VS_alakazam</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>745.0</td>\n",
       "      <td>124.166667</td>\n",
       "      <td>250.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>64.382753</td>\n",
       "      <td>505.0</td>\n",
       "      <td>84.166667</td>\n",
       "      <td>130.0</td>\n",
       "      <td>...</td>\n",
       "      <td>495.0</td>\n",
       "      <td>-15.000000</td>\n",
       "      <td>345.0</td>\n",
       "      <td>6</td>\n",
       "      <td>-0.000690</td>\n",
       "      <td>snorlax_VS_gengar</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>730.0</td>\n",
       "      <td>121.666667</td>\n",
       "      <td>250.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>65.362239</td>\n",
       "      <td>465.0</td>\n",
       "      <td>77.500000</td>\n",
       "      <td>110.0</td>\n",
       "      <td>...</td>\n",
       "      <td>655.0</td>\n",
       "      <td>33.333333</td>\n",
       "      <td>345.0</td>\n",
       "      <td>6</td>\n",
       "      <td>-0.014574</td>\n",
       "      <td>snorlax_VS_zapdos</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>685.0</td>\n",
       "      <td>114.166667</td>\n",
       "      <td>250.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>70.794107</td>\n",
       "      <td>455.0</td>\n",
       "      <td>75.833333</td>\n",
       "      <td>110.0</td>\n",
       "      <td>...</td>\n",
       "      <td>625.0</td>\n",
       "      <td>-2.500000</td>\n",
       "      <td>320.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.006923</td>\n",
       "      <td>tauros_VS_chansey</td>\n",
       "      <td>1.083333</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.005405</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 236 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   battle_id  player_won  p1_base_hp_sum  p1_base_hp_mean  p1_base_hp_max  \\\n",
       "0          0           1           695.0       115.833333           250.0   \n",
       "1          1           1           740.0       123.333333           250.0   \n",
       "2          2           1           745.0       124.166667           250.0   \n",
       "3          3           1           730.0       121.666667           250.0   \n",
       "4          4           1           685.0       114.166667           250.0   \n",
       "\n",
       "   p1_base_hp_min  p1_base_hp_std  p1_base_atk_sum  p1_base_atk_mean  \\\n",
       "0            55.0       69.367179            435.0         72.500000   \n",
       "1            65.0       64.204534            435.0         72.500000   \n",
       "2            60.0       64.382753            505.0         84.166667   \n",
       "3            60.0       65.362239            465.0         77.500000   \n",
       "4            50.0       70.794107            455.0         75.833333   \n",
       "\n",
       "   p1_base_atk_max  ...  team_hp_sum_minus_p2lead_hp  \\\n",
       "0            110.0  ...                        635.0   \n",
       "1            110.0  ...                        685.0   \n",
       "2            130.0  ...                        495.0   \n",
       "3            110.0  ...                        655.0   \n",
       "4            110.0  ...                        625.0   \n",
       "\n",
       "   team_spa_mean_minus_p2spa  speed_advantage  n_unique_types_diff  \\\n",
       "0                   0.000000            365.0                    3   \n",
       "1                 -45.000000            250.0                    4   \n",
       "2                 -15.000000            345.0                    6   \n",
       "3                  33.333333            345.0                    6   \n",
       "4                  -2.500000            320.0                    4   \n",
       "\n",
       "   damage_per_turn_diff           last_pair  p1_vs_lead_avg_effectiveness  \\\n",
       "0             -0.070393  starmie_VS_snorlax                      1.083333   \n",
       "1             -0.012174  tauros_VS_alakazam                      1.000000   \n",
       "2             -0.000690   snorlax_VS_gengar                      1.000000   \n",
       "3             -0.014574   snorlax_VS_zapdos                      1.000000   \n",
       "4              0.006923   tauros_VS_chansey                      1.083333   \n",
       "\n",
       "   p1_vs_lead_max_effectiveness  p1_super_effective_options  \\\n",
       "0                           2.0                           1   \n",
       "1                           1.0                           0   \n",
       "2                           1.0                           0   \n",
       "3                           1.0                           0   \n",
       "4                           2.0                           1   \n",
       "\n",
       "   p1_se_options_vs_lead_bulk  \n",
       "0                    0.005405  \n",
       "1                    0.000000  \n",
       "2                    0.000000  \n",
       "3                    0.000000  \n",
       "4                    0.005405  \n",
       "\n",
       "[5 rows x 236 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import math\n",
    "from collections import Counter\n",
    "\n",
    "# === TYPE CHART (Gen 1) ===\n",
    "TYPE_CHART = {\n",
    "    'normal': {'rock': 0.5, 'ghost': 0},\n",
    "    'fire': {'fire': 0.5, 'water': 0.5, 'grass': 2, 'ice': 2, 'bug': 2, 'rock': 0.5, 'dragon': 0.5},\n",
    "    'water': {'fire': 2, 'water': 0.5, 'grass': 0.5, 'ground': 2, 'rock': 2, 'dragon': 0.5},\n",
    "    'grass': {'fire': 0.5, 'water': 2, 'grass': 0.5, 'poison': 0.5, 'ground': 2, 'flying': 0.5, 'bug': 0.5, 'rock': 2, 'dragon': 0.5},\n",
    "    'electric': {'water': 2, 'grass': 0.5, 'electric': 0.5, 'ground': 0, 'flying': 2, 'dragon': 0.5},\n",
    "    'ice': {'fire': 0.5, 'water': 0.5, 'grass': 2, 'ground': 2, 'flying': 2, 'dragon': 2},\n",
    "    'fighting': {'normal': 2, 'ice': 2, 'poison': 0.5, 'flying': 0.5, 'psychic': 0.5, 'bug': 0.5, 'rock': 2, 'ghost': 0},\n",
    "    'poison': {'grass': 2, 'poison': 0.5, 'ground': 0.5, 'bug': 2, 'rock': 0.5, 'ghost': 0.5},\n",
    "    'ground': {'fire': 2, 'grass': 0.5, 'electric': 2, 'poison': 2, 'flying': 0, 'bug': 0.5, 'rock': 2},\n",
    "    'flying': {'grass': 2, 'electric': 0.5, 'fighting': 2, 'bug': 2, 'rock': 0.5},\n",
    "    'psychic': {'fighting': 2, 'poison': 2, 'psychic': 0.5, 'ghost': 0},\n",
    "    'bug': {'fire': 0.5, 'grass': 2, 'fighting': 0.5, 'poison': 2, 'flying': 0.5, 'psychic': 2, 'ghost': 0.5},\n",
    "    'rock': {'fire': 2, 'ice': 2, 'fighting': 0.5, 'ground': 0.5, 'flying': 2, 'bug': 2},\n",
    "    'ghost': {'normal': 0, 'psychic': 0, 'ghost': 2},\n",
    "    'dragon': {'dragon': 2}\n",
    "}\n",
    "\n",
    "def get_effectiveness(attack_type: str, defense_types: list) -> float:\n",
    "    if not attack_type or not defense_types:\n",
    "        return 1.0\n",
    "    eff = 1.0\n",
    "    for d in defense_types:\n",
    "        eff *= TYPE_CHART.get(attack_type, {}).get(d, 1.0)\n",
    "    return eff\n",
    "\n",
    "def calculate_type_advantage(team1: list, team2_lead: dict) -> dict:\n",
    "    out = {'p1_vs_lead_avg_effectiveness': 0.0, 'p1_vs_lead_max_effectiveness': 0.0, 'p1_super_effective_options': 0}\n",
    "    if not team1 or not team2_lead:\n",
    "        return out\n",
    "    lead_types = [t.lower() for t in team2_lead.get('types', [])]\n",
    "    if not lead_types:\n",
    "        return out\n",
    "    effs = []\n",
    "    for p in team1:\n",
    "        p_types = [t.lower() for t in p.get('types', [])]\n",
    "        max_eff = 0.0\n",
    "        for pt in p_types:\n",
    "            max_eff = max(max_eff, get_effectiveness(pt, lead_types))\n",
    "        effs.append(max_eff)\n",
    "    if not effs:\n",
    "        return out\n",
    "    out['p1_vs_lead_avg_effectiveness'] = float(np.mean(effs))\n",
    "    out['p1_vs_lead_max_effectiveness'] = float(np.max(effs))\n",
    "    out['p1_super_effective_options'] = int(sum(1 for e in effs if e >= 2))\n",
    "    return out\n",
    "\n",
    "def _entropy(counter: Counter) -> float:\n",
    "    total = sum(counter.values())\n",
    "    if total == 0:\n",
    "        return 0.0\n",
    "    ent = 0.0\n",
    "    for v in counter.values():\n",
    "        p = v / total\n",
    "        if p > 0:\n",
    "            ent -= p * math.log(p, 2)\n",
    "    return ent\n",
    "\n",
    "def team_aggregate_features(team: list, prefix: str = 'p1_') -> dict:\n",
    "    stats = ['base_hp','base_atk','base_def','base_spa','base_spd','base_spe']\n",
    "    out = {}\n",
    "    vals = {s: [] for s in stats}\n",
    "    levels = []\n",
    "    types_counter = Counter()\n",
    "    names = []\n",
    "    for p in team:\n",
    "        names.append(p.get('name',''))\n",
    "        for s in stats:\n",
    "            vals[s].append(p.get(s, 0))\n",
    "        levels.append(p.get('level', 0))\n",
    "        for t in p.get('types', []):\n",
    "            types_counter[t.lower()] += 1\n",
    "    for s in stats:\n",
    "        arr = np.array(vals[s], dtype=float)\n",
    "        out[f'{prefix}{s}_sum'] = float(arr.sum())\n",
    "        out[f'{prefix}{s}_mean'] = float(arr.mean())\n",
    "        out[f'{prefix}{s}_max'] = float(arr.max())\n",
    "        out[f'{prefix}{s}_min'] = float(arr.min())\n",
    "        out[f'{prefix}{s}_std'] = float(arr.std())\n",
    "    level_arr = np.array(levels, dtype=float)\n",
    "    out[f'{prefix}level_mean'] = float(level_arr.mean()) if level_arr.size else 0.0\n",
    "    out[f'{prefix}level_sum'] = float(level_arr.sum()) if level_arr.size else 0.0\n",
    "    out[f'{prefix}n_unique_types'] = int(len(types_counter))\n",
    "    common_types = ['normal','fire','water','electric','grass','psychic','ice','dragon','rock','ground','flying']\n",
    "    for t in common_types:\n",
    "        out[f'{prefix}type_{t}_count'] = int(types_counter.get(t, 0))\n",
    "    out[f'{prefix}lead_name'] = names[0] if names else ''\n",
    "    out[f'{prefix}n_unique_names'] = int(len(set(names)))\n",
    "    out[f'{prefix}type_entropy'] = float(_entropy(types_counter))\n",
    "    spe_arr = np.array(vals['base_spe'], dtype=float)\n",
    "    out[f'{prefix}spe_p25'] = float(np.percentile(spe_arr, 25)) if spe_arr.size else 0.0\n",
    "    out[f'{prefix}spe_p50'] = float(np.percentile(spe_arr, 50)) if spe_arr.size else 0.0\n",
    "    out[f'{prefix}spe_p75'] = float(np.percentile(spe_arr, 75)) if spe_arr.size else 0.0\n",
    "    return out\n",
    "\n",
    "def lead_vs_lead_features(p1_lead: dict, p2_lead: dict) -> dict:\n",
    "    out = {}\n",
    "    stats = ['base_hp','base_atk','base_def','base_spa','base_spd','base_spe']\n",
    "    for s in stats:\n",
    "        out[f'lead_diff_{s}'] = float(p1_lead.get(s,0) - p2_lead.get(s,0))\n",
    "    out['lead_speed_advantage'] = float(p1_lead.get('base_spe',0) - p2_lead.get('base_spe',0))\n",
    "    p1_types = [t.lower() for t in p1_lead.get('types', [])]\n",
    "    p2_types = [t.lower() for t in p2_lead.get('types', [])]\n",
    "    max_eff = 0.0\n",
    "    for pt in p1_types:\n",
    "        max_eff = max(max_eff, get_effectiveness(pt, p2_types))\n",
    "    out['lead_p1_vs_p2_effectiveness'] = float(max_eff)\n",
    "    return out\n",
    "\n",
    "def lead_aggregate_features(pokemon: dict, prefix: str = 'p2_lead_') -> dict:\n",
    "    out = {}\n",
    "    stats = ['base_hp','base_atk','base_def','base_spa','base_spd','base_spe']\n",
    "    for s in stats:\n",
    "        out[f'{prefix}{s}'] = float(pokemon.get(s,0))\n",
    "    out[f'{prefix}level'] = int(pokemon.get('level',0))\n",
    "    types = [x.lower() for x in pokemon.get('types', [])]\n",
    "    common_types = ['normal','fire','water','electric','grass','psychic','ice','dragon','rock','ground','flying']\n",
    "    for t in common_types:\n",
    "        out[f'{prefix}type_{t}'] = int(t in types)\n",
    "    out[f'{prefix}name'] = pokemon.get('name','')\n",
    "    out[f'{prefix}n_unique_types'] = int(len(set(types)))\n",
    "    return out\n",
    "\n",
    "def summary_from_timeline(timeline: list, p1_team: list) -> dict:\n",
    "    out = {}\n",
    "    if not timeline:\n",
    "        return {'tl_p1_moves':0,'tl_p2_moves':0,'tl_p1_est_damage':0.0,'tl_p2_est_damage':0.0,'damage_diff':0.0}\n",
    "    p1_moves = p2_moves = 0\n",
    "    p1_damage = p2_damage = 0.0\n",
    "    p1_last_active = p2_last_active = ''\n",
    "    p1_last_hp = p2_last_hp = np.nan\n",
    "    p1_fainted = p2_fainted = 0\n",
    "    p1_fainted_names = set()\n",
    "    p2_fainted_names = set()\n",
    "    last_p1_hp = {}\n",
    "    last_p2_hp = {}\n",
    "    p1_comeback_kos = 0\n",
    "    p2_comeback_kos = 0\n",
    "    p1_inflicted_statuses = Counter()\n",
    "    p2_inflicted_statuses = Counter()\n",
    "    p1_pokemon_statuses = {}\n",
    "    p2_pokemon_statuses = {}\n",
    "    p1_move_type_counts = Counter()\n",
    "    p2_move_type_counts = Counter()\n",
    "    p1_damage_first2 = 0.0\n",
    "    p2_damage_first2 = 0.0\n",
    "\n",
    "    # NEW: per-turn damage accumulation, KO timing and early/late KO counters\n",
    "    p1_dmg_by_turn = {}  # damage inflitto da p1 (contro p2) per turno\n",
    "    p2_dmg_by_turn = {}  # damage inflitto da p2 (contro p1) per turno\n",
    "    seen_turns = set()\n",
    "    first_ko_turn_p1_taken = None   # primo KO subìto da p1 (p1_fainted++)\n",
    "    first_ko_turn_p1_inflicted = None  # primo KO inflitto da p1 (p2_fainted++)\n",
    "    early_threshold = 10\n",
    "    p1_kos_early = p1_kos_late = 0\n",
    "    p2_kos_early = p2_kos_late = 0\n",
    "\n",
    "    for turn in timeline[:30]:\n",
    "        prev_p1_fainted, prev_p2_fainted = p1_fainted, p2_fainted\n",
    "        p1_state = turn.get('p1_pokemon_state',{}) or {}\n",
    "        p2_state = turn.get('p2_pokemon_state',{}) or {}\n",
    "        tnum = turn.get('turn', None)\n",
    "        if tnum is None:\n",
    "            # fallback: usa lunghezza dei turni visti + 1\n",
    "            tnum = (len(seen_turns) + 1)\n",
    "        seen_turns.add(tnum)\n",
    "\n",
    "        if p1_state.get('name'):\n",
    "            p1_last_active = p1_state.get('name')\n",
    "        if p2_state.get('name'):\n",
    "            p2_last_active = p2_state.get('name')\n",
    "\n",
    "        if p1_state.get('fainted') and p1_state.get('name') not in p1_fainted_names:\n",
    "            p1_fainted += 1\n",
    "            p1_fainted_names.add(p1_state.get('name'))\n",
    "            if first_ko_turn_p1_taken is None:\n",
    "                first_ko_turn_p1_taken = tnum\n",
    "            if tnum <= early_threshold: p2_kos_early += 1\n",
    "            else: p2_kos_late += 1\n",
    "        if p2_state.get('fainted') and p2_state.get('name') not in p2_fainted_names:\n",
    "            p2_fainted += 1\n",
    "            p2_fainted_names.add(p2_state.get('name'))\n",
    "            if first_ko_turn_p1_inflicted is None:\n",
    "                first_ko_turn_p1_inflicted = tnum\n",
    "            if tnum <= early_threshold: p1_kos_early += 1\n",
    "            else: p1_kos_late += 1\n",
    "\n",
    "        p2_name, p2_hp = p2_state.get('name'), p2_state.get('hp_pct')\n",
    "        if p2_name and p2_hp is not None:\n",
    "            prev_hp = last_p2_hp.get(p2_name)\n",
    "            if prev_hp is not None:\n",
    "                delta = max(0.0, prev_hp - p2_hp)\n",
    "                p1_damage += delta\n",
    "                p1_dmg_by_turn[tnum] = p1_dmg_by_turn.get(tnum, 0.0) + delta\n",
    "                if turn.get('turn',999) <= 2:\n",
    "                    p1_damage_first2 += delta\n",
    "            last_p2_hp[p2_name] = p2_hp\n",
    "\n",
    "        p1_name, p1_hp = p1_state.get('name'), p1_state.get('hp_pct')\n",
    "        if p1_name and p1_hp is not None:\n",
    "            prev_hp = last_p1_hp.get(p1_name)\n",
    "            if prev_hp is not None:\n",
    "                delta = max(0.0, prev_hp - p1_hp)\n",
    "                p2_damage += delta\n",
    "                p2_dmg_by_turn[tnum] = p2_dmg_by_turn.get(tnum, 0.0) + delta\n",
    "                if turn.get('turn',999) <= 2:\n",
    "                    p2_damage_first2 += delta\n",
    "            last_p1_hp[p1_name] = p1_hp\n",
    "\n",
    "        damage_diff_so_far = p1_damage - p2_damage\n",
    "        if p2_fainted > prev_p2_fainted and damage_diff_so_far < -1.0:\n",
    "            p1_comeback_kos += 1\n",
    "        if p1_fainted > prev_p1_fainted and damage_diff_so_far > 1.0:\n",
    "            p2_comeback_kos += 1\n",
    "\n",
    "        p2_status = p2_state.get('status')\n",
    "        if p2_name and p2_status and p2_pokemon_statuses.get(p2_name) != p2_status:\n",
    "            p1_inflicted_statuses[p2_status] += 1\n",
    "            p2_pokemon_statuses[p2_name] = p2_status\n",
    "        p1_status = p1_state.get('status')\n",
    "        if p1_name and p1_status and p1_pokemon_statuses.get(p1_name) != p1_status:\n",
    "            p2_inflicted_statuses[p1_status] += 1\n",
    "            p1_pokemon_statuses[p1_name] = p1_status\n",
    "\n",
    "        p1_move = turn.get('p1_move_details') or {}\n",
    "        p2_move = turn.get('p2_move_details') or {}\n",
    "        if p1_move and p1_move.get('type'):\n",
    "            p1_move_type_counts[(p1_move.get('type') or '').lower()] += 1\n",
    "        if p2_move and p2_move.get('type'):\n",
    "            p2_move_type_counts[(p2_move.get('type') or '').lower()] += 1\n",
    "        if turn.get('p1_move_details'):\n",
    "            p1_moves += 1\n",
    "        if turn.get('p2_move_details'):\n",
    "            p2_moves += 1\n",
    "        p1_last_hp = p1_state.get('hp_pct', np.nan)\n",
    "        p2_last_hp = p2_state.get('hp_pct', np.nan)\n",
    "\n",
    "    # ...existing code computing out[...] baseline metrics...\n",
    "    out['tl_p1_moves'] = int(p1_moves)\n",
    "    out['tl_p2_moves'] = int(p2_moves)\n",
    "    out['tl_p1_est_damage'] = float(p1_damage)\n",
    "    out['tl_p2_est_damage'] = float(p2_damage)\n",
    "    # NUOVE FEATURE: conteggio KO per squadra e rate normalizzati per turno\n",
    "    out['tl_p1_fainted'] = int(p1_fainted)\n",
    "    out['tl_p2_fainted'] = int(p2_fainted)\n",
    "    turns_count = max(1, len(seen_turns))\n",
    "    out['tl_p1_fainted_rate'] = float(out['tl_p1_fainted'] / turns_count)\n",
    "    out['tl_p2_fainted_rate'] = float(out['tl_p2_fainted'] / turns_count)\n",
    "    # fine nuovi features\n",
    "    out['damage_diff'] = float(p1_damage - p2_damage)\n",
    "    out['fainted_diff'] = int(p1_fainted - p2_fainted)\n",
    "    out['tl_p1_last_hp'] = float(p1_last_hp) if not np.isnan(p1_last_hp) else 0.0\n",
    "    out['tl_p2_last_hp'] = float(p2_last_hp) if not np.isnan(p2_last_hp) else 0.0\n",
    "    out['tl_p1_last_active'] = p1_last_active\n",
    "    out['tl_p2_last_active'] = p2_last_active\n",
    "    if p1_team:\n",
    "        p1_total_hp_sum = sum(p.get('base_hp',0) for p in p1_team)\n",
    "        p1_avg_def = np.mean([p.get('base_def',0) for p in p1_team] or [0])\n",
    "        p1_avg_spd = np.mean([p.get('base_spd',0) for p in p1_team] or [0])\n",
    "        out['tl_p2_damage_vs_p1_hp_pool'] = float(p2_damage / (p1_total_hp_sum + 1e-6))\n",
    "        out['tl_p1_defensive_endurance'] = float((p1_avg_def + p1_avg_spd) / (p2_damage + 1e-6))\n",
    "    out['tl_p1_comeback_kos'] = int(p1_comeback_kos)\n",
    "    out['tl_p2_comeback_kos'] = int(p2_comeback_kos)\n",
    "    out['tl_comeback_kos_diff'] = int(p1_comeback_kos - p2_comeback_kos)\n",
    "\n",
    "    common_statuses = ['brn','par','slp','frz','psn','tox']\n",
    "    for status in common_statuses:\n",
    "        out[f'tl_p1_inflicted_{status}_count'] = int(p1_inflicted_statuses.get(status,0))\n",
    "        out[f'tl_p2_inflicted_{status}_count'] = int(p2_inflicted_statuses.get(status,0))\n",
    "        out[f'tl_inflicted_{status}_diff'] = int(p1_inflicted_statuses.get(status,0) - p2_inflicted_statuses.get(status,0))\n",
    "\n",
    "    common_move_types = ['normal','fire','water','electric','grass','psychic','ice','dragon','rock','ground','flying','ghost','bug','poison','fighting']\n",
    "    for mt in common_move_types:\n",
    "        out[f'tl_p1_move_type_{mt}_count'] = int(p1_move_type_counts.get(mt,0))\n",
    "        out[f'tl_p2_move_type_{mt}_count'] = int(p2_move_type_counts.get(mt,0))\n",
    "        out[f'tl_move_type_{mt}_count_diff'] = int(p1_move_type_counts.get(mt,0) - p2_move_type_counts.get(mt,0))\n",
    "\n",
    "    out['tl_p1_damage_first2'] = float(p1_damage_first2)\n",
    "    out['tl_p2_damage_first2'] = float(p2_damage_first2)\n",
    "    out['tl_first2_damage_diff'] = float(p1_damage_first2 - p2_damage_first2)\n",
    "\n",
    "    # NEW: derived, normalized and late-game features\n",
    "    turns_count = max(1, len(seen_turns))\n",
    "    out['tl_turns_count'] = int(turns_count)\n",
    "    out['tl_p1_moves_rate'] = float(p1_moves / turns_count)\n",
    "    out['tl_p2_moves_rate'] = float(p2_moves / turns_count)\n",
    "    out['tl_p1_damage_per_turn'] = float(p1_damage / turns_count)\n",
    "    out['tl_p2_damage_per_turn'] = float(p2_damage / turns_count)\n",
    "    out['tl_damage_rate_diff'] = float(out['tl_p1_damage_per_turn'] - out['tl_p2_damage_per_turn'])\n",
    "\n",
    "    # last-5-turns damage window\n",
    "    if seen_turns:\n",
    "        recent_turns = sorted(seen_turns)[-5:]\n",
    "        p1_last5 = sum(p1_dmg_by_turn.get(t,0.0) for t in recent_turns)\n",
    "        p2_last5 = sum(p2_dmg_by_turn.get(t,0.0) for t in recent_turns)\n",
    "    else:\n",
    "        p1_last5 = p2_last5 = 0.0\n",
    "    out['tl_p1_damage_last5'] = float(p1_last5)\n",
    "    out['tl_p2_damage_last5'] = float(p2_last5)\n",
    "    out['tl_last5_damage_diff'] = float(p1_last5 - p2_last5)\n",
    "    # NEW: ratio danno ultimi 5 turni vs totale\n",
    "    out['tl_p1_last5_damage_ratio'] = float(p1_last5 / (p1_damage + 1e-6))\n",
    "    out['tl_p2_last5_damage_ratio'] = float(p2_last5 / (p2_damage + 1e-6))\n",
    "    out['tl_last5_damage_ratio_diff'] = float(out['tl_p1_last5_damage_ratio'] - out['tl_p2_last5_damage_ratio'])\n",
    "\n",
    "    # time-weighted damage advantage (peso crescente con il turno)\n",
    "    if seen_turns:\n",
    "        ts = sorted(seen_turns)\n",
    "        w = np.linspace(1.0, 2.0, num=len(ts))  # pesi crescenti\n",
    "        w = w / (w.sum() + 1e-9)\n",
    "        adv = [(p1_dmg_by_turn.get(t,0.0) - p2_dmg_by_turn.get(t,0.0)) for t in ts]\n",
    "        out['tl_weighted_damage_diff'] = float(np.dot(w, adv))\n",
    "    else:\n",
    "        out['tl_weighted_damage_diff'] = 0.0\n",
    "\n",
    "    # NEW: comeback indicator (cambio di segno dell'adv cumulativo)\n",
    "    if seen_turns:\n",
    "        ts = sorted(seen_turns)\n",
    "        cum = 0.0\n",
    "        signs = []\n",
    "        for t in ts:\n",
    "            cum += (p1_dmg_by_turn.get(t,0.0) - p2_dmg_by_turn.get(t,0.0))\n",
    "            s = 1 if cum > 1e-9 else (-1 if cum < -1e-9 else 0)\n",
    "            if s != 0:\n",
    "                if not signs or signs[-1] != s:\n",
    "                    signs.append(s)\n",
    "        sign_flips = max(0, len(signs) - 1)\n",
    "        comeback_flag = 1 if (len(signs) >= 2 and signs[0] != signs[-1]) else 0\n",
    "    else:\n",
    "        sign_flips = 0\n",
    "        comeback_flag = 0\n",
    "    out['tl_damage_adv_sign_flips'] = int(sign_flips)\n",
    "    out['tl_comeback_flag'] = int(comeback_flag)\n",
    "\n",
    "    # KO timing and early/late counts\n",
    "    out['tl_first_ko_turn_p1_inflicted'] = int(first_ko_turn_p1_inflicted or 0)\n",
    "    out['tl_first_ko_turn_p1_taken'] = int(first_ko_turn_p1_taken or 0)\n",
    "    out['tl_first_ko_turn_diff'] = int((first_ko_turn_p1_inflicted or 0) - (first_ko_turn_p1_taken or 0))\n",
    "    out['tl_kos_early_p1'] = int(p1_kos_early)\n",
    "    out['tl_kos_late_p1'] = int(p1_kos_late)\n",
    "    out['tl_kos_early_p2'] = int(p2_kos_early)\n",
    "    out['tl_kos_late_p2'] = int(p2_kos_late)\n",
    "\n",
    "    # normalized status rates per turn\n",
    "    for status in common_statuses:\n",
    "        c1 = p1_inflicted_statuses.get(status,0)\n",
    "        c2 = p2_inflicted_statuses.get(status,0)\n",
    "        out[f'tl_p1_inflicted_{status}_rate'] = float(c1 / turns_count)\n",
    "        out[f'tl_p2_inflicted_{status}_rate'] = float(c2 / turns_count)\n",
    "        out[f'tl_inflicted_{status}_rate_diff'] = float((c1 - c2) / turns_count)\n",
    "\n",
    "    return out\n",
    "\n",
    "def ability_features(team: list, prefix: str) -> dict:\n",
    "    immunity_abilities = {'levitate':0,'volt_absorb':0,'water_absorb':0,'flash_fire':0}\n",
    "    stat_drop_abilities = {'intimidate':0}\n",
    "    weather_abilities = {'drought':0,'drizzle':0,'sand_stream':0}\n",
    "    out = {}\n",
    "    for pokemon in team:\n",
    "        ability = (pokemon.get('ability','') or '').lower().replace(' ','_')\n",
    "        if ability in immunity_abilities:\n",
    "            immunity_abilities[ability] += 1\n",
    "        if ability in stat_drop_abilities:\n",
    "            stat_drop_abilities[ability] += 1\n",
    "        if ability in weather_abilities:\n",
    "            weather_abilities[ability] += 1\n",
    "    for ability,count in immunity_abilities.items():\n",
    "        out[f'{prefix}ability_{ability}_count'] = int(count)\n",
    "    for ability,count in stat_drop_abilities.items():\n",
    "        out[f'{prefix}ability_{ability}_count'] = int(count)\n",
    "    for ability,count in weather_abilities.items():\n",
    "        out[f'{prefix}ability_{ability}_count'] = int(count)\n",
    "    out[f'{prefix}total_immunity_abilities'] = int(sum(immunity_abilities.values()))\n",
    "    out[f'{prefix}total_stat_drop_abilities'] = int(sum(stat_drop_abilities.values()))\n",
    "    return out\n",
    "\n",
    "def prepare_record_features(record: dict, max_turns: int = 30) -> dict:\n",
    "    out = {}\n",
    "    out['battle_id'] = record.get('battle_id')\n",
    "    if 'player_won' in record:\n",
    "        out['player_won'] = int(bool(record.get('player_won')))\n",
    "    p1_team = record.get('p1_team_details', [])\n",
    "    out.update(team_aggregate_features(p1_team, prefix='p1_'))\n",
    "    p2_lead = record.get('p2_lead_details', {})\n",
    "    out.update(lead_aggregate_features(p2_lead, prefix='p2_lead_'))\n",
    "    out.update(ability_features(p1_team, prefix='p1_'))\n",
    "    p1_lead = p1_team[0] if p1_team else {}\n",
    "    out.update(lead_vs_lead_features(p1_lead, p2_lead))\n",
    "    out.update(ability_features([p2_lead], prefix='p2_lead_'))\n",
    "    out['p1_intimidate_vs_lead'] = 1 if out.get('p1_ability_intimidate_count',0) > 0 else 0\n",
    "    tl = record.get('battle_timeline', [])\n",
    "    out.update(summary_from_timeline(tl[:max_turns], p1_team))\n",
    "    out['team_hp_sum_minus_p2lead_hp'] = out.get('p1_base_hp_sum', 0) - out.get('p2_lead_base_hp', 0)\n",
    "    out['team_spa_mean_minus_p2spa'] = out.get('p1_base_spa_mean', 0) - out.get('p2_lead_base_spa', 0)\n",
    "    out['speed_advantage'] = out.get('p1_base_spe_sum', 0) - out.get('p2_lead_base_spe', 0)\n",
    "    out['n_unique_types_diff'] = out.get('p1_n_unique_types', 0) - out.get('p2_lead_n_unique_types', 1)\n",
    "    p1_moves = max(out.get('tl_p1_moves',1),1)\n",
    "    p2_moves = max(out.get('tl_p2_moves',1),1)\n",
    "    out['damage_per_turn_diff'] = (out.get('tl_p1_est_damage',0.0)/p1_moves) - (out.get('tl_p2_est_damage',0.0)/p2_moves)\n",
    "    out['last_pair'] = f\"{out.get('tl_p1_last_active','')}_VS_{out.get('tl_p2_last_active','')}\"\n",
    "    out.update(calculate_type_advantage(p1_team, p2_lead))\n",
    "    p2_lead_bulk = out.get('p2_lead_base_def',1) + out.get('p2_lead_base_spd',1)\n",
    "    out['p1_se_options_vs_lead_bulk'] = out.get('p1_super_effective_options',0) / (p2_lead_bulk + 1e-6)\n",
    "    p2_team = record.get('p2_team_details', [])\n",
    "    if p2_team:\n",
    "        out.update(team_aggregate_features(p2_team, prefix='p2_'))\n",
    "        out['team_hp_sum_diff'] = out.get('p1_base_hp_sum',0) - out.get('p2_base_hp_sum',0)\n",
    "        out['team_spa_mean_diff'] = out.get('p1_base_spa_mean',0) - out.get('p2_base_spa_mean',0)\n",
    "        out['team_spe_mean_diff'] = out.get('p1_base_spe_mean',0) - out.get('p2_base_spe_mean',0)\n",
    "        out['n_unique_types_team_diff'] = out.get('p1_n_unique_types',0) - out.get('p2_n_unique_types',0)\n",
    "    return out\n",
    "\n",
    "def create_features_from_raw(data: list) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    for b in tqdm(data, desc='FE'):\n",
    "        try:\n",
    "            feat = prepare_record_features(b, max_turns=30)\n",
    "            if 'battle_id' not in feat:\n",
    "                feat['battle_id'] = b.get('battle_id')\n",
    "            rows.append(feat)\n",
    "        except Exception as e:\n",
    "            rows.append({'battle_id': b.get('battle_id'), 'error': 1})\n",
    "    df = pd.DataFrame(rows)\n",
    "    if 'player_won' in df.columns:\n",
    "        df['player_won'] = df['player_won'].astype(int)\n",
    "    return df.fillna(0)\n",
    "\n",
    "train_df = create_features_from_raw(train_raw)\n",
    "test_df = create_features_from_raw(test_raw)\n",
    "print('Feature shape train/test:', train_df.shape, test_df.shape)\n",
    "display(train_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6903c13a",
   "metadata": {},
   "source": [
    "# Search best features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "626ecdb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TreeExplainer non funzionante o formato non previsto, fallback a shap.Explainer: could not convert string to float: '[5.06E-1]'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PermutationExplainer explainer: 2001it [04:40,  7.11it/s]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shap_arr.shape: (2000, 229) X_sample.shape: (2000, 229) X_all.shape: (10000, 229)\n",
      "Top-100 features SHAP salvate in top100_shap_features.csv\n",
      "                           feature  shap_mean_abs\n",
      "0          tl_weighted_damage_diff       0.152290\n",
      "1                      tl_p2_moves       0.054426\n",
      "2            tl_inflicted_par_diff       0.050478\n",
      "3        tl_p2_inflicted_slp_count       0.037319\n",
      "4                      tl_p1_moves       0.027659\n",
      "5        tl_p1_inflicted_slp_count       0.024133\n",
      "6                    tl_p1_last_hp       0.023037\n",
      "7                      damage_diff       0.018435\n",
      "8  tl_move_type_psychic_count_diff       0.016813\n",
      "9             tl_last5_damage_diff       0.016260\n"
     ]
    }
   ],
   "source": [
    "# Cell: estrai top-100 features con SHAP (robusta)\n",
    "import shap\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Candidate features: tutte le colonne numeriche tranne id/target\n",
    "candidate_features = [c for c in train_df.columns if c not in ('battle_id','player_won') and train_df[c].dtype != 'object']\n",
    "X_all = train_df[candidate_features].astype(float).fillna(0.0)\n",
    "y_all = train_df['player_won'].astype(int).values\n",
    "\n",
    "# Per velocità, campiona fino a N per training + spiegazioni\n",
    "RND = np.random.RandomState(42)\n",
    "N_SAMPLE = min(2000, len(X_all))\n",
    "sample_idx = RND.choice(len(X_all), size=N_SAMPLE, replace=False)\n",
    "X_sample = X_all.iloc[sample_idx]\n",
    "y_sample = y_all[sample_idx]\n",
    "\n",
    "# Allena un modello leggero (veloce) usato solo per SHAP\n",
    "shap_clf = XGBClassifier(n_estimators=300, max_depth=3, learning_rate=0.05,\n",
    "                         use_label_encoder=False, eval_metric='logloss',\n",
    "                         random_state=42, n_jobs=4, tree_method='hist')\n",
    "shap_clf.fit(X_sample, y_sample)\n",
    "\n",
    "# Calcola SHAP con fallback robusto\n",
    "shap_arr = None\n",
    "explainer = None\n",
    "ev = None\n",
    "\n",
    "def _to_shap_array(shap_vals, n_samples, n_features):\n",
    "    arr = np.array(shap_vals)\n",
    "    if arr.ndim == 3:\n",
    "        # Possibili layout: (n_samples, n_classes, n_features) o (n_classes, n_samples, n_features)\n",
    "        if arr.shape[0] == n_samples and arr.shape[2] == n_features:\n",
    "            class_idx = 1 if arr.shape[1] > 1 else 0\n",
    "            return arr[:, class_idx, :]\n",
    "        if arr.shape[1] == n_samples and arr.shape[2] == n_features:\n",
    "            class_idx = 1 if arr.shape[0] > 1 else 0\n",
    "            return arr[class_idx, :, :]\n",
    "        if arr.shape[0] == n_samples and arr.shape[1] == n_features:\n",
    "            class_idx = 1 if arr.shape[2] > 1 else 0\n",
    "            return arr[:, :, class_idx]\n",
    "        raise RuntimeError(f\"Formato 3D SHAP non riconosciuto: {arr.shape}\")\n",
    "    elif arr.ndim == 2:\n",
    "        # Atteso (n_samples, n_features) oppure trasposto\n",
    "        if arr.shape[0] == n_samples and arr.shape[1] == n_features:\n",
    "            return arr\n",
    "        if arr.shape[1] == n_samples and arr.shape[0] == n_features:\n",
    "            return arr.T\n",
    "        raise RuntimeError(f\"Formato 2D SHAP non riconosciuto: {arr.shape}\")\n",
    "    else:\n",
    "        raise RuntimeError(f\"Formato SHAP inatteso: {arr.shape}\")\n",
    "\n",
    "n_samples = X_sample.shape[0]\n",
    "n_features = X_sample.shape[1]\n",
    "\n",
    "try:\n",
    "    explainer = shap.TreeExplainer(shap_clf)\n",
    "    shap_vals = explainer.shap_values(X_sample)\n",
    "    shap_arr = _to_shap_array(shap_vals, n_samples, n_features)\n",
    "except Exception as e:\n",
    "    print(\"TreeExplainer non funzionante o formato non previsto, fallback a shap.Explainer:\", e)\n",
    "    explainer = shap.Explainer(shap_clf.predict_proba, X_sample)\n",
    "    ev = explainer(X_sample)\n",
    "    vals = ev.values\n",
    "    shap_arr = _to_shap_array(vals, n_samples, n_features)\n",
    "\n",
    "# Debug shapes se serve\n",
    "print(\"shap_arr.shape:\", getattr(shap_arr, \"shape\", None), \"X_sample.shape:\", getattr(X_sample, \"shape\", None), \"X_all.shape:\", getattr(X_all, \"shape\", None))\n",
    "\n",
    "# Allinea nomi feature e calcola importanza\n",
    "shap_imp = np.abs(shap_arr).mean(axis=0)\n",
    "\n",
    "# Recupera nomi feature nell'ordine usato per SHAP\n",
    "feat_names = None\n",
    "try:\n",
    "    if explainer is not None and getattr(explainer, 'feature_names', None) is not None:\n",
    "        feat_names = list(explainer.feature_names)\n",
    "except Exception:\n",
    "    feat_names = None\n",
    "\n",
    "if feat_names is None and ev is not None:\n",
    "    try:\n",
    "        if getattr(ev, 'feature_names', None) is not None:\n",
    "            feat_names = list(ev.feature_names)\n",
    "    except Exception:\n",
    "        feat_names = None\n",
    "\n",
    "if feat_names is None:\n",
    "    try:\n",
    "        feat_names = list(X_sample.columns)\n",
    "    except Exception:\n",
    "        feat_names = None\n",
    "\n",
    "if feat_names is None:\n",
    "    feat_names = list(X_all.columns)\n",
    "\n",
    "if len(shap_imp) != len(feat_names):\n",
    "    raise ValueError(f\"Incoerenza lunghezze: len(shap_imp)={len(shap_imp)}, len(feat_names)={len(feat_names)}. Controlla X_sample/colonne.\")\n",
    "\n",
    "imp_df = pd.DataFrame({'feature': feat_names, 'shap_mean_abs': shap_imp})\n",
    "imp_df = imp_df.sort_values('shap_mean_abs', ascending=False).reset_index(drop=True)\n",
    "\n",
    "TOP_K = min(100, len(imp_df))\n",
    "top100 = imp_df['feature'].iloc[:TOP_K].tolist()\n",
    "\n",
    "imp_df.to_csv('shap_feature_importances_all.csv', index=False)\n",
    "pd.DataFrame({'feature': top100}).to_csv('top100_shap_features.csv', index=False)\n",
    "\n",
    "print(f\"Top-{TOP_K} features SHAP salvate in top100_shap_features.csv\")\n",
    "print(imp_df.head(10))\n",
    "\n",
    "SHAP_TOP100 = top100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf62000",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2cede4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num FEATURES numeriche rilevate (ALL): 229\n",
      "Num FEATURES effettive usate (FEATURES): 229\n",
      "Num TOP100 caricate: 100\n",
      "Preprocessing (no transformers) completato.\n",
      "train_val size: 8000 holdout size: 2000\n",
      "Preprocessed feature count: 229\n"
     ]
    }
   ],
   "source": [
    "# ====== Preprocessing (senza transformer sklearn) =========\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# base exclusions\n",
    "exclude_cols = ['battle_id', 'player_won']\n",
    "string_cols = train_df.select_dtypes(include=['object']).columns.tolist()\n",
    "exclude_cols.extend(string_cols)\n",
    "\n",
    "# tutte le colonne numeriche candidate\n",
    "ALL_NUMERIC_FEATURES = [c for c in train_df.columns if c not in exclude_cols]\n",
    "\n",
    "# flag per usare top features se necessario\n",
    "use_top_features = False\n",
    "\n",
    "# carica TOP100 se presente (comportamento invariato)\n",
    "top100_path = r'top100_shap_features.csv'\n",
    "try:\n",
    "    top100_df = pd.read_csv(top100_path)\n",
    "    TOP100 = [str(x).strip() for x in top100_df['feature'].tolist()]\n",
    "except Exception:\n",
    "    TOP100 = []\n",
    "\n",
    "# --- INIZIO: filtro dalle keep_features_list se richiesto ---\n",
    "features_filter = False  # imposta True per applicare il filtro, False per comportamento attuale\n",
    "keep_list_path = 'keep_features_list.txt'\n",
    "\n",
    "if features_filter:\n",
    "    try:\n",
    "        import os\n",
    "        if os.path.exists(keep_list_path):\n",
    "            keep_df = pd.read_csv(keep_list_path, header=None)\n",
    "            keep_list = [str(x).strip() for x in keep_df.iloc[:, 0].tolist()]\n",
    "            # mantieni solo feature numeriche valide presenti in ALL_NUMERIC_FEATURES\n",
    "            filtered = [f for f in ALL_NUMERIC_FEATURES if f in keep_list]\n",
    "            if filtered:\n",
    "                # sovrascrive FEATURES più avanti: qui memorizziamo in temp\n",
    "                FEATURES_FROM_KEEP = filtered\n",
    "                print(f\"features_filter=ON: trovato {len(filtered)} feature valide in {keep_list_path}\")\n",
    "            else:\n",
    "                FEATURES_FROM_KEEP = None\n",
    "                print(f\"features_filter=ON: nessuna feature di {keep_list_path} presente in ALL_NUMERIC_FEATURES\")\n",
    "        else:\n",
    "            FEATURES_FROM_KEEP = None\n",
    "            print(f\"features_filter=ON ma file {keep_list_path} non trovato. Nessun filtro applicato.\")\n",
    "    except Exception as e:\n",
    "        FEATURES_FROM_KEEP = None\n",
    "        print(\"Errore caricando keep_features_list.txt, nessun filtro applicato:\", e)\n",
    "else:\n",
    "    FEATURES_FROM_KEEP = None\n",
    "# --- FINE: filtro dalle keep_features_list ---\n",
    "\n",
    "if use_top_features and TOP100:\n",
    "    FEATURES = [f for f in TOP100 if f in ALL_NUMERIC_FEATURES]\n",
    "elif features_filter:\n",
    "    FEATURES = FEATURES_FROM_KEEP\n",
    "else:\n",
    "    FEATURES = ALL_NUMERIC_FEATURES\n",
    "\n",
    "print(f'Num FEATURES numeriche rilevate (ALL): {len(ALL_NUMERIC_FEATURES)}')\n",
    "print(f'Num FEATURES effettive usate (FEATURES): {len(FEATURES)}')\n",
    "print(f'Num TOP100 caricate: {len(TOP100)}')\n",
    "\n",
    "# costruisco DataFrame numerico raw\n",
    "num_df = train_df[FEATURES].astype(float).replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "# Imputazione semplice: usiamo la mediana per ogni feature calcolata sul train\n",
    "medians = num_df.median()\n",
    "train_imputed = num_df.fillna(medians)\n",
    "\n",
    "# NON eseguo alcuno scaling: lascio i valori nella loro scala naturale\n",
    "train_preproc_df = train_imputed.copy()\n",
    "\n",
    "# target\n",
    "y = train_df['player_won'].astype(int).values\n",
    "\n",
    "# split holdout (20%) - mantengo comportamento originale\n",
    "X = train_preproc_df.values\n",
    "X_train_val, X_holdout, y_train_val, y_holdout, idx_train_val, idx_holdout = train_test_split(\n",
    "    X, y, train_df.index.values, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "print('Preprocessing (no transformers) completato.')\n",
    "print('train_val size:', X_train_val.shape[0], 'holdout size:', X_holdout.shape[0])\n",
    "print('Preprocessed feature count:', len(FEATURES))\n",
    "\n",
    "# Allinea e imputa test_df usando le mediane del train (coerente con l'imputazione sopra)\n",
    "test_aligned = test_df.reindex(columns=FEATURES, fill_value=np.nan).astype(float).replace([np.inf, -np.inf], np.nan)\n",
    "test_imputed = test_aligned.fillna(medians)\n",
    "test_preproc_df = pd.DataFrame(test_imputed.values, columns=FEATURES, index=test_df.index)\n",
    "\n",
    "# Variabili pronte per le celle successive:\n",
    "# FEATURES, X, y, X_train_val, X_holdout, y_train_val, y_holdout, test_preproc_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0101cad5",
   "metadata": {},
   "source": [
    "# Feature filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "916a0a8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analisi feature: 229 features\n",
      "Feature selection summary salvata in feature_selection_candidates.csv\n",
      "Esempio TOP 30 (to_drop prima):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>missing_pct</th>\n",
       "      <th>unique</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "      <th>mi</th>\n",
       "      <th>max_abs_corr</th>\n",
       "      <th>psi</th>\n",
       "      <th>drop_missing</th>\n",
       "      <th>drop_constant</th>\n",
       "      <th>drop_low_std</th>\n",
       "      <th>drop_low_mi</th>\n",
       "      <th>unstable_psi</th>\n",
       "      <th>high_corr_flag</th>\n",
       "      <th>drop_high_corr</th>\n",
       "      <th>to_drop</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>damage_diff</th>\n",
       "      <td>0.0</td>\n",
       "      <td>5886</td>\n",
       "      <td>1.469337</td>\n",
       "      <td>-5.345115</td>\n",
       "      <td>4.157353</td>\n",
       "      <td>0.188876</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.006958</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tl_damage_rate_diff</th>\n",
       "      <td>0.0</td>\n",
       "      <td>6209</td>\n",
       "      <td>0.048978</td>\n",
       "      <td>-0.178170</td>\n",
       "      <td>0.138578</td>\n",
       "      <td>0.179022</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.006958</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tl_p2_est_damage</th>\n",
       "      <td>0.0</td>\n",
       "      <td>4536</td>\n",
       "      <td>1.159621</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.800000</td>\n",
       "      <td>0.107689</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.060739</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tl_p1_moves</th>\n",
       "      <td>0.0</td>\n",
       "      <td>26</td>\n",
       "      <td>3.073948</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>0.080224</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.003664</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tl_p2_moves_rate</th>\n",
       "      <td>0.0</td>\n",
       "      <td>27</td>\n",
       "      <td>0.102139</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.066995</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.002273</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tl_p1_est_damage</th>\n",
       "      <td>0.0</td>\n",
       "      <td>4343</td>\n",
       "      <td>0.879283</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.817803</td>\n",
       "      <td>0.058031</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.038250</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tl_inflicted_slp_diff</th>\n",
       "      <td>0.0</td>\n",
       "      <td>14</td>\n",
       "      <td>1.328172</td>\n",
       "      <td>-6.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>0.054041</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.493303</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tl_p1_inflicted_slp_rate</th>\n",
       "      <td>0.0</td>\n",
       "      <td>8</td>\n",
       "      <td>0.029157</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.233333</td>\n",
       "      <td>0.047021</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.593849</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tl_inflicted_frz_diff</th>\n",
       "      <td>0.0</td>\n",
       "      <td>6</td>\n",
       "      <td>0.549196</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.042962</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>22.683193</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tl_p2_inflicted_slp_rate</th>\n",
       "      <td>0.0</td>\n",
       "      <td>7</td>\n",
       "      <td>0.029060</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.031291</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.001029</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tl_inflicted_par_rate_diff</th>\n",
       "      <td>0.0</td>\n",
       "      <td>13</td>\n",
       "      <td>0.048442</td>\n",
       "      <td>-0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.030204</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.532303</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tl_p1_inflicted_frz_count</th>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.373515</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.024847</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000450</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tl_p1_inflicted_par_rate</th>\n",
       "      <td>0.0</td>\n",
       "      <td>7</td>\n",
       "      <td>0.034080</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.022462</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.002054</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p1_base_spd_std</th>\n",
       "      <td>0.0</td>\n",
       "      <td>520</td>\n",
       "      <td>3.612545</td>\n",
       "      <td>8.498366</td>\n",
       "      <td>35.939764</td>\n",
       "      <td>0.017822</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.196123</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tl_p2_inflicted_frz_rate</th>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.012360</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.017409</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.573013</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tl_p2_inflicted_par_count</th>\n",
       "      <td>0.0</td>\n",
       "      <td>7</td>\n",
       "      <td>1.028143</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.001687</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tl_first_ko_turn_p1_inflicted</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012194</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p1_base_hp_max</th>\n",
       "      <td>0.0</td>\n",
       "      <td>8</td>\n",
       "      <td>39.924412</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>250.000000</td>\n",
       "      <td>0.012158</td>\n",
       "      <td>0.983239</td>\n",
       "      <td>0.188051</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p1_base_hp_mean</th>\n",
       "      <td>0.0</td>\n",
       "      <td>206</td>\n",
       "      <td>13.405444</td>\n",
       "      <td>63.333333</td>\n",
       "      <td>135.833333</td>\n",
       "      <td>0.010929</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.001941</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p1_intimidate_vs_lead</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010051</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p2_lead_ability_drizzle_count</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009034</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p1_base_spa_mean</th>\n",
       "      <td>0.0</td>\n",
       "      <td>57</td>\n",
       "      <td>7.121009</td>\n",
       "      <td>73.333333</td>\n",
       "      <td>121.666667</td>\n",
       "      <td>0.008551</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.216115</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p2_lead_total_stat_drop_abilities</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007820</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p1_ability_sand_stream_count</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007600</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p1_n_unique_names</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.006702</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p2_lead_ability_drought_count</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006695</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lead_speed_advantage</th>\n",
       "      <td>0.0</td>\n",
       "      <td>39</td>\n",
       "      <td>30.008406</td>\n",
       "      <td>-100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.006595</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.002821</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tl_kos_late_p1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006530</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tl_turns_count</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>0.006254</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p1_base_spa_sum</th>\n",
       "      <td>0.0</td>\n",
       "      <td>57</td>\n",
       "      <td>42.726056</td>\n",
       "      <td>440.000000</td>\n",
       "      <td>730.000000</td>\n",
       "      <td>0.005995</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.216115</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   missing_pct  unique        std         min  \\\n",
       "feature                                                                         \n",
       "damage_diff                                0.0    5886   1.469337   -5.345115   \n",
       "tl_damage_rate_diff                        0.0    6209   0.048978   -0.178170   \n",
       "tl_p2_est_damage                           0.0    4536   1.159621    0.000000   \n",
       "tl_p1_moves                                0.0      26   3.073948    0.000000   \n",
       "tl_p2_moves_rate                           0.0      27   0.102139    0.000000   \n",
       "tl_p1_est_damage                           0.0    4343   0.879283    0.000000   \n",
       "tl_inflicted_slp_diff                      0.0      14   1.328172   -6.000000   \n",
       "tl_p1_inflicted_slp_rate                   0.0       8   0.029157    0.000000   \n",
       "tl_inflicted_frz_diff                      0.0       6   0.549196   -3.000000   \n",
       "tl_p2_inflicted_slp_rate                   0.0       7   0.029060    0.000000   \n",
       "tl_inflicted_par_rate_diff                 0.0      13   0.048442   -0.200000   \n",
       "tl_p1_inflicted_frz_count                  0.0       3   0.373515    0.000000   \n",
       "tl_p1_inflicted_par_rate                   0.0       7   0.034080    0.000000   \n",
       "p1_base_spd_std                            0.0     520   3.612545    8.498366   \n",
       "tl_p2_inflicted_frz_rate                   0.0       4   0.012360    0.000000   \n",
       "tl_p2_inflicted_par_count                  0.0       7   1.028143    0.000000   \n",
       "tl_first_ko_turn_p1_inflicted              0.0       1   0.000000    0.000000   \n",
       "p1_base_hp_max                             0.0       8  39.924412   80.000000   \n",
       "p1_base_hp_mean                            0.0     206  13.405444   63.333333   \n",
       "p1_intimidate_vs_lead                      0.0       1   0.000000    0.000000   \n",
       "p2_lead_ability_drizzle_count              0.0       1   0.000000    0.000000   \n",
       "p1_base_spa_mean                           0.0      57   7.121009   73.333333   \n",
       "p2_lead_total_stat_drop_abilities          0.0       1   0.000000    0.000000   \n",
       "p1_ability_sand_stream_count               0.0       1   0.000000    0.000000   \n",
       "p1_n_unique_names                          0.0       1   0.000000    6.000000   \n",
       "p2_lead_ability_drought_count              0.0       1   0.000000    0.000000   \n",
       "lead_speed_advantage                       0.0      39  30.008406 -100.000000   \n",
       "tl_kos_late_p1                             0.0       1   0.000000    0.000000   \n",
       "tl_turns_count                             0.0       1   0.000000   30.000000   \n",
       "p1_base_spa_sum                            0.0      57  42.726056  440.000000   \n",
       "\n",
       "                                          max        mi  max_abs_corr  \\\n",
       "feature                                                                 \n",
       "damage_diff                          4.157353  0.188876      1.000000   \n",
       "tl_damage_rate_diff                  0.138578  0.179022      1.000000   \n",
       "tl_p2_est_damage                     6.800000  0.107689      1.000000   \n",
       "tl_p1_moves                         30.000000  0.080224      1.000000   \n",
       "tl_p2_moves_rate                     1.000000  0.066995      1.000000   \n",
       "tl_p1_est_damage                     5.817803  0.058031      1.000000   \n",
       "tl_inflicted_slp_diff                7.000000  0.054041      1.000000   \n",
       "tl_p1_inflicted_slp_rate             0.233333  0.047021      1.000000   \n",
       "tl_inflicted_frz_diff                2.000000  0.042962      1.000000   \n",
       "tl_p2_inflicted_slp_rate             0.200000  0.031291      1.000000   \n",
       "tl_inflicted_par_rate_diff           0.200000  0.030204      1.000000   \n",
       "tl_p1_inflicted_frz_count            2.000000  0.024847      1.000000   \n",
       "tl_p1_inflicted_par_rate             0.200000  0.022462      1.000000   \n",
       "p1_base_spd_std                     35.939764  0.017822      1.000000   \n",
       "tl_p2_inflicted_frz_rate             0.100000  0.017409      1.000000   \n",
       "tl_p2_inflicted_par_count            6.000000  0.015625      1.000000   \n",
       "tl_first_ko_turn_p1_inflicted        0.000000  0.012194      0.000000   \n",
       "p1_base_hp_max                     250.000000  0.012158      0.983239   \n",
       "p1_base_hp_mean                    135.833333  0.010929      1.000000   \n",
       "p1_intimidate_vs_lead                0.000000  0.010051      0.000000   \n",
       "p2_lead_ability_drizzle_count        0.000000  0.009034      0.000000   \n",
       "p1_base_spa_mean                   121.666667  0.008551      1.000000   \n",
       "p2_lead_total_stat_drop_abilities    0.000000  0.007820      0.000000   \n",
       "p1_ability_sand_stream_count         0.000000  0.007600      0.000000   \n",
       "p1_n_unique_names                    6.000000  0.006702      0.000000   \n",
       "p2_lead_ability_drought_count        0.000000  0.006695      0.000000   \n",
       "lead_speed_advantage               100.000000  0.006595      1.000000   \n",
       "tl_kos_late_p1                       0.000000  0.006530      0.000000   \n",
       "tl_turns_count                      30.000000  0.006254      0.000000   \n",
       "p1_base_spa_sum                    730.000000  0.005995      1.000000   \n",
       "\n",
       "                                         psi  drop_missing  drop_constant  \\\n",
       "feature                                                                     \n",
       "damage_diff                         0.006958         False          False   \n",
       "tl_damage_rate_diff                 0.006958         False          False   \n",
       "tl_p2_est_damage                    0.060739         False          False   \n",
       "tl_p1_moves                         0.003664         False          False   \n",
       "tl_p2_moves_rate                    0.002273         False          False   \n",
       "tl_p1_est_damage                    0.038250         False          False   \n",
       "tl_inflicted_slp_diff               0.493303         False          False   \n",
       "tl_p1_inflicted_slp_rate            3.593849         False          False   \n",
       "tl_inflicted_frz_diff              22.683193         False          False   \n",
       "tl_p2_inflicted_slp_rate            0.001029         False          False   \n",
       "tl_inflicted_par_rate_diff          0.532303         False          False   \n",
       "tl_p1_inflicted_frz_count           0.000450         False          False   \n",
       "tl_p1_inflicted_par_rate            0.002054         False          False   \n",
       "p1_base_spd_std                     0.196123         False          False   \n",
       "tl_p2_inflicted_frz_rate            3.573013         False          False   \n",
       "tl_p2_inflicted_par_count           0.001687         False          False   \n",
       "tl_first_ko_turn_p1_inflicted       0.000000         False           True   \n",
       "p1_base_hp_max                      0.188051         False          False   \n",
       "p1_base_hp_mean                     0.001941         False          False   \n",
       "p1_intimidate_vs_lead               0.000000         False           True   \n",
       "p2_lead_ability_drizzle_count       0.000000         False           True   \n",
       "p1_base_spa_mean                    0.216115         False          False   \n",
       "p2_lead_total_stat_drop_abilities   0.000000         False           True   \n",
       "p1_ability_sand_stream_count        0.000000         False           True   \n",
       "p1_n_unique_names                   0.000000         False           True   \n",
       "p2_lead_ability_drought_count       0.000000         False           True   \n",
       "lead_speed_advantage                0.002821         False          False   \n",
       "tl_kos_late_p1                      0.000000         False           True   \n",
       "tl_turns_count                      0.000000         False           True   \n",
       "p1_base_spa_sum                     0.216115         False          False   \n",
       "\n",
       "                                   drop_low_std  drop_low_mi  unstable_psi  \\\n",
       "feature                                                                      \n",
       "damage_diff                               False        False         False   \n",
       "tl_damage_rate_diff                       False        False         False   \n",
       "tl_p2_est_damage                          False        False         False   \n",
       "tl_p1_moves                               False        False         False   \n",
       "tl_p2_moves_rate                          False        False         False   \n",
       "tl_p1_est_damage                          False        False         False   \n",
       "tl_inflicted_slp_diff                     False        False         False   \n",
       "tl_p1_inflicted_slp_rate                  False        False          True   \n",
       "tl_inflicted_frz_diff                     False        False          True   \n",
       "tl_p2_inflicted_slp_rate                  False        False         False   \n",
       "tl_inflicted_par_rate_diff                False        False          True   \n",
       "tl_p1_inflicted_frz_count                 False        False         False   \n",
       "tl_p1_inflicted_par_rate                  False        False         False   \n",
       "p1_base_spd_std                           False        False         False   \n",
       "tl_p2_inflicted_frz_rate                  False        False          True   \n",
       "tl_p2_inflicted_par_count                 False        False         False   \n",
       "tl_first_ko_turn_p1_inflicted              True        False         False   \n",
       "p1_base_hp_max                            False        False         False   \n",
       "p1_base_hp_mean                           False        False         False   \n",
       "p1_intimidate_vs_lead                      True        False         False   \n",
       "p2_lead_ability_drizzle_count              True        False         False   \n",
       "p1_base_spa_mean                          False        False         False   \n",
       "p2_lead_total_stat_drop_abilities          True        False         False   \n",
       "p1_ability_sand_stream_count               True        False         False   \n",
       "p1_n_unique_names                          True        False         False   \n",
       "p2_lead_ability_drought_count              True        False         False   \n",
       "lead_speed_advantage                      False        False         False   \n",
       "tl_kos_late_p1                             True        False         False   \n",
       "tl_turns_count                             True        False         False   \n",
       "p1_base_spa_sum                           False        False         False   \n",
       "\n",
       "                                   high_corr_flag drop_high_corr  to_drop  \n",
       "feature                                                                    \n",
       "damage_diff                                  True           True     True  \n",
       "tl_damage_rate_diff                          True           True     True  \n",
       "tl_p2_est_damage                             True           True     True  \n",
       "tl_p1_moves                                  True           True     True  \n",
       "tl_p2_moves_rate                             True           True     True  \n",
       "tl_p1_est_damage                             True           True     True  \n",
       "tl_inflicted_slp_diff                        True           True     True  \n",
       "tl_p1_inflicted_slp_rate                     True           True     True  \n",
       "tl_inflicted_frz_diff                        True           True     True  \n",
       "tl_p2_inflicted_slp_rate                     True           True     True  \n",
       "tl_inflicted_par_rate_diff                   True           True     True  \n",
       "tl_p1_inflicted_frz_count                    True           True     True  \n",
       "tl_p1_inflicted_par_rate                     True           True     True  \n",
       "p1_base_spd_std                              True           True     True  \n",
       "tl_p2_inflicted_frz_rate                     True           True     True  \n",
       "tl_p2_inflicted_par_count                    True           True     True  \n",
       "tl_first_ko_turn_p1_inflicted               False            NaN     True  \n",
       "p1_base_hp_max                               True           True     True  \n",
       "p1_base_hp_mean                              True           True     True  \n",
       "p1_intimidate_vs_lead                       False            NaN     True  \n",
       "p2_lead_ability_drizzle_count               False            NaN     True  \n",
       "p1_base_spa_mean                             True           True     True  \n",
       "p2_lead_total_stat_drop_abilities           False            NaN     True  \n",
       "p1_ability_sand_stream_count                False            NaN     True  \n",
       "p1_n_unique_names                           False            NaN     True  \n",
       "p2_lead_ability_drought_count               False            NaN     True  \n",
       "lead_speed_advantage                         True           True     True  \n",
       "tl_kos_late_p1                              False            NaN     True  \n",
       "tl_turns_count                              False            NaN     True  \n",
       "p1_base_spa_sum                              True           True     True  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEEP: 103  DROP: 126\n",
      "Auto-salvato 'keep_features_list.txt' con 103 feature (una per riga).\n"
     ]
    }
   ],
   "source": [
    "# --- NEW CELL: feature selection candidates (statistics, MI, corr, PSI) ---\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "\n",
    "def feature_basic_stats(df, features):\n",
    "    rows = []\n",
    "    for f in features:\n",
    "        col = df[f]\n",
    "        rows.append({\n",
    "            'feature': f,\n",
    "            'missing_pct': float(col.isna().mean()),\n",
    "            'unique': int(col.nunique(dropna=True)),\n",
    "            'std': float(col.std()),\n",
    "            'min': float(col.min()) if np.isfinite(col.min()) else np.nan,\n",
    "            'max': float(col.max()) if np.isfinite(col.max()) else np.nan\n",
    "        })\n",
    "    return pd.DataFrame(rows).set_index('feature')\n",
    "\n",
    "def psi(expected, actual, buckets=10):\n",
    "    # simple PSI by quantile buckets\n",
    "    def _hist(arr):\n",
    "        try:\n",
    "            labels = pd.qcut(arr.rank(method='first'), buckets, labels=False)\n",
    "        except Exception:\n",
    "            labels = pd.cut(arr, buckets, labels=False, duplicates='drop')\n",
    "            labels = np.nan_to_num(labels, nan=0).astype(int)\n",
    "        cnt = np.bincount(labels, minlength=buckets).astype(float)\n",
    "        return cnt / max(1.0, cnt.sum())\n",
    "    e = _hist(expected.fillna(0).values)\n",
    "    a = _hist(actual.fillna(0).values)\n",
    "    eps = 1e-6\n",
    "    return float(np.sum((e - a) * np.log((e + eps) / (a + eps))))\n",
    "\n",
    "# run only if FEATURES is defined\n",
    "try:\n",
    "    feats = list(FEATURES)\n",
    "except NameError:\n",
    "    feats = [c for c in train_df.columns if c not in ('battle_id','player_won') and train_df[c].dtype != 'object']\n",
    "\n",
    "print(f\"Analisi feature: {len(feats)} features\")\n",
    "\n",
    "# basic stats\n",
    "stats_df = feature_basic_stats(train_df, feats)\n",
    "\n",
    "# mutual information\n",
    "X = train_df[feats].fillna(0).astype(float)\n",
    "y = train_df['player_won'].astype(int).values\n",
    "mi = mutual_info_classif(X, y, discrete_features=False, random_state=42)\n",
    "mi_ser = pd.Series(mi, index=feats, name='mi')\n",
    "\n",
    "# max pairwise correlation (train)\n",
    "corr = X.corr().abs()\n",
    "max_corr = corr.where(~np.eye(len(corr),dtype=bool)).max(axis=0).fillna(0)\n",
    "max_corr.name = 'max_abs_corr'\n",
    "\n",
    "# PSI vs test (if test_df present)\n",
    "psilist = []\n",
    "for f in feats:\n",
    "    if f in test_df.columns:\n",
    "        psilist.append(psi(train_df[f].fillna(0), test_df[f].fillna(0), buckets=10))\n",
    "    else:\n",
    "        psilist.append(np.nan)\n",
    "psi_ser = pd.Series(psilist, index=feats, name='psi')\n",
    "\n",
    "# assemble summary\n",
    "summary = stats_df.join(mi_ser).join(max_corr).join(psi_ser)\n",
    "summary['drop_missing'] = summary['missing_pct'] > 0.5\n",
    "summary['drop_constant'] = summary['unique'] <= 1\n",
    "summary['drop_low_std'] = summary['std'].abs() < 1e-8\n",
    "mi_thr = summary['mi'].quantile(0.05) if summary['mi'].notna().any() else 0.0\n",
    "summary['drop_low_mi'] = summary['mi'] <= mi_thr\n",
    "summary['unstable_psi'] = summary['psi'] > 0.5\n",
    "summary['high_corr_flag'] = summary['max_abs_corr'] > 0.95\n",
    "\n",
    "# decide drops: combine rules but keep decision explainable\n",
    "drop_reasons = []\n",
    "drop_set = set()\n",
    "for f, row in summary.iterrows():\n",
    "    reasons = []\n",
    "    if row['drop_missing']:\n",
    "        reasons.append('missing>0.5')\n",
    "    if row['drop_constant']:\n",
    "        reasons.append('constant')\n",
    "    if row['drop_low_std']:\n",
    "        reasons.append('std~0')\n",
    "    if row['drop_low_mi']:\n",
    "        reasons.append('low_mi')\n",
    "    # high corr: mark but decide to drop the one with lower MI within correlated pairs later\n",
    "    if reasons:\n",
    "        drop_reasons.append((f, ';'.join(reasons)))\n",
    "        drop_set.add(f)\n",
    "\n",
    "# resolve high-corr groups (keep feature with higher MI)\n",
    "if summary['high_corr_flag'].any():\n",
    "    cor_mat = corr\n",
    "    visited = set()\n",
    "    for i, fi in enumerate(feats):\n",
    "        if fi in visited:\n",
    "            continue\n",
    "        # find strongly correlated partners\n",
    "        partners = [fj for fj in feats if (fj != fi and cor_mat.at[fi,fj] > 0.95)]\n",
    "        if partners:\n",
    "            group = [fi] + partners\n",
    "            visited.update(group)\n",
    "            # pick best by MI\n",
    "            group_mi = summary.loc[group, 'mi'].fillna(-1.0)\n",
    "            keep = group_mi.idxmax()\n",
    "            for g in group:\n",
    "                if g != keep:\n",
    "                    drop_set.add(g)\n",
    "                    summary.at[g, 'drop_high_corr'] = True\n",
    "            summary.at[keep, 'drop_high_corr'] = False\n",
    "\n",
    "summary['to_drop'] = summary.index.isin(drop_set)\n",
    "summary = summary.sort_values(['to_drop','mi'], ascending=[False, False])\n",
    "\n",
    "# save results\n",
    "out_path = 'feature_selection_candidates.csv'\n",
    "summary.reset_index().to_csv(out_path, index=False)\n",
    "print(f\"Feature selection summary salvata in {out_path}\")\n",
    "print(\"Esempio TOP 30 (to_drop prima):\")\n",
    "display(summary.head(30))\n",
    "\n",
    "# convenience lists\n",
    "KEEP = [c for c in summary.index.tolist() if not summary.at[c,'to_drop']]\n",
    "DROP = [c for c in summary.index.tolist() if summary.at[c,'to_drop']]\n",
    "print(f\"KEEP: {len(KEEP)}  DROP: {len(DROP)}\")\n",
    "# ... you can later modify thresholds and re-run this cell to update candidates ...\n",
    "\n",
    "# --- INIZIO: scrivi keep_features_list.txt dalle feature KEEP generate qui ---\n",
    "keep_path = 'keep_features_list.txt'\n",
    "seen = set()\n",
    "cleaned_keep = []\n",
    "for f in KEEP:\n",
    "    ff = str(f).strip()\n",
    "    if not ff or ff in seen:\n",
    "        continue\n",
    "    seen.add(ff)\n",
    "    cleaned_keep.append(ff)\n",
    "\n",
    "if cleaned_keep:\n",
    "    with open(keep_path, 'w', encoding='utf-8') as fh:\n",
    "        for feat in cleaned_keep:\n",
    "            fh.write(feat + '\\n')\n",
    "    print(f\"Auto-salvato '{keep_path}' con {len(cleaned_keep)} feature (una per riga).\")\n",
    "else:\n",
    "    # se KEEP è vuoto, rimuovere file esistente per evitare confusione\n",
    "    try:\n",
    "        import os\n",
    "        if os.path.exists(keep_path):\n",
    "            os.remove(keep_path)\n",
    "            print(f\"KEEP vuoto: rimosso eventuale '{keep_path}' esistente.\")\n",
    "    except Exception:\n",
    "        pass\n",
    "# --- FINE: scrivi keep_features_list.txt ---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb05f0e",
   "metadata": {},
   "source": [
    "# Hyperparameter serch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "38364108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Time-boxed GridSearchCV (<= ~2 ore) ===\n",
      "scale_pos_weight auto≈1.00 -> grid=[1.0, 1.25]\n",
      "Warmup per stimare t_fit...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 53\u001b[39m\n\u001b[32m     43\u001b[39m t0 = time.time()\n\u001b[32m     44\u001b[39m gs_warm = GridSearchCV(\n\u001b[32m     45\u001b[39m     base_clf,\n\u001b[32m     46\u001b[39m     param_grid=[{k:[v] \u001b[38;5;28;01mfor\u001b[39;00m k,v \u001b[38;5;129;01min\u001b[39;00m d.items()} \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m warm_params],\n\u001b[32m   (...)\u001b[39m\u001b[32m     51\u001b[39m     refit=\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m     52\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m \u001b[43mgs_warm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_val\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     54\u001b[39m elapsed_warm = time.time() - t0\n\u001b[32m     55\u001b[39m fits_warm = \u001b[38;5;28mlen\u001b[39m(warm_params) * cv_inner.get_n_splits()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1365\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1358\u001b[39m     estimator._validate_params()\n\u001b[32m   1360\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1361\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1362\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1363\u001b[39m     )\n\u001b[32m   1364\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1365\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\model_selection\\_search.py:1051\u001b[39m, in \u001b[36mBaseSearchCV.fit\u001b[39m\u001b[34m(self, X, y, **params)\u001b[39m\n\u001b[32m   1045\u001b[39m     results = \u001b[38;5;28mself\u001b[39m._format_results(\n\u001b[32m   1046\u001b[39m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[32m   1047\u001b[39m     )\n\u001b[32m   1049\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[32m-> \u001b[39m\u001b[32m1051\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1053\u001b[39m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[32m   1054\u001b[39m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[32m   1055\u001b[39m first_test_score = all_out[\u001b[32m0\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mtest_scores\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\model_selection\\_search.py:1605\u001b[39m, in \u001b[36mGridSearchCV._run_search\u001b[39m\u001b[34m(self, evaluate_candidates)\u001b[39m\n\u001b[32m   1603\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[32m   1604\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1605\u001b[39m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\model_selection\\_search.py:997\u001b[39m, in \u001b[36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[39m\u001b[34m(candidate_params, cv, more_results)\u001b[39m\n\u001b[32m    989\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.verbose > \u001b[32m0\u001b[39m:\n\u001b[32m    990\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[32m    991\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[33m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[33m candidates,\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    992\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m fits\u001b[39m\u001b[33m\"\u001b[39m.format(\n\u001b[32m    993\u001b[39m             n_splits, n_candidates, n_candidates * n_splits\n\u001b[32m    994\u001b[39m         )\n\u001b[32m    995\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m997\u001b[39m out = \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    998\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    999\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1000\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1001\u001b[39m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1002\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1003\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1004\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1005\u001b[39m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1006\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1007\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1008\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1009\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1010\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1011\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mrouted_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplitter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1012\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1013\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1015\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) < \u001b[32m1\u001b[39m:\n\u001b[32m   1016\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1017\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mNo fits were performed. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1018\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mWas the CV iterator empty? \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1019\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mWere there no candidates?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1020\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\parallel.py:82\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m     73\u001b[39m warning_filters = warnings.filters\n\u001b[32m     74\u001b[39m iterable_with_config_and_warning_filters = (\n\u001b[32m     75\u001b[39m     (\n\u001b[32m     76\u001b[39m         _with_config_and_warning_filters(delayed_func, config, warning_filters),\n\u001b[32m   (...)\u001b[39m\u001b[32m     80\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[32m     81\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config_and_warning_filters\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\joblib\\parallel.py:2072\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   2066\u001b[39m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[32m   2067\u001b[39m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[32m   2068\u001b[39m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[32m   2069\u001b[39m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[32m   2070\u001b[39m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[32m-> \u001b[39m\u001b[32m2072\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\joblib\\parallel.py:1682\u001b[39m, in \u001b[36mParallel._get_outputs\u001b[39m\u001b[34m(self, iterator, pre_dispatch)\u001b[39m\n\u001b[32m   1679\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[32m   1681\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backend.retrieval_context():\n\u001b[32m-> \u001b[39m\u001b[32m1682\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m._retrieve()\n\u001b[32m   1684\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[32m   1685\u001b[39m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[32m   1686\u001b[39m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[32m   1687\u001b[39m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[32m   1688\u001b[39m     \u001b[38;5;28mself\u001b[39m._exception = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\joblib\\parallel.py:1800\u001b[39m, in \u001b[36mParallel._retrieve\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1789\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_ordered:\n\u001b[32m   1790\u001b[39m     \u001b[38;5;66;03m# Case ordered: wait for completion (or error) of the next job\u001b[39;00m\n\u001b[32m   1791\u001b[39m     \u001b[38;5;66;03m# that have been dispatched and not retrieved yet. If no job\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1795\u001b[39m     \u001b[38;5;66;03m# control only have to be done on the amount of time the next\u001b[39;00m\n\u001b[32m   1796\u001b[39m     \u001b[38;5;66;03m# dispatched job is pending.\u001b[39;00m\n\u001b[32m   1797\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (nb_jobs == \u001b[32m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   1798\u001b[39m         \u001b[38;5;28mself\u001b[39m._jobs[\u001b[32m0\u001b[39m].get_status(timeout=\u001b[38;5;28mself\u001b[39m.timeout) == TASK_PENDING\n\u001b[32m   1799\u001b[39m     ):\n\u001b[32m-> \u001b[39m\u001b[32m1800\u001b[39m         time.sleep(\u001b[32m0.01\u001b[39m)\n\u001b[32m   1801\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m   1803\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m nb_jobs == \u001b[32m0\u001b[39m:\n\u001b[32m   1804\u001b[39m     \u001b[38;5;66;03m# Case unordered: jobs are added to the list of jobs to\u001b[39;00m\n\u001b[32m   1805\u001b[39m     \u001b[38;5;66;03m# retrieve `self._jobs` only once completed or in error, which\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1811\u001b[39m     \u001b[38;5;66;03m# timeouts before any other dispatched job has completed and\u001b[39;00m\n\u001b[32m   1812\u001b[39m     \u001b[38;5;66;03m# been added to `self._jobs` to be retrieved.\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# === Grid Search time-boxed (<= ~2 ore) ===\n",
    "print(\"=== Time-boxed GridSearchCV (<= ~2 ore) ===\")\n",
    "import time, os\n",
    "import numpy as np, pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV, ParameterGrid\n",
    "try:\n",
    "    import joblib\n",
    "    CPU_COUNT = joblib.cpu_count()\n",
    "except Exception:\n",
    "    CPU_COUNT = os.cpu_count() or 4\n",
    "\n",
    "cv_inner = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Base estimator\n",
    "base_clf = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42, n_jobs=1, tree_method='hist')\n",
    "\n",
    "# Calcola uno scale_pos_weight automatico e cerca attorno ad esso\n",
    "pos_rate = float(y_train_val.mean())\n",
    "spw_auto = float((1.0 - pos_rate) / max(pos_rate, 1e-9))\n",
    "spw_grid = sorted({1.0, max(1.0, spw_auto*0.75), max(1.0, spw_auto), max(1.0, spw_auto*1.25)})\n",
    "print(f'scale_pos_weight auto≈{spw_auto:.2f} -> grid={spw_grid}')\n",
    "\n",
    "# Griglia COARSE (regolarizzata) — include scale_pos_weight\n",
    "grid_coarse = {\n",
    "    'n_estimators':      [300, 500, 700],\n",
    "    'max_depth':         [3, 4],\n",
    "    'min_child_weight':  [3, 5, 7],\n",
    "    'learning_rate':     [0.03, 0.05, 0.07],\n",
    "    'subsample':         [0.7, 0.8, 0.9],\n",
    "    'colsample_bytree':  [0.7, 0.8, 0.9],\n",
    "    'gamma':             [0.1, 0.2, 0.3],\n",
    "    'reg_alpha':         [0.05, 0.1, 0.2],\n",
    "    'reg_lambda':        [2.0, 3.0, 4.0],\n",
    "    'scale_pos_weight':  spw_grid\n",
    "}\n",
    "\n",
    "# Stima tempo per-fit (warmup) — CORRETTO param_grid\n",
    "warm_params = [\n",
    "    {'n_estimators': 500, 'max_depth': 3, 'min_child_weight': 5, 'learning_rate': 0.05, 'subsample': 0.8, 'colsample_bytree': 0.8, 'gamma': 0.2, 'reg_alpha': 0.1, 'reg_lambda': 3.0, 'scale_pos_weight': max(1.0, spw_auto)},\n",
    "    {'n_estimators': 700, 'max_depth': 4, 'min_child_weight': 5, 'learning_rate': 0.03, 'subsample': 0.8, 'colsample_bytree': 0.8, 'gamma': 0.2, 'reg_alpha': 0.1, 'reg_lambda': 3.0, 'scale_pos_weight': max(1.0, spw_auto*1.25)}\n",
    "]\n",
    "print(\"Warmup per stimare t_fit...\")\n",
    "t0 = time.time()\n",
    "gs_warm = GridSearchCV(\n",
    "    base_clf,\n",
    "    param_grid=[{k:[v] for k,v in d.items()} for d in warm_params],\n",
    "    scoring='balanced_accuracy',\n",
    "    cv=cv_inner,\n",
    "    n_jobs=-1,\n",
    "    verbose=0,\n",
    "    refit=False\n",
    ")\n",
    "gs_warm.fit(X_train_val, y_train_val)\n",
    "elapsed_warm = time.time() - t0\n",
    "fits_warm = len(warm_params) * cv_inner.get_n_splits()\n",
    "t_fit_per_fold = max(0.01, elapsed_warm / fits_warm)\n",
    "print(f\"Warmup: {elapsed_warm:.2f}s per {fits_warm} fit -> ~{t_fit_per_fold:.3f}s/fit\")\n",
    "\n",
    "# Budget totale ~2 ore con margine sicurezza\n",
    "TARGET_SECONDS = int(2*3600*0.9)\n",
    "speedup = max(1, min(CPU_COUNT, cv_inner.get_n_splits()))\n",
    "max_combos = int((TARGET_SECONDS * speedup) / (t_fit_per_fold * cv_inner.get_n_splits()))\n",
    "max_combos = int(max(48, min(max_combos, 2000)))\n",
    "print(f\"CPU={CPU_COUNT}, speedup~{speedup}, max_combos≈{max_combos}\")\n",
    "\n",
    "# Costruisci tutte le combinazioni e campiona fino a max_combos\n",
    "all_points = list(ParameterGrid(grid_coarse))\n",
    "total = len(all_points)\n",
    "print(f\"Candidate totali nella griglia: {total}\")\n",
    "rng = np.random.default_rng(42)\n",
    "if total > max_combos:\n",
    "    idx = rng.choice(total, size=max_combos, replace=False)\n",
    "    sampled = [all_points[i] for i in idx]\n",
    "else:\n",
    "    sampled = all_points\n",
    "print(f\"Config selezionate: {len(sampled)}\")\n",
    "\n",
    "# Converte in lista di 'micro-grid' (1 punto ciascuno) — CORRETTO\n",
    "param_grid_list = [{k:[v] for k,v in pt.items()} for pt in sampled]\n",
    "\n",
    "print(\"Esecuzione GridSearch time-boxed...\")\n",
    "t1 = time.time()\n",
    "gs = GridSearchCV(\n",
    "    estimator=base_clf,\n",
    "    param_grid=param_grid_list,\n",
    "    scoring='balanced_accuracy',\n",
    "    cv=cv_inner,\n",
    "    n_jobs=-1,\n",
    "    verbose=2,\n",
    "    refit=True\n",
    ")\n",
    "gs.fit(X_train_val, y_train_val)\n",
    "elapsed = time.time() - t1\n",
    "\n",
    "results_df = pd.DataFrame(gs.cv_results_).sort_values('rank_test_score')\n",
    "csv_path = 'hp_search_results_timeboxed_grid.csv'\n",
    "results_df.to_csv(csv_path, index=False)\n",
    "best_params = gs.best_params_\n",
    "\n",
    "print(f\"\\n✅ Salvato {csv_path} ({len(results_df)} righe)\")\n",
    "print(\"Migliori iperparametri:\")\n",
    "for k, v in best_params.items():\n",
    "    print(f\"  {k}: {v}\")\n",
    "print(f\"Best CV (balanced_accuracy): {gs.best_score_:.4f}\")\n",
    "print(f\"Tempo GridSearch: {elapsed/60:.1f} min (budget ~{TARGET_SECONDS/60:.0f} min)\")\n",
    "print(\"Ora puoi usare 'best_params' nelle celle successive.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "dcbd7e33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-06 23:49:19,098] A new study created in memory with name: no-name-25fbef7b-92e9-4e7a-b9f1-c6ceaf31b544\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "OPTUNA FINE-TUNING - RICERCA FOCALIZZATA ATTORNO A BASELINE\n",
      "======================================================================\n",
      "\n",
      "Avvio 50 trial (1° trial = baseline, poi TPE)...\n",
      "\n",
      "⚠️ Warning fit: XGBClassifier.fit() got an unexpected keyword argument 'callbacks'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-06 23:49:20,785] Trial 0 finished with value: 0.827 and parameters: {'learning_rate': 0.035, 'max_depth': 3, 'n_estimators': 900, 'max_bin': 256, 'min_child_weight': 9, 'gamma': 0.5, 'subsample': 0.7, 'colsample_bytree': 0.7, 'colsample_bynode': 0.7, 'colsample_bylevel': 0.8, 'reg_alpha': 0.4, 'reg_lambda': 10.0, 'max_delta_step': 1}. Best is trial 0 with value: 0.827.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Warning fit: XGBClassifier.fit() got an unexpected keyword argument 'callbacks'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-06 23:49:30,632] Trial 1 finished with value: 0.8265 and parameters: {'learning_rate': 0.03476852692609299, 'max_depth': 3, 'n_estimators': 899, 'max_bin': 248, 'min_child_weight': 9, 'gamma': 0.5005488255307521, 'subsample': 0.7189071324479795, 'colsample_bytree': 0.6878950618483477, 'colsample_bynode': 0.6978577205688217, 'colsample_bylevel': 0.8066910394876151, 'reg_alpha': 0.3879782367216648, 'reg_lambda': 10.101285805370015, 'max_delta_step': 1}. Best is trial 0 with value: 0.827.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Warning fit: XGBClassifier.fit() got an unexpected keyword argument 'callbacks'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-06 23:49:32,972] Trial 2 finished with value: 0.824 and parameters: {'learning_rate': 0.058588342660541505, 'max_depth': 2, 'n_estimators': 1075, 'max_bin': 433, 'min_child_weight': 5, 'gamma': 0.26432776350546017, 'subsample': 0.5201849961848604, 'colsample_bytree': 0.864766689287175, 'colsample_bynode': 0.8981761930224601, 'colsample_bylevel': 0.6412641535530137, 'reg_alpha': 0.5487283159585304, 'reg_lambda': 5.491624100942104, 'max_delta_step': 2}. Best is trial 0 with value: 0.827.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Warning fit: XGBClassifier.fit() got an unexpected keyword argument 'callbacks'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-06 23:49:35,428] Trial 3 finished with value: 0.823 and parameters: {'learning_rate': 0.010529967743918466, 'max_depth': 5, 'n_estimators': 731, 'max_bin': 130, 'min_child_weight': 15, 'gamma': 0.7110475118678332, 'subsample': 0.8422337139158931, 'colsample_bytree': 0.5426086911496627, 'colsample_bynode': 0.5540950302579799, 'colsample_bylevel': 0.9985642153294361, 'reg_alpha': 0.20525612628712334, 'reg_lambda': 14.65337946065455, 'max_delta_step': 5}. Best is trial 0 with value: 0.827.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Warning fit: XGBClassifier.fit() got an unexpected keyword argument 'callbacks'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-06 23:49:37,051] Trial 4 finished with value: 0.8225 and parameters: {'learning_rate': 0.03313607799416753, 'max_depth': 1, 'n_estimators': 896, 'max_bin': 305, 'min_child_weight': 10, 'gamma': 0.48616098083360076, 'subsample': 0.6336701687776085, 'colsample_bytree': 0.7208673405398668, 'colsample_bynode': 0.727684455145088, 'colsample_bylevel': 0.787308666528242, 'reg_alpha': 0.3799406054337648, 'reg_lambda': 10.302316000991786, 'max_delta_step': 0}. Best is trial 0 with value: 0.827.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Warning fit: XGBClassifier.fit() got an unexpected keyword argument 'callbacks'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-06 23:49:41,257] Trial 5 finished with value: 0.8215 and parameters: {'learning_rate': 0.05132173996798651, 'max_depth': 5, 'n_estimators': 1033, 'max_bin': 446, 'min_child_weight': 7, 'gamma': 0.5002896690347844, 'subsample': 0.7480549387959301, 'colsample_bytree': 0.6931447349379222, 'colsample_bynode': 0.7065206662627888, 'colsample_bylevel': 0.8238463572387134, 'reg_alpha': 0.5103299544198501, 'reg_lambda': 7.132833356311223, 'max_delta_step': 4}. Best is trial 0 with value: 0.827.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Warning fit: XGBClassifier.fit() got an unexpected keyword argument 'callbacks'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-06 23:49:43,299] Trial 6 finished with value: 0.8245 and parameters: {'learning_rate': 0.015584758086674678, 'max_depth': 3, 'n_estimators': 772, 'max_bin': 198, 'min_child_weight': 14, 'gamma': 0.6580562009798354, 'subsample': 0.8961858197838287, 'colsample_bytree': 0.5560013993607453, 'colsample_bynode': 0.5577440224186132, 'colsample_bylevel': 0.6758907709476152, 'reg_alpha': 0.28708691675932585, 'reg_lambda': 13.190673762345009, 'max_delta_step': 3}. Best is trial 0 with value: 0.827.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Warning fit: XGBClassifier.fit() got an unexpected keyword argument 'callbacks'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-06 23:49:46,702] Trial 7 finished with value: 0.819 and parameters: {'learning_rate': 0.042644233465294355, 'max_depth': 4, 'n_estimators': 990, 'max_bin': 358, 'min_child_weight': 12, 'gamma': 0.3506818104614414, 'subsample': 0.6042708667303991, 'colsample_bytree': 0.8136965453522745, 'colsample_bynode': 0.8239398842218817, 'colsample_bylevel': 0.9352317373142313, 'reg_alpha': 0.4615535851701884, 'reg_lambda': 7.843601868522235, 'max_delta_step': 0}. Best is trial 0 with value: 0.827.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Warning fit: XGBClassifier.fit() got an unexpected keyword argument 'callbacks'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-06 23:49:48,550] Trial 8 finished with value: 0.8195 and parameters: {'learning_rate': 0.025276362353724567, 'max_depth': 1, 'n_estimators': 827, 'max_bin': 511, 'min_child_weight': 8, 'gamma': 0.5990483200027006, 'subsample': 0.8070415171190455, 'colsample_bytree': 0.6176614901833262, 'colsample_bynode': 0.6344984789779685, 'colsample_bylevel': 0.7267626635152488, 'reg_alpha': 0.3193212198876563, 'reg_lambda': 12.013996257570197, 'max_delta_step': 2}. Best is trial 0 with value: 0.827.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Warning fit: XGBClassifier.fit() got an unexpected keyword argument 'callbacks'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-06 23:49:51,689] Trial 9 finished with value: 0.828 and parameters: {'learning_rate': 0.023323950745284924, 'max_depth': 4, 'n_estimators': 960, 'max_bin': 308, 'min_child_weight': 12, 'gamma': 0.3989528930661683, 'subsample': 0.6367000926452869, 'colsample_bytree': 0.7582574702948834, 'colsample_bynode': 0.7915855181058915, 'colsample_bylevel': 0.8860341794131611, 'reg_alpha': 0.44237266779130013, 'reg_lambda': 8.334320598165451, 'max_delta_step': 1}. Best is trial 9 with value: 0.828.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Warning fit: XGBClassifier.fit() got an unexpected keyword argument 'callbacks'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-06 23:49:54,829] Trial 10 finished with value: 0.828 and parameters: {'learning_rate': 0.022599069527023553, 'max_depth': 4, 'n_estimators': 975, 'max_bin': 358, 'min_child_weight': 12, 'gamma': 0.38705185623784305, 'subsample': 0.5030794686269633, 'colsample_bytree': 0.7900685645280803, 'colsample_bynode': 0.8113526169287668, 'colsample_bylevel': 0.9111376006923405, 'reg_alpha': 0.599765397635074, 'reg_lambda': 7.6577901498944865, 'max_delta_step': 3}. Best is trial 9 with value: 0.828.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Warning fit: XGBClassifier.fit() got an unexpected keyword argument 'callbacks'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-06 23:49:57,978] Trial 11 finished with value: 0.824 and parameters: {'learning_rate': 0.021910031928754587, 'max_depth': 4, 'n_estimators': 981, 'max_bin': 363, 'min_child_weight': 12, 'gamma': 0.34740273713640746, 'subsample': 0.500997484782991, 'colsample_bytree': 0.7820963773721878, 'colsample_bynode': 0.8199113620908142, 'colsample_bylevel': 0.9022868695194126, 'reg_alpha': 0.5995977729369859, 'reg_lambda': 7.927135841069047, 'max_delta_step': 3}. Best is trial 9 with value: 0.828.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Warning fit: XGBClassifier.fit() got an unexpected keyword argument 'callbacks'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-06 23:50:01,314] Trial 12 finished with value: 0.824 and parameters: {'learning_rate': 0.02350172271317825, 'max_depth': 4, 'n_estimators': 980, 'max_bin': 330, 'min_child_weight': 12, 'gamma': 0.3927624331817231, 'subsample': 0.5943926275573392, 'colsample_bytree': 0.7866620526137165, 'colsample_bynode': 0.8048946358360277, 'colsample_bylevel': 0.8922468402573281, 'reg_alpha': 0.47574104233157494, 'reg_lambda': 5.992997180875783, 'max_delta_step': 4}. Best is trial 9 with value: 0.828.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Warning fit: XGBClassifier.fit() got an unexpected keyword argument 'callbacks'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-06 23:50:04,737] Trial 13 finished with value: 0.825 and parameters: {'learning_rate': 0.01764099062802981, 'max_depth': 4, 'n_estimators': 948, 'max_bin': 412, 'min_child_weight': 13, 'gamma': 0.41799289753679125, 'subsample': 0.5548709527101245, 'colsample_bytree': 0.8899371640050817, 'colsample_bynode': 0.8976421601380132, 'colsample_bylevel': 0.9708876675717859, 'reg_alpha': 0.5820797955818061, 'reg_lambda': 8.90679628710091, 'max_delta_step': 1}. Best is trial 9 with value: 0.828.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Warning fit: XGBClassifier.fit() got an unexpected keyword argument 'callbacks'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-06 23:50:08,711] Trial 14 finished with value: 0.8195 and parameters: {'learning_rate': 0.029687202461133985, 'max_depth': 5, 'n_estimators': 1088, 'max_bin': 271, 'min_child_weight': 11, 'gamma': 0.2976646329847751, 'subsample': 0.6351819929436423, 'colsample_bytree': 0.7592104192075209, 'colsample_bynode': 0.7715407250186762, 'colsample_bylevel': 0.8631851197538537, 'reg_alpha': 0.45612668913288545, 'reg_lambda': 6.757523620398064, 'max_delta_step': 2}. Best is trial 9 with value: 0.828.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Warning fit: XGBClassifier.fit() got an unexpected keyword argument 'callbacks'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-06 23:50:10,711] Trial 15 finished with value: 0.826 and parameters: {'learning_rate': 0.04374060249859062, 'max_depth': 2, 'n_estimators': 842, 'max_bin': 381, 'min_child_weight': 14, 'gamma': 0.4300971708133748, 'subsample': 0.6716386623546919, 'colsample_bytree': 0.841970302157797, 'colsample_bynode': 0.7665140725820534, 'colsample_bylevel': 0.9405490189343315, 'reg_alpha': 0.5373458075806263, 'reg_lambda': 8.889419728348368, 'max_delta_step': 4}. Best is trial 9 with value: 0.828.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Warning fit: XGBClassifier.fit() got an unexpected keyword argument 'callbacks'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-06 23:50:13,575] Trial 16 finished with value: 0.8245 and parameters: {'learning_rate': 0.010377327039659549, 'max_depth': 4, 'n_estimators': 1033, 'max_bin': 189, 'min_child_weight': 11, 'gamma': 0.5794282054190651, 'subsample': 0.5573922803488951, 'colsample_bytree': 0.6408208534970825, 'colsample_bynode': 0.848548556384197, 'colsample_bylevel': 0.8577806267749256, 'reg_alpha': 0.317273888656067, 'reg_lambda': 5.042600704123307, 'max_delta_step': 3}. Best is trial 9 with value: 0.828.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Warning fit: XGBClassifier.fit() got an unexpected keyword argument 'callbacks'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-06 23:50:16,020] Trial 17 finished with value: 0.83 and parameters: {'learning_rate': 0.026933918169389524, 'max_depth': 3, 'n_estimators': 943, 'max_bin': 304, 'min_child_weight': 15, 'gamma': 0.3640002264427433, 'subsample': 0.7646801838048869, 'colsample_bytree': 0.7460088554636374, 'colsample_bynode': 0.639084261488893, 'colsample_bylevel': 0.9210124038311636, 'reg_alpha': 0.2236732830127453, 'reg_lambda': 11.128683316949994, 'max_delta_step': 1}. Best is trial 17 with value: 0.83.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Warning fit: XGBClassifier.fit() got an unexpected keyword argument 'callbacks'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-06 23:50:17,917] Trial 18 finished with value: 0.825 and parameters: {'learning_rate': 0.02866295836032223, 'max_depth': 2, 'n_estimators': 859, 'max_bin': 285, 'min_child_weight': 15, 'gamma': 0.30058415293973256, 'subsample': 0.7717083120694559, 'colsample_bytree': 0.7354418337575523, 'colsample_bynode': 0.6397271633627091, 'colsample_bylevel': 0.7362829644487112, 'reg_alpha': 0.20726183978972879, 'reg_lambda': 11.695661864646713, 'max_delta_step': 0}. Best is trial 17 with value: 0.83.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Warning fit: XGBClassifier.fit() got an unexpected keyword argument 'callbacks'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-06 23:50:20,180] Trial 19 finished with value: 0.8235 and parameters: {'learning_rate': 0.04002394553889389, 'max_depth': 3, 'n_estimators': 939, 'max_bin': 206, 'min_child_weight': 14, 'gamma': 0.45276630170791, 'subsample': 0.6738830450107247, 'colsample_bytree': 0.643113776096649, 'colsample_bynode': 0.6457730453255128, 'colsample_bylevel': 0.8623368191555644, 'reg_alpha': 0.2582170779000309, 'reg_lambda': 11.60538939878262, 'max_delta_step': 1}. Best is trial 17 with value: 0.83.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Warning fit: XGBClassifier.fit() got an unexpected keyword argument 'callbacks'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-06 23:50:22,504] Trial 20 finished with value: 0.828 and parameters: {'learning_rate': 0.01642755922284916, 'max_depth': 2, 'n_estimators': 1036, 'max_bin': 320, 'min_child_weight': 15, 'gamma': 0.5545268119254297, 'subsample': 0.791902950043108, 'colsample_bytree': 0.8178766726929959, 'colsample_bynode': 0.5169612157954635, 'colsample_bylevel': 0.9625753638453611, 'reg_alpha': 0.35596887759470264, 'reg_lambda': 13.083433959133869, 'max_delta_step': 1}. Best is trial 17 with value: 0.83.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Warning fit: XGBClassifier.fit() got an unexpected keyword argument 'callbacks'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-06 23:50:25,074] Trial 21 finished with value: 0.823 and parameters: {'learning_rate': 0.019852796823646685, 'max_depth': 3, 'n_estimators': 940, 'max_bin': 336, 'min_child_weight': 13, 'gamma': 0.357254886872706, 'subsample': 0.7371661819244653, 'colsample_bytree': 0.7576980671687085, 'colsample_bynode': 0.7747419303815859, 'colsample_bylevel': 0.9026082659512672, 'reg_alpha': 0.41211497819014226, 'reg_lambda': 8.540673719086467, 'max_delta_step': 2}. Best is trial 17 with value: 0.83.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Warning fit: XGBClassifier.fit() got an unexpected keyword argument 'callbacks'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-06 23:50:28,585] Trial 22 finished with value: 0.8245 and parameters: {'learning_rate': 0.0279194366858868, 'max_depth': 4, 'n_estimators': 1008, 'max_bin': 393, 'min_child_weight': 11, 'gamma': 0.3853323134890955, 'subsample': 0.8392189563427166, 'colsample_bytree': 0.793078678587833, 'colsample_bynode': 0.6037702251823018, 'colsample_bylevel': 0.9278106022909172, 'reg_alpha': 0.24652558116317103, 'reg_lambda': 10.835591370237301, 'max_delta_step': 3}. Best is trial 17 with value: 0.83.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Warning fit: XGBClassifier.fit() got an unexpected keyword argument 'callbacks'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-06 23:50:32,141] Trial 23 finished with value: 0.8275 and parameters: {'learning_rate': 0.024243612565688504, 'max_depth': 5, 'n_estimators': 948, 'max_bin': 227, 'min_child_weight': 13, 'gamma': 0.314226123604932, 'subsample': 0.5583518320581365, 'colsample_bytree': 0.7400515378693457, 'colsample_bynode': 0.8582221603280678, 'colsample_bylevel': 0.8833325678175743, 'reg_alpha': 0.49749090522648987, 'reg_lambda': 9.278213679250497, 'max_delta_step': 2}. Best is trial 17 with value: 0.83.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Warning fit: XGBClassifier.fit() got an unexpected keyword argument 'callbacks'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-06 23:50:35,010] Trial 24 finished with value: 0.8235 and parameters: {'learning_rate': 0.014651865777200495, 'max_depth': 3, 'n_estimators': 866, 'max_bin': 295, 'min_child_weight': 10, 'gamma': 0.4507742289100171, 'subsample': 0.675054082748384, 'colsample_bytree': 0.8362607255479276, 'colsample_bynode': 0.7414330710245162, 'colsample_bylevel': 0.987363166221956, 'reg_alpha': 0.4341993204978584, 'reg_lambda': 6.754691313476613, 'max_delta_step': 0}. Best is trial 17 with value: 0.83.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Warning fit: XGBClassifier.fit() got an unexpected keyword argument 'callbacks'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-06 23:50:38,178] Trial 25 finished with value: 0.8225 and parameters: {'learning_rate': 0.030895420756839494, 'max_depth': 4, 'n_estimators': 966, 'max_bin': 347, 'min_child_weight': 13, 'gamma': 0.3841895800179522, 'subsample': 0.6195867720326407, 'colsample_bytree': 0.7674633649829914, 'colsample_bynode': 0.6803533550982502, 'colsample_bylevel': 0.8423657066243853, 'reg_alpha': 0.34777756330417275, 'reg_lambda': 7.915015807076238, 'max_delta_step': 5}. Best is trial 17 with value: 0.83.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Warning fit: XGBClassifier.fit() got an unexpected keyword argument 'callbacks'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-06 23:50:41,472] Trial 26 finished with value: 0.826 and parameters: {'learning_rate': 0.025798546428979635, 'max_depth': 4, 'n_estimators': 916, 'max_bin': 491, 'min_child_weight': 12, 'gamma': 0.25524309953602575, 'subsample': 0.5848489415377721, 'colsample_bytree': 0.6823481322395012, 'colsample_bynode': 0.79072724639617, 'colsample_bylevel': 0.7575900607872407, 'reg_alpha': 0.5656361171915351, 'reg_lambda': 9.420332084373573, 'max_delta_step': 1}. Best is trial 17 with value: 0.83.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Warning fit: XGBClassifier.fit() got an unexpected keyword argument 'callbacks'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-06 23:50:44,381] Trial 27 finished with value: 0.828 and parameters: {'learning_rate': 0.020226993752791442, 'max_depth': 3, 'n_estimators': 1008, 'max_bin': 303, 'min_child_weight': 6, 'gamma': 0.32930332990502253, 'subsample': 0.5273038376180819, 'colsample_bytree': 0.8994119631340372, 'colsample_bynode': 0.8562093883934132, 'colsample_bylevel': 0.926332446748375, 'reg_alpha': 0.5213335841336528, 'reg_lambda': 10.849950722594185, 'max_delta_step': 2}. Best is trial 17 with value: 0.83.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Warning fit: XGBClassifier.fit() got an unexpected keyword argument 'callbacks'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-06 23:50:48,513] Trial 28 finished with value: 0.8215 and parameters: {'learning_rate': 0.036770283337387924, 'max_depth': 5, 'n_estimators': 1059, 'max_bin': 383, 'min_child_weight': 14, 'gamma': 0.4065997023954557, 'subsample': 0.7694873507738675, 'colsample_bytree': 0.6572680132666904, 'colsample_bynode': 0.5918097880547311, 'colsample_bylevel': 0.9571279839707805, 'reg_alpha': 0.4347342882491866, 'reg_lambda': 7.430069541491518, 'max_delta_step': 3}. Best is trial 17 with value: 0.83.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Warning fit: XGBClassifier.fit() got an unexpected keyword argument 'callbacks'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-06 23:50:51,114] Trial 29 finished with value: 0.8225 and parameters: {'learning_rate': 0.03810739245486754, 'max_depth': 4, 'n_estimators': 807, 'max_bin': 251, 'min_child_weight': 9, 'gamma': 0.46521424203748957, 'subsample': 0.701936154441081, 'colsample_bytree': 0.7227936578023534, 'colsample_bynode': 0.6739806446125416, 'colsample_bylevel': 0.8289901619244561, 'reg_alpha': 0.25659614043461176, 'reg_lambda': 9.678670468777737, 'max_delta_step': 1}. Best is trial 17 with value: 0.83.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Warning fit: XGBClassifier.fit() got an unexpected keyword argument 'callbacks'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-06 23:50:53,372] Trial 30 finished with value: 0.825 and parameters: {'learning_rate': 0.03323917744873356, 'max_depth': 3, 'n_estimators': 915, 'max_bin': 163, 'min_child_weight': 10, 'gamma': 0.5427522257861856, 'subsample': 0.8885335307911475, 'colsample_bytree': 0.808894620153653, 'colsample_bynode': 0.7417118562133773, 'colsample_bylevel': 0.9134523517443287, 'reg_alpha': 0.4918359040452287, 'reg_lambda': 6.295872379998453, 'max_delta_step': 4}. Best is trial 17 with value: 0.83.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Warning fit: XGBClassifier.fit() got an unexpected keyword argument 'callbacks'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-06 23:50:55,739] Trial 31 finished with value: 0.824 and parameters: {'learning_rate': 0.014201169923058114, 'max_depth': 2, 'n_estimators': 1042, 'max_bin': 321, 'min_child_weight': 15, 'gamma': 0.5297184883817323, 'subsample': 0.8193843874721138, 'colsample_bytree': 0.8284368123430704, 'colsample_bynode': 0.5964482315789056, 'colsample_bylevel': 0.9604480445658062, 'reg_alpha': 0.3564341936401978, 'reg_lambda': 13.671099650482617, 'max_delta_step': 1}. Best is trial 17 with value: 0.83.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Warning fit: XGBClassifier.fit() got an unexpected keyword argument 'callbacks'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-06 23:50:57,930] Trial 32 finished with value: 0.8245 and parameters: {'learning_rate': 0.017838391848022155, 'max_depth': 2, 'n_estimators': 1011, 'max_bin': 262, 'min_child_weight': 15, 'gamma': 0.6078185730837715, 'subsample': 0.7853111122027194, 'colsample_bytree': 0.8059940570314775, 'colsample_bynode': 0.5089542943072007, 'colsample_bylevel': 0.8819860528825318, 'reg_alpha': 0.31226987034920634, 'reg_lambda': 12.54897821842771, 'max_delta_step': 1}. Best is trial 17 with value: 0.83.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Warning fit: XGBClassifier.fit() got an unexpected keyword argument 'callbacks'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-06 23:50:59,640] Trial 33 finished with value: 0.8175 and parameters: {'learning_rate': 0.020833445183300877, 'max_depth': 1, 'n_estimators': 885, 'max_bin': 316, 'min_child_weight': 14, 'gamma': 0.5411056124983253, 'subsample': 0.7276788828451599, 'colsample_bytree': 0.8689395574898171, 'colsample_bynode': 0.5079211126769085, 'colsample_bylevel': 0.9515645852903627, 'reg_alpha': 0.3642436401336262, 'reg_lambda': 14.620007090596054, 'max_delta_step': 1}. Best is trial 17 with value: 0.83.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Warning fit: XGBClassifier.fit() got an unexpected keyword argument 'callbacks'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-06 23:51:02,160] Trial 34 finished with value: 0.8225 and parameters: {'learning_rate': 0.027124984690207694, 'max_depth': 2, 'n_estimators': 1097, 'max_bin': 284, 'min_child_weight': 15, 'gamma': 0.27732809483526055, 'subsample': 0.8666970030710195, 'colsample_bytree': 0.8566884246035268, 'colsample_bynode': 0.550347950780187, 'colsample_bylevel': 0.9813631256703983, 'reg_alpha': 0.40830837036593187, 'reg_lambda': 13.823072622394873, 'max_delta_step': 0}. Best is trial 17 with value: 0.83.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Warning fit: XGBClassifier.fit() got an unexpected keyword argument 'callbacks'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-06 23:51:04,577] Trial 35 finished with value: 0.826 and parameters: {'learning_rate': 0.012464407249146398, 'max_depth': 3, 'n_estimators': 923, 'max_bin': 236, 'min_child_weight': 9, 'gamma': 0.6550471591472655, 'subsample': 0.7971206784972484, 'colsample_bytree': 0.7706858219820507, 'colsample_bynode': 0.528590590276854, 'colsample_bylevel': 0.8018142443468852, 'reg_alpha': 0.2231889044136359, 'reg_lambda': 10.37350998944361, 'max_delta_step': 2}. Best is trial 17 with value: 0.83.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Warning fit: XGBClassifier.fit() got an unexpected keyword argument 'callbacks'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-06 23:51:07,043] Trial 36 finished with value: 0.8245 and parameters: {'learning_rate': 0.017696241824102746, 'max_depth': 2, 'n_estimators': 966, 'max_bin': 432, 'min_child_weight': 11, 'gamma': 0.47837579530618335, 'subsample': 0.7080111782039765, 'colsample_bytree': 0.7073526050654216, 'colsample_bynode': 0.7173834193664125, 'colsample_bylevel': 0.9933372307039863, 'reg_alpha': 0.284881771579538, 'reg_lambda': 8.35933539639973, 'max_delta_step': 0}. Best is trial 17 with value: 0.83.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Warning fit: XGBClassifier.fit() got an unexpected keyword argument 'callbacks'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-06 23:51:09,111] Trial 37 finished with value: 0.8195 and parameters: {'learning_rate': 0.02270293714187802, 'max_depth': 1, 'n_estimators': 1068, 'max_bin': 362, 'min_child_weight': 8, 'gamma': 0.5167590177622825, 'subsample': 0.753027864531497, 'colsample_bytree': 0.7551977169812151, 'colsample_bynode': 0.6935628608666784, 'colsample_bylevel': 0.6284198784332296, 'reg_alpha': 0.38137154947784513, 'reg_lambda': 12.98747091091306, 'max_delta_step': 1}. Best is trial 17 with value: 0.83.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Warning fit: XGBClassifier.fit() got an unexpected keyword argument 'callbacks'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-06 23:51:13,065] Trial 38 finished with value: 0.8225 and parameters: {'learning_rate': 0.031237332934545715, 'max_depth': 5, 'n_estimators': 1022, 'max_bin': 343, 'min_child_weight': 13, 'gamma': 0.7482672034634053, 'subsample': 0.6451744935197238, 'colsample_bytree': 0.583437219401962, 'colsample_bynode': 0.5760528314974878, 'colsample_bylevel': 0.7746256325471543, 'reg_alpha': 0.54871740827158, 'reg_lambda': 11.139516542827163, 'max_delta_step': 2}. Best is trial 17 with value: 0.83.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Warning fit: XGBClassifier.fit() got an unexpected keyword argument 'callbacks'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-06 23:51:15,858] Trial 39 finished with value: 0.8215 and parameters: {'learning_rate': 0.0540970609645624, 'max_depth': 3, 'n_estimators': 1051, 'max_bin': 306, 'min_child_weight': 14, 'gamma': 0.3634288114704128, 'subsample': 0.8419596580422526, 'colsample_bytree': 0.6830832356619286, 'colsample_bynode': 0.6182488140605673, 'colsample_bylevel': 0.9199957084011516, 'reg_alpha': 0.33324253790233205, 'reg_lambda': 10.014506818493428, 'max_delta_step': 0}. Best is trial 17 with value: 0.83.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Warning fit: XGBClassifier.fit() got an unexpected keyword argument 'callbacks'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-06 23:51:18,800] Trial 40 finished with value: 0.8255 and parameters: {'learning_rate': 0.018275465008466347, 'max_depth': 3, 'n_estimators': 993, 'max_bin': 411, 'min_child_weight': 5, 'gamma': 0.5712979713069237, 'subsample': 0.7733404159772269, 'colsample_bytree': 0.8228001533738049, 'colsample_bynode': 0.8367699255353823, 'colsample_bylevel': 0.9434177775137621, 'reg_alpha': 0.27343021310091187, 'reg_lambda': 12.170592712205409, 'max_delta_step': 2}. Best is trial 17 with value: 0.83.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Warning fit: XGBClassifier.fit() got an unexpected keyword argument 'callbacks'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-06 23:51:21,669] Trial 41 finished with value: 0.8265 and parameters: {'learning_rate': 0.020540379858768623, 'max_depth': 3, 'n_estimators': 1003, 'max_bin': 305, 'min_child_weight': 7, 'gamma': 0.3269253303216845, 'subsample': 0.527626440372335, 'colsample_bytree': 0.893747868855859, 'colsample_bynode': 0.8740596178077351, 'colsample_bylevel': 0.9202337807351552, 'reg_alpha': 0.5204516803645075, 'reg_lambda': 10.59884654065265, 'max_delta_step': 1}. Best is trial 17 with value: 0.83.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Warning fit: XGBClassifier.fit() got an unexpected keyword argument 'callbacks'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-06 23:51:25,073] Trial 42 finished with value: 0.8255 and parameters: {'learning_rate': 0.025214497384164078, 'max_depth': 4, 'n_estimators': 971, 'max_bin': 322, 'min_child_weight': 6, 'gamma': 0.3319351040351094, 'subsample': 0.5065459551260044, 'colsample_bytree': 0.871584383876979, 'colsample_bynode': 0.799895388370917, 'colsample_bylevel': 0.880301655405256, 'reg_alpha': 0.5230122518222181, 'reg_lambda': 11.206559916609, 'max_delta_step': 2}. Best is trial 17 with value: 0.83.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Warning fit: XGBClassifier.fit() got an unexpected keyword argument 'callbacks'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-06 23:51:28,363] Trial 43 finished with value: 0.826 and parameters: {'learning_rate': 0.015960514554318052, 'max_depth': 4, 'n_estimators': 1015, 'max_bin': 278, 'min_child_weight': 5, 'gamma': 0.42653984984446064, 'subsample': 0.5299507229296874, 'colsample_bytree': 0.5134404973330368, 'colsample_bynode': 0.8824719100756997, 'colsample_bylevel': 0.9041548894555669, 'reg_alpha': 0.5708444507033904, 'reg_lambda': 12.508301093973824, 'max_delta_step': 3}. Best is trial 17 with value: 0.83.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Warning fit: XGBClassifier.fit() got an unexpected keyword argument 'callbacks'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-06 23:51:30,259] Trial 44 finished with value: 0.822 and parameters: {'learning_rate': 0.02234882611279579, 'max_depth': 1, 'n_estimators': 960, 'max_bin': 357, 'min_child_weight': 8, 'gamma': 0.36287977209858585, 'subsample': 0.5659850063130013, 'colsample_bytree': 0.7883033840754209, 'colsample_bynode': 0.8207354462900643, 'colsample_bylevel': 0.9700159928145884, 'reg_alpha': 0.46895197293534097, 'reg_lambda': 7.379886345673161, 'max_delta_step': 1}. Best is trial 17 with value: 0.83.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Warning fit: XGBClassifier.fit() got an unexpected keyword argument 'callbacks'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-06 23:51:33,335] Trial 45 finished with value: 0.8255 and parameters: {'learning_rate': 0.012153536706639413, 'max_depth': 3, 'n_estimators': 994, 'max_bin': 303, 'min_child_weight': 12, 'gamma': 0.27921000593863443, 'subsample': 0.5397170421743301, 'colsample_bytree': 0.7359287177271789, 'colsample_bynode': 0.8555244063377648, 'colsample_bylevel': 0.932739238634548, 'reg_alpha': 0.5952714353277588, 'reg_lambda': 14.964431063868602, 'max_delta_step': 2}. Best is trial 17 with value: 0.83.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Warning fit: XGBClassifier.fit() got an unexpected keyword argument 'callbacks'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-06 23:51:36,625] Trial 46 finished with value: 0.8255 and parameters: {'learning_rate': 0.0199786136859381, 'max_depth': 4, 'n_estimators': 1024, 'max_bin': 264, 'min_child_weight': 9, 'gamma': 0.3337629389480449, 'subsample': 0.5763639001623867, 'colsample_bytree': 0.8490844029089726, 'colsample_bynode': 0.7543684990704321, 'colsample_bylevel': 0.8222985959808353, 'reg_alpha': 0.43840193969338104, 'reg_lambda': 8.428390859949324, 'max_delta_step': 3}. Best is trial 17 with value: 0.83.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Warning fit: XGBClassifier.fit() got an unexpected keyword argument 'callbacks'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-06 23:51:38,544] Trial 47 finished with value: 0.8255 and parameters: {'learning_rate': 0.025742423923135804, 'max_depth': 2, 'n_estimators': 743, 'max_bin': 372, 'min_child_weight': 10, 'gamma': 0.4003968371578644, 'subsample': 0.6064708403609336, 'colsample_bytree': 0.8011947068821332, 'colsample_bynode': 0.8341359938496176, 'colsample_bylevel': 0.6876410721150595, 'reg_alpha': 0.48997166446572704, 'reg_lambda': 9.708333664556385, 'max_delta_step': 1}. Best is trial 17 with value: 0.83.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Warning fit: XGBClassifier.fit() got an unexpected keyword argument 'callbacks'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-06 23:51:41,271] Trial 48 finished with value: 0.829 and parameters: {'learning_rate': 0.02356500935866276, 'max_depth': 3, 'n_estimators': 890, 'max_bin': 338, 'min_child_weight': 6, 'gamma': 0.49536799135630916, 'subsample': 0.5138982263026048, 'colsample_bytree': 0.8782788092523248, 'colsample_bynode': 0.8067378506179144, 'colsample_bylevel': 0.871828483173245, 'reg_alpha': 0.5464642418229524, 'reg_lambda': 14.060529232431005, 'max_delta_step': 4}. Best is trial 17 with value: 0.83.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Warning fit: XGBClassifier.fit() got an unexpected keyword argument 'callbacks'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-06 23:51:43,502] Trial 49 finished with value: 0.8245 and parameters: {'learning_rate': 0.030388051898137054, 'max_depth': 2, 'n_estimators': 894, 'max_bin': 399, 'min_child_weight': 12, 'gamma': 0.4929037014539197, 'subsample': 0.8263812711635129, 'colsample_bytree': 0.7076660390172161, 'colsample_bynode': 0.6554322052439002, 'colsample_bylevel': 0.8443833840761443, 'reg_alpha': 0.5598765826950408, 'reg_lambda': 14.33945850158953, 'max_delta_step': 4}. Best is trial 17 with value: 0.83.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "OPTUNA FINE-TUNING RESULTS\n",
      "======================================================================\n",
      "Best accuracy (holdout): 83.00%\n",
      "Best trial: #17\n",
      "\n",
      "⚙️ Best hyperparameters:\n",
      "  learning_rate       : 0.0269\n",
      "  max_depth           : 3\n",
      "  n_estimators        : 943\n",
      "  max_bin             : 304\n",
      "  min_child_weight    : 15\n",
      "  gamma               : 0.3640\n",
      "  subsample           : 0.7647\n",
      "  colsample_bytree    : 0.7460\n",
      "  colsample_bynode    : 0.6391\n",
      "  colsample_bylevel   : 0.9210\n",
      "  reg_alpha           : 0.2237\n",
      "  reg_lambda          : 11.1287\n",
      "  max_delta_step      : 1\n",
      "\n",
      "✅ Salvati 50 trial in 'optuna_finetuning_trials.csv'\n",
      "\n",
      "📊 CONFRONTO:\n",
      "  Baseline (trial 0):  82.70%\n",
      "  Best (trial 17):     83.00%\n",
      "  Improvement:         +0.30%\n",
      "\n",
      "✅ MIGLIORAMENTO SIGNIFICATIVO → Usa best_params Optuna\n",
      "\n",
      "✅ Variabile 'best_params' aggiornata con HP Optuna\n",
      "\n",
      "🎯 NEXT STEP: Esegui cella 'Cross validation' per validare su 10-fold\n"
     ]
    }
   ],
   "source": [
    "# === SOSTITUISCI COMPLETAMENTE cella 14 (Optuna Fine-Tuning) ===\n",
    "\n",
    "import optuna\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"OPTUNA FINE-TUNING - RICERCA FOCALIZZATA ATTORNO A BASELINE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# 1. Parametri baseline (bussola)\n",
    "known_params = {\n",
    "    'booster': 'gbtree',\n",
    "    'tree_method': 'hist',\n",
    "    'max_bin': 256,\n",
    "    'learning_rate': 0.035,\n",
    "    'n_estimators': 900,\n",
    "    'max_depth': 3,\n",
    "    'min_child_weight': 9,\n",
    "    'gamma': 0.5,\n",
    "    'subsample': 0.7,\n",
    "    'colsample_bytree': 0.7,\n",
    "    'colsample_bynode': 0.7,\n",
    "    'colsample_bylevel': 0.8,\n",
    "    'reg_alpha': 0.4,\n",
    "    'reg_lambda': 10.0,\n",
    "    'max_delta_step': 1\n",
    "}\n",
    "\n",
    "# 2. Funzione obiettivo con spazio RISTRETTO\n",
    "def objective_fine_tuning(trial):\n",
    "    param = {\n",
    "        'booster': 'gbtree',\n",
    "        'tree_method': 'hist',\n",
    "        'use_label_encoder': False,\n",
    "        'eval_metric': 'logloss',\n",
    "        'random_state': 42,\n",
    "        \n",
    "        # ✅ Range ristretti attorno a baseline\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.06),\n",
    "        'max_depth': trial.suggest_int('max_depth', 1, 5),\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 700, 1100),\n",
    "        'max_bin': trial.suggest_int('max_bin', 128, 512),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 5, 15),\n",
    "        'gamma': trial.suggest_float('gamma', 0.25, 0.75),\n",
    "        'subsample': trial.suggest_float('subsample', 0.5, 0.9),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 0.9),\n",
    "        'colsample_bynode': trial.suggest_float('colsample_bynode', 0.5, 0.9),\n",
    "        'colsample_bylevel': trial.suggest_float('colsample_bylevel', 0.6, 1.0),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 0.2, 0.6),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 5.0, 15.0),\n",
    "        'max_delta_step': trial.suggest_int('max_delta_step', 0, 5)\n",
    "    }\n",
    "\n",
    "    EARLY_STOPPING_ROUNDS = 50\n",
    "    \n",
    "    # ✅ FIX: callback con maximize=False per logloss\n",
    "    model = xgb.XGBClassifier(**param)\n",
    "    \n",
    "    try:\n",
    "        cb = getattr(xgb.callback, \"EarlyStopping\", None)\n",
    "        if cb is not None:\n",
    "            model.fit(\n",
    "                X_train_val, y_train_val, \n",
    "                eval_set=[(X_holdout, y_holdout)], \n",
    "                callbacks=[cb(rounds=EARLY_STOPPING_ROUNDS, save_best=True, maximize=False)],  # ✅ maximize=False!\n",
    "                verbose=False\n",
    "            )\n",
    "        else:\n",
    "            # Fallback per versioni vecchie\n",
    "            model.fit(\n",
    "                X_train_val, y_train_val, \n",
    "                eval_set=[(X_holdout, y_holdout)],\n",
    "                early_stopping_rounds=EARLY_STOPPING_ROUNDS,\n",
    "                verbose=False\n",
    "            )\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Warning fit: {e}\")\n",
    "        model.fit(X_train_val, y_train_val, verbose=False)\n",
    "    \n",
    "    # Valuta accuracy su holdout\n",
    "    preds = model.predict(X_holdout)\n",
    "    accuracy = accuracy_score(y_holdout, preds)\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "# 3. Sampler aggressivo (TPE dopo 1 trial)\n",
    "sampler = optuna.samplers.TPESampler(n_startup_trials=1, seed=42)\n",
    "\n",
    "study = optuna.create_study(\n",
    "    direction='maximize', \n",
    "    sampler=sampler,\n",
    "    pruner=optuna.pruners.MedianPruner(n_startup_trials=5, n_warmup_steps=3)\n",
    ")\n",
    "\n",
    "# 4. ✅ Inserisci baseline come primo trial\n",
    "study.enqueue_trial(known_params)\n",
    "\n",
    "# 5. Ottimizzazione\n",
    "N_TRIALS = 50\n",
    "print(f\"\\nAvvio {N_TRIALS} trial (1° trial = baseline, poi TPE)...\\n\")\n",
    "\n",
    "study.optimize(objective_fine_tuning, n_trials=N_TRIALS, timeout=3600, gc_after_trial=True)\n",
    "\n",
    "# 6. Report\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"OPTUNA FINE-TUNING RESULTS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Best accuracy (holdout): {study.best_value*100:.2f}%\")\n",
    "print(f\"Best trial: #{study.best_trial.number}\")\n",
    "\n",
    "print(f\"\\n⚙️ Best hyperparameters:\")\n",
    "for k, v in study.best_params.items():\n",
    "    if isinstance(v, float):\n",
    "        print(f\"  {k:20s}: {v:.4f}\")\n",
    "    else:\n",
    "        print(f\"  {k:20s}: {v}\")\n",
    "\n",
    "# Salva risultati\n",
    "import pandas as pd\n",
    "trials_df = study.trials_dataframe()\n",
    "trials_df = trials_df.sort_values('value', ascending=False)\n",
    "trials_df.to_csv('optuna_finetuning_trials.csv', index=False)\n",
    "print(f\"\\n✅ Salvati {len(trials_df)} trial in 'optuna_finetuning_trials.csv'\")\n",
    "\n",
    "# Confronto con baseline\n",
    "baseline_trial = trials_df[trials_df['number'] == 0].iloc[0] if len(trials_df) > 0 else None\n",
    "if baseline_trial is not None:\n",
    "    baseline_acc = baseline_trial['value']\n",
    "    improvement = (study.best_value - baseline_acc) * 100\n",
    "    print(f\"\\n📊 CONFRONTO:\")\n",
    "    print(f\"  Baseline (trial 0):  {baseline_acc*100:.2f}%\")\n",
    "    print(f\"  Best (trial {study.best_trial.number}):     {study.best_value*100:.2f}%\")\n",
    "    print(f\"  Improvement:         {improvement:+.2f}%\")\n",
    "    \n",
    "    if improvement > 0.3:\n",
    "        print(f\"\\n✅ MIGLIORAMENTO SIGNIFICATIVO → Usa best_params Optuna\")\n",
    "        USE_OPTUNA = True\n",
    "    else:\n",
    "        print(f\"\\n⚠️ Miglioramento marginale (<0.3%) → Mantieni baseline\")\n",
    "        USE_OPTUNA = False\n",
    "else:\n",
    "    USE_OPTUNA = False\n",
    "\n",
    "# Assegna best_params per CV\n",
    "if USE_OPTUNA:\n",
    "    best_params = study.best_params.copy()\n",
    "    best_params.update({\n",
    "        'booster': 'gbtree',\n",
    "        'tree_method': 'hist',\n",
    "        'use_label_encoder': False,\n",
    "        'eval_metric': 'logloss',\n",
    "        'random_state': 42\n",
    "    })\n",
    "    print(f\"\\n✅ Variabile 'best_params' aggiornata con HP Optuna\")\n",
    "else:\n",
    "    best_params = known_params.copy()\n",
    "    print(f\"\\n⚠️ Mantieni best_params baseline\")\n",
    "\n",
    "print(\"\\n🎯 NEXT STEP: Esegui cella 'Cross validation' per validare su 10-fold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "239a614a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-06 21:16:01,279] A new study created in memory with name: no-name-899fcf9f-026c-4652-a526-1daefbb64b83\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "OPTUNA - ACCURACY MAXIMIZATION CON GAP TARGET ~4%\n",
      "======================================================================\n",
      "Avvio ottimizzazione Optuna...\n",
      "Trials: 120 | CV folds: 5 | Gap target: 4.0% ± 1%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-06 21:16:32,747] Trial 0 finished with value: 0.5311407497219631 and parameters: {'n_estimators': 900, 'learning_rate': 0.10985745201142037, 'max_depth': 5, 'min_child_weight': 8, 'gamma': 0.1872223685309238, 'reg_alpha': 0.3899863008405066, 'reg_lambda': 1.638919733850194, 'subsample': 0.9098528437324805, 'colsample_bytree': 0.8303345035229626, 'colsample_bylevel': 0.8770181444490113}. Best is trial 0 with value: 0.5311407497219631.\n",
      "[I 2025-11-06 21:16:55,684] Trial 1 finished with value: 0.540425 and parameters: {'n_estimators': 500, 'learning_rate': 0.11370159575730848, 'max_depth': 6, 'min_child_weight': 5, 'gamma': 0.21818996064852073, 'reg_alpha': 0.45851127463358454, 'reg_lambda': 4.346664672554915, 'subsample': 0.8074269294896713, 'colsample_bytree': 0.7795835055926348, 'colsample_bylevel': 0.7728072850495105}. Best is trial 1 with value: 0.540425.\n",
      "[I 2025-11-06 21:17:29,057] Trial 2 finished with value: 0.7368320923951681 and parameters: {'n_estimators': 1100, 'learning_rate': 0.02567895331620816, 'max_depth': 4, 'min_child_weight': 6, 'gamma': 0.5472839810604431, 'reg_alpha': 1.962939903482534, 'reg_lambda': 3.196411603741957, 'subsample': 0.8042703315240834, 'colsample_bytree': 0.8277243706586127, 'colsample_bylevel': 0.7116126031799994}. Best is trial 2 with value: 0.7368320923951681.\n",
      "[I 2025-11-06 21:17:58,184] Trial 3 finished with value: 0.8207500000000002 and parameters: {'n_estimators': 1100, 'learning_rate': 0.027147107064325292, 'max_depth': 3, 'min_child_weight': 12, 'gamma': 1.1587584396894712, 'reg_alpha': 2.0209933702911527, 'reg_lambda': 4.350751460907078, 'subsample': 0.6793016342019151, 'colsample_bytree': 0.855269907953647, 'colsample_bylevel': 0.8100381234349003}. Best is trial 3 with value: 0.8207500000000002.\n",
      "[I 2025-11-06 21:18:14,549] Trial 4 finished with value: 0.8170104515075935 and parameters: {'n_estimators': 600, 'learning_rate': 0.04856825805999369, 'max_depth': 3, 'min_child_weight': 12, 'gamma': 0.3105359779200203, 'reg_alpha': 1.656305710884955, 'reg_lambda': 4.42882183698352, 'subsample': 0.8060204063533433, 'colsample_bytree': 0.8140130838029839, 'colsample_bylevel': 0.7462136138813817}. Best is trial 3 with value: 0.8207500000000002.\n",
      "[I 2025-11-06 21:18:56,568] Trial 5 finished with value: 0.5545674329182918 and parameters: {'n_estimators': 1500, 'learning_rate': 0.08020483900081053, 'max_depth': 6, 'min_child_weight': 11, 'gamma': 0.7174799745733021, 'reg_alpha': 2.304685587557792, 'reg_lambda': 1.9734175225711144, 'subsample': 0.7087948587257435, 'colsample_bytree': 0.6635681866731614, 'colsample_bylevel': 0.7813325826908161}. Best is trial 3 with value: 0.8207500000000002.\n",
      "[I 2025-11-06 21:19:34,977] Trial 6 finished with value: 0.5858613783485016 and parameters: {'n_estimators': 900, 'learning_rate': 0.03252225000724057, 'max_depth': 6, 'min_child_weight': 6, 'gamma': 0.3371214116248569, 'reg_alpha': 1.3567402078956212, 'reg_lambda': 2.5501664747223893, 'subsample': 0.8906590942262118, 'colsample_bytree': 0.6723651931039313, 'colsample_bylevel': 0.9467217341501293}. Best is trial 3 with value: 0.8207500000000002.\n",
      "[I 2025-11-06 21:20:08,401] Trial 7 finished with value: 0.8069759037128549 and parameters: {'n_estimators': 1300, 'learning_rate': 0.02855359871232564, 'max_depth': 3, 'min_child_weight': 11, 'gamma': 0.8482288126171406, 'reg_alpha': 1.8225179201024684, 'reg_lambda': 9.483973813545402, 'subsample': 0.6722133955202271, 'colsample_bytree': 0.7575397185632817, 'colsample_bylevel': 0.7289672648812824}. Best is trial 3 with value: 0.8207500000000002.\n",
      "[I 2025-11-06 21:20:55,463] Trial 8 finished with value: 0.5522145595325568 and parameters: {'n_estimators': 1400, 'learning_rate': 0.061101266510534887, 'max_depth': 4, 'min_child_weight': 3, 'gamma': 0.3731787860587946, 'reg_alpha': 0.8129583050668676, 'reg_lambda': 9.025667961718705, 'subsample': 0.841267241406564, 'colsample_bytree': 0.9161638227728979, 'colsample_bylevel': 0.8180537312904873}. Best is trial 3 with value: 0.8207500000000002.\n",
      "[I 2025-11-06 21:21:21,912] Trial 9 finished with value: 0.5991737375347292 and parameters: {'n_estimators': 600, 'learning_rate': 0.07178642382355842, 'max_depth': 6, 'min_child_weight': 8, 'gamma': 0.9251606159454732, 'reg_alpha': 1.2344889909109769, 'reg_lambda': 6.7500611232019345, 'subsample': 0.7782623055075649, 'colsample_bytree': 0.6576257380232285, 'colsample_bylevel': 0.726972856748326}. Best is trial 3 with value: 0.8207500000000002.\n",
      "[I 2025-11-06 21:21:29,371] Trial 10 pruned. \n",
      "[I 2025-11-06 21:22:05,411] Trial 11 finished with value: 0.7122168124824718 and parameters: {'n_estimators': 800, 'learning_rate': 0.02669835736577288, 'max_depth': 6, 'min_child_weight': 11, 'gamma': 0.7600845078125081, 'reg_alpha': 2.1786514754692945, 'reg_lambda': 9.840392845890259, 'subsample': 0.7059710176658107, 'colsample_bytree': 0.9177676995469932, 'colsample_bylevel': 0.8348355604789126}. Best is trial 3 with value: 0.8207500000000002.\n",
      "[I 2025-11-06 21:22:20,488] Trial 12 pruned. \n",
      "[I 2025-11-06 21:22:30,311] Trial 13 pruned. \n",
      "[I 2025-11-06 21:22:48,134] Trial 14 pruned. \n",
      "[I 2025-11-06 21:23:00,061] Trial 15 pruned. \n",
      "[I 2025-11-06 21:23:11,681] Trial 16 pruned. \n",
      "[I 2025-11-06 21:23:22,316] Trial 17 pruned. \n",
      "[I 2025-11-06 21:23:30,245] Trial 18 pruned. \n",
      "[I 2025-11-06 21:23:47,694] Trial 19 pruned. \n",
      "[I 2025-11-06 21:23:58,434] Trial 20 pruned. \n",
      "[I 2025-11-06 21:24:12,362] Trial 21 pruned. \n",
      "[I 2025-11-06 21:24:25,789] Trial 22 pruned. \n",
      "[I 2025-11-06 21:24:57,398] Trial 23 finished with value: 0.7957971272672147 and parameters: {'n_estimators': 1200, 'learning_rate': 0.033925122115622375, 'max_depth': 3, 'min_child_weight': 12, 'gamma': 1.189532635372319, 'reg_alpha': 1.430340293117878, 'reg_lambda': 11.655568667095, 'subsample': 0.6731501593606547, 'colsample_bytree': 0.9459696184609803, 'colsample_bylevel': 0.7406355905675518}. Best is trial 3 with value: 0.8207500000000002.\n",
      "[I 2025-11-06 21:25:27,004] Trial 24 finished with value: 0.7797266532980751 and parameters: {'n_estimators': 1000, 'learning_rate': 0.023521709611070998, 'max_depth': 4, 'min_child_weight': 11, 'gamma': 1.0182732855288903, 'reg_alpha': 1.7177971807941215, 'reg_lambda': 4.514001285839418, 'subsample': 0.7333880374538936, 'colsample_bytree': 0.794039453580385, 'colsample_bylevel': 0.7000636881157738}. Best is trial 3 with value: 0.8207500000000002.\n",
      "[I 2025-11-06 21:25:40,006] Trial 25 pruned. \n",
      "[I 2025-11-06 21:25:54,367] Trial 26 pruned. \n",
      "[I 2025-11-06 21:26:14,226] Trial 27 pruned. \n",
      "[I 2025-11-06 21:26:53,241] Trial 28 finished with value: 0.7697797849524525 and parameters: {'n_estimators': 1000, 'learning_rate': 0.045195397594616306, 'max_depth': 3, 'min_child_weight': 9, 'gamma': 0.7826325583461498, 'reg_alpha': 1.456373977130447, 'reg_lambda': 5.979068588290657, 'subsample': 0.7308704194644745, 'colsample_bytree': 0.6994534765924068, 'colsample_bylevel': 0.9124231612792275}. Best is trial 3 with value: 0.8207500000000002.\n",
      "[I 2025-11-06 21:27:33,375] Trial 29 pruned. \n",
      "[I 2025-11-06 21:27:55,178] Trial 30 pruned. \n",
      "[I 2025-11-06 21:28:56,266] Trial 31 finished with value: 0.8017437561154658 and parameters: {'n_estimators': 1200, 'learning_rate': 0.03343244329597517, 'max_depth': 3, 'min_child_weight': 12, 'gamma': 1.1900303508904353, 'reg_alpha': 1.4588058275325884, 'reg_lambda': 11.76131224613374, 'subsample': 0.6736085965107387, 'colsample_bytree': 0.9282793967981942, 'colsample_bylevel': 0.7407135950832812}. Best is trial 3 with value: 0.8207500000000002.\n",
      "[I 2025-11-06 21:29:21,549] Trial 32 pruned. \n",
      "[I 2025-11-06 21:30:14,019] Trial 33 finished with value: 0.7781297912869454 and parameters: {'n_estimators': 1100, 'learning_rate': 0.04243476195963392, 'max_depth': 3, 'min_child_weight': 11, 'gamma': 1.0940304245316093, 'reg_alpha': 1.2837894333665623, 'reg_lambda': 10.759631680528317, 'subsample': 0.7778728382071936, 'colsample_bytree': 0.9442321109383083, 'colsample_bylevel': 0.7563175799348717}. Best is trial 3 with value: 0.8207500000000002.\n",
      "[I 2025-11-06 21:30:39,201] Trial 34 pruned. \n",
      "[I 2025-11-06 21:31:11,690] Trial 35 finished with value: 0.8124717729025376 and parameters: {'n_estimators': 1100, 'learning_rate': 0.028419518671427994, 'max_depth': 3, 'min_child_weight': 11, 'gamma': 0.20428121629068124, 'reg_alpha': 1.5410589989776655, 'reg_lambda': 4.57058952549137, 'subsample': 0.8230533978038652, 'colsample_bytree': 0.8799527141286994, 'colsample_bylevel': 0.7238214028798794}. Best is trial 3 with value: 0.8207500000000002.\n",
      "[I 2025-11-06 21:31:44,104] Trial 36 finished with value: 0.7610756356388741 and parameters: {'n_estimators': 900, 'learning_rate': 0.028222897486380228, 'max_depth': 4, 'min_child_weight': 11, 'gamma': 0.2384726660066489, 'reg_alpha': 1.5778352259693802, 'reg_lambda': 4.253137104436378, 'subsample': 0.8197267056167435, 'colsample_bytree': 0.8161497941566092, 'colsample_bylevel': 0.725597991293938}. Best is trial 3 with value: 0.8207500000000002.\n",
      "[I 2025-11-06 21:32:06,188] Trial 37 pruned. \n",
      "[I 2025-11-06 21:32:30,717] Trial 38 pruned. \n",
      "[I 2025-11-06 21:32:48,813] Trial 39 pruned. \n",
      "[I 2025-11-06 21:33:01,988] Trial 40 pruned. \n",
      "[I 2025-11-06 21:33:15,573] Trial 41 pruned. \n",
      "[I 2025-11-06 21:33:28,032] Trial 42 pruned. \n",
      "[I 2025-11-06 21:33:55,143] Trial 43 pruned. \n",
      "[I 2025-11-06 21:34:23,513] Trial 44 pruned. \n",
      "[I 2025-11-06 21:34:46,846] Trial 45 pruned. \n",
      "[I 2025-11-06 21:35:04,298] Trial 46 pruned. \n",
      "[I 2025-11-06 21:35:23,380] Trial 47 pruned. \n",
      "[I 2025-11-06 21:35:30,494] Trial 48 pruned. \n",
      "[I 2025-11-06 21:35:47,773] Trial 49 pruned. \n",
      "[I 2025-11-06 21:35:58,772] Trial 50 pruned. \n",
      "[I 2025-11-06 21:36:12,307] Trial 51 pruned. \n",
      "[I 2025-11-06 21:36:35,384] Trial 52 pruned. \n",
      "[I 2025-11-06 21:37:03,074] Trial 53 pruned. \n",
      "[I 2025-11-06 21:37:43,889] Trial 54 pruned. \n",
      "[I 2025-11-06 21:38:01,742] Trial 55 pruned. \n",
      "[I 2025-11-06 21:38:17,716] Trial 56 pruned. \n",
      "[I 2025-11-06 21:38:52,303] Trial 57 pruned. \n",
      "[I 2025-11-06 21:39:25,661] Trial 58 finished with value: 0.804204391655228 and parameters: {'n_estimators': 1100, 'learning_rate': 0.033594920638050696, 'max_depth': 3, 'min_child_weight': 10, 'gamma': 0.014656794824942199, 'reg_alpha': 1.9323624640659343, 'reg_lambda': 11.39779271568304, 'subsample': 0.6877983982804093, 'colsample_bytree': 0.8812137515561916, 'colsample_bylevel': 0.7474766184980698}. Best is trial 3 with value: 0.8207500000000002.\n",
      "[I 2025-11-06 21:39:39,414] Trial 59 pruned. \n",
      "[I 2025-11-06 21:39:56,106] Trial 60 pruned. \n",
      "[I 2025-11-06 21:40:49,934] Trial 61 finished with value: 0.7987857292359094 and parameters: {'n_estimators': 1200, 'learning_rate': 0.033318132256037435, 'max_depth': 3, 'min_child_weight': 11, 'gamma': 0.29316867323603935, 'reg_alpha': 1.6589962804337894, 'reg_lambda': 10.154629676409169, 'subsample': 0.6898890604613358, 'colsample_bytree': 0.9217179084210043, 'colsample_bylevel': 0.7397179566561192}. Best is trial 3 with value: 0.8207500000000002.\n",
      "[I 2025-11-06 21:41:05,075] Trial 62 pruned. \n",
      "[I 2025-11-06 21:41:18,457] Trial 63 pruned. \n",
      "[I 2025-11-06 21:41:32,210] Trial 64 pruned. \n",
      "[I 2025-11-06 21:42:15,336] Trial 65 finished with value: 0.8153395322452037 and parameters: {'n_estimators': 1300, 'learning_rate': 0.026247765949767837, 'max_depth': 3, 'min_child_weight': 8, 'gamma': 0.5145190836087504, 'reg_alpha': 2.082639509673963, 'reg_lambda': 11.406198799651746, 'subsample': 0.6556606744988538, 'colsample_bytree': 0.879721262752483, 'colsample_bylevel': 0.777232942160422}. Best is trial 3 with value: 0.8207500000000002.\n",
      "[I 2025-11-06 21:42:59,136] Trial 66 finished with value: 0.8128065527307475 and parameters: {'n_estimators': 1300, 'learning_rate': 0.02682240122240699, 'max_depth': 3, 'min_child_weight': 8, 'gamma': 0.5098215024173443, 'reg_alpha': 2.18278830047681, 'reg_lambda': 11.486509804390717, 'subsample': 0.6550665752580724, 'colsample_bytree': 0.879078390419533, 'colsample_bylevel': 0.7898246527122348}. Best is trial 3 with value: 0.8207500000000002.\n",
      "[I 2025-11-06 21:43:15,963] Trial 67 pruned. \n",
      "[I 2025-11-06 21:43:37,023] Trial 68 pruned. \n",
      "[I 2025-11-06 21:43:53,068] Trial 69 pruned. \n",
      "[I 2025-11-06 21:44:21,987] Trial 70 pruned. \n",
      "[I 2025-11-06 21:45:16,713] Trial 71 finished with value: 0.8179687544326264 and parameters: {'n_estimators': 1300, 'learning_rate': 0.023966691417992923, 'max_depth': 3, 'min_child_weight': 8, 'gamma': 0.5119589891405654, 'reg_alpha': 2.0899935310817117, 'reg_lambda': 11.913193754450317, 'subsample': 0.680702354571993, 'colsample_bytree': 0.8828712646842111, 'colsample_bylevel': 0.8091130749966208}. Best is trial 3 with value: 0.8207500000000002.\n",
      "[I 2025-11-06 21:46:09,091] Trial 72 finished with value: 0.8201737747060582 and parameters: {'n_estimators': 1300, 'learning_rate': 0.02381102545508907, 'max_depth': 3, 'min_child_weight': 8, 'gamma': 0.48296974786193714, 'reg_alpha': 2.0604155986292443, 'reg_lambda': 11.31734583445796, 'subsample': 0.6823541625032348, 'colsample_bytree': 0.8869121218033074, 'colsample_bylevel': 0.816079595943579}. Best is trial 3 with value: 0.8207500000000002.\n",
      "[I 2025-11-06 21:46:30,480] Trial 73 pruned. \n",
      "[I 2025-11-06 21:46:52,407] Trial 74 pruned. \n",
      "[I 2025-11-06 21:47:50,033] Trial 75 finished with value: 0.81944862658091 and parameters: {'n_estimators': 1400, 'learning_rate': 0.020460312852217288, 'max_depth': 3, 'min_child_weight': 8, 'gamma': 0.645506142746186, 'reg_alpha': 2.331355469528339, 'reg_lambda': 5.003778180330222, 'subsample': 0.6582102566476465, 'colsample_bytree': 0.8426663829207914, 'colsample_bylevel': 0.8096128633444397}. Best is trial 3 with value: 0.8207500000000002.\n",
      "[I 2025-11-06 21:48:46,570] Trial 76 finished with value: 0.8176730555928362 and parameters: {'n_estimators': 1400, 'learning_rate': 0.02029777750520596, 'max_depth': 3, 'min_child_weight': 8, 'gamma': 0.6507536238733191, 'reg_alpha': 2.3389220951085066, 'reg_lambda': 4.357387421112214, 'subsample': 0.6608325866181366, 'colsample_bytree': 0.841198078926563, 'colsample_bylevel': 0.8003294141574002}. Best is trial 3 with value: 0.8207500000000002.\n",
      "[I 2025-11-06 21:49:09,344] Trial 77 pruned. \n",
      "[I 2025-11-06 21:49:44,463] Trial 78 pruned. \n",
      "[I 2025-11-06 21:50:08,619] Trial 79 pruned. \n",
      "[I 2025-11-06 21:50:29,100] Trial 80 pruned. \n",
      "[I 2025-11-06 21:50:54,661] Trial 81 pruned. \n",
      "[I 2025-11-06 21:51:22,593] Trial 82 pruned. \n",
      "[I 2025-11-06 21:51:52,713] Trial 83 pruned. \n",
      "[I 2025-11-06 21:52:17,774] Trial 84 pruned. \n",
      "[I 2025-11-06 21:52:28,223] Trial 85 pruned. \n",
      "[I 2025-11-06 21:52:57,159] Trial 86 finished with value: 0.8148075867027389 and parameters: {'n_estimators': 600, 'learning_rate': 0.02326762340054907, 'max_depth': 3, 'min_child_weight': 9, 'gamma': 0.6106303667263974, 'reg_alpha': 2.246417971832911, 'reg_lambda': 4.018152527459047, 'subsample': 0.682997247181726, 'colsample_bytree': 0.8747244635951043, 'colsample_bylevel': 0.7758458695945847}. Best is trial 3 with value: 0.8207500000000002.\n",
      "[I 2025-11-06 21:53:08,657] Trial 87 pruned. \n",
      "[I 2025-11-06 21:53:20,602] Trial 88 pruned. \n",
      "[I 2025-11-06 21:53:35,135] Trial 89 pruned. \n",
      "[I 2025-11-06 21:54:22,372] Trial 90 finished with value: 0.7948556854914561 and parameters: {'n_estimators': 800, 'learning_rate': 0.02258885122263716, 'max_depth': 4, 'min_child_weight': 8, 'gamma': 0.4909883161559483, 'reg_alpha': 2.1680860568777423, 'reg_lambda': 3.5524389352875896, 'subsample': 0.6628755041326481, 'colsample_bytree': 0.8733639993475323, 'colsample_bylevel': 0.8557590593301906}. Best is trial 3 with value: 0.8207500000000002.\n",
      "[I 2025-11-06 21:54:33,794] Trial 91 pruned. \n",
      "[I 2025-11-06 21:54:42,954] Trial 92 pruned. \n",
      "[I 2025-11-06 21:55:24,733] Trial 93 pruned. \n",
      "[I 2025-11-06 21:55:43,916] Trial 94 pruned. \n",
      "[I 2025-11-06 21:56:11,593] Trial 95 pruned. \n",
      "[I 2025-11-06 21:56:28,489] Trial 96 pruned. \n",
      "[I 2025-11-06 21:56:46,295] Trial 97 pruned. \n",
      "[I 2025-11-06 21:57:05,427] Trial 98 pruned. \n",
      "[I 2025-11-06 21:57:22,652] Trial 99 pruned. \n",
      "[I 2025-11-06 21:58:08,123] Trial 100 pruned. \n",
      "[I 2025-11-06 21:58:33,808] Trial 101 pruned. \n",
      "[I 2025-11-06 21:58:48,025] Trial 102 pruned. \n",
      "[I 2025-11-06 21:59:02,091] Trial 103 pruned. \n",
      "[I 2025-11-06 21:59:51,597] Trial 104 finished with value: 0.7981817629374762 and parameters: {'n_estimators': 1400, 'learning_rate': 0.027050380935905522, 'max_depth': 3, 'min_child_weight': 12, 'gamma': 0.972172986832007, 'reg_alpha': 1.9767634969683279, 'reg_lambda': 4.481756207666791, 'subsample': 0.6672139736736695, 'colsample_bytree': 0.7960285830603875, 'colsample_bylevel': 0.778519850295708}. Best is trial 3 with value: 0.8207500000000002.\n",
      "[I 2025-11-06 22:00:05,737] Trial 105 pruned. \n",
      "[I 2025-11-06 22:00:32,873] Trial 106 pruned. \n",
      "[I 2025-11-06 22:00:48,666] Trial 107 pruned. \n",
      "[I 2025-11-06 22:01:01,222] Trial 108 pruned. \n",
      "[I 2025-11-06 22:01:15,360] Trial 109 pruned. \n",
      "[I 2025-11-06 22:01:42,514] Trial 110 pruned. \n",
      "[I 2025-11-06 22:02:04,394] Trial 111 pruned. \n",
      "[I 2025-11-06 22:02:53,789] Trial 112 finished with value: 0.8219486316601976 and parameters: {'n_estimators': 1100, 'learning_rate': 0.028303556697850518, 'max_depth': 3, 'min_child_weight': 10, 'gamma': 0.6837108619071602, 'reg_alpha': 1.9504715982500367, 'reg_lambda': 11.279430937233052, 'subsample': 0.6873254296027391, 'colsample_bytree': 0.8829442796726085, 'colsample_bylevel': 0.7089243372546584}. Best is trial 112 with value: 0.8219486316601976.\n",
      "[I 2025-11-06 22:03:12,901] Trial 113 pruned. \n",
      "[I 2025-11-06 22:03:36,828] Trial 114 pruned. \n",
      "[I 2025-11-06 22:03:58,173] Trial 115 pruned. \n",
      "[I 2025-11-06 22:04:18,896] Trial 116 pruned. \n",
      "[I 2025-11-06 22:04:33,363] Trial 117 pruned. \n",
      "[I 2025-11-06 22:05:05,952] Trial 118 finished with value: 0.8195389864442365 and parameters: {'n_estimators': 1200, 'learning_rate': 0.02644585208880618, 'max_depth': 3, 'min_child_weight': 11, 'gamma': 0.7793472470959626, 'reg_alpha': 2.2744146266414154, 'reg_lambda': 11.896342451623122, 'subsample': 0.6839162546771809, 'colsample_bytree': 0.8712842917330839, 'colsample_bylevel': 0.7838087810478371}. Best is trial 112 with value: 0.8219486316601976.\n",
      "[I 2025-11-06 22:05:19,317] Trial 119 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "OPTUNA OPTIMIZATION RESULTS\n",
      "======================================================================\n",
      "Best objective score: 0.8219\n",
      "Best trial: #112\n",
      "\n",
      "📊 Best trial metrics:\n",
      "  Mean CV accuracy:    82.54%\n",
      "  Mean train-val gap:  4.73%  (target: 4.0%)\n",
      "  Std CV accuracy:     0.86%\n",
      "\n",
      "⚙️ Best hyperparameters:\n",
      "  n_estimators        : 1100\n",
      "  learning_rate       : 0.0283\n",
      "  max_depth           : 3\n",
      "  min_child_weight    : 10\n",
      "  gamma               : 0.6837\n",
      "  reg_alpha           : 1.9505\n",
      "  reg_lambda          : 11.2794\n",
      "  subsample           : 0.6873\n",
      "  colsample_bytree    : 0.8829\n",
      "  colsample_bylevel   : 0.7089\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'user_attrs'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3811\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3812\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3813\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:167\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:196\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7096\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: 'user_attrs'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[55]\u001b[39m\u001b[32m, line 181\u001b[39m\n\u001b[32m    179\u001b[39m \u001b[38;5;66;03m# Aggiungi colonne user_attrs per analisi\u001b[39;00m\n\u001b[32m    180\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m attr \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m'\u001b[39m\u001b[33mmean_val_acc\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mmean_gap\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mstd_val\u001b[39m\u001b[33m'\u001b[39m]:\n\u001b[32m--> \u001b[39m\u001b[32m181\u001b[39m     trials_df[attr] = \u001b[43mtrials_df\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43muser_attrs\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m.apply(\u001b[38;5;28;01mlambda\u001b[39;00m x: x.get(attr, np.nan) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m np.nan)\n\u001b[32m    183\u001b[39m trials_df = trials_df.sort_values(\u001b[33m'\u001b[39m\u001b[33mvalue\u001b[39m\u001b[33m'\u001b[39m, ascending=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    184\u001b[39m trials_df.to_csv(\u001b[33m'\u001b[39m\u001b[33moptuna_trials_gap4_target.csv\u001b[39m\u001b[33m'\u001b[39m, index=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\pandas\\core\\frame.py:4113\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns.nlevels > \u001b[32m1\u001b[39m:\n\u001b[32m   4112\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem_multilevel(key)\n\u001b[32m-> \u001b[39m\u001b[32m4113\u001b[39m indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4114\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[32m   4115\u001b[39m     indexer = [indexer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\pandas\\core\\indexes\\base.py:3819\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3814\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   3815\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc.Iterable)\n\u001b[32m   3816\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[32m   3817\u001b[39m     ):\n\u001b[32m   3818\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[32m-> \u001b[39m\u001b[32m3819\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   3820\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   3821\u001b[39m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[32m   3822\u001b[39m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[32m   3823\u001b[39m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[32m   3824\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n",
      "\u001b[31mKeyError\u001b[39m: 'user_attrs'"
     ]
    }
   ],
   "source": [
    "# === SOSTITUISCI COMPLETAMENTE cella Optuna (dopo \"# Hyperparameter search\") ===\n",
    "\n",
    "import optuna\n",
    "import numpy as np\n",
    "from optuna.samplers import TPESampler\n",
    "from optuna.pruners import MedianPruner\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from xgboost import XGBClassifier\n",
    "import xgboost as xgb\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"OPTUNA - ACCURACY MAXIMIZATION CON GAP TARGET ~4%\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "N_TRIALS = 120  # ridotto da 150 per convergenza veloce\n",
    "EARLY_STOPPING_ROUNDS = 50\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# TARGET: mantieni gap medio tra 3-5% (ideale 4%)\n",
    "GAP_TARGET_MIN = 0.03\n",
    "GAP_TARGET_MAX = 0.05\n",
    "GAP_TARGET_IDEAL = 0.04\n",
    "\n",
    "def _predict_proba_best(clf, X):\n",
    "    \"\"\"Predict proba usando best_iteration se disponibile.\"\"\"\n",
    "    it = getattr(clf, \"best_iteration\", None)\n",
    "    try:\n",
    "        if it is not None:\n",
    "            return clf.predict_proba(X, iteration_range=(0, int(it)+1))[:,1]\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        b = clf.get_booster()\n",
    "        nt = getattr(b, \"best_ntree_limit\", None)\n",
    "        if nt is not None:\n",
    "            return clf.predict_proba(X, ntree_limit=int(nt))[:,1]\n",
    "    except Exception:\n",
    "        pass\n",
    "    return clf.predict_proba(X)[:,1]\n",
    "\n",
    "def objective(trial):\n",
    "    # ✅ SEARCH SPACE OTTIMIZZATO per accuracy con gap 4%\n",
    "    params = {\n",
    "        \"booster\": \"gbtree\",\n",
    "        \"tree_method\": \"hist\",\n",
    "        \"max_bin\": 256,  # fisso per velocità\n",
    "        \n",
    "        # Alberi: range ampio con early stopping\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 500, 1500, step=100),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.02, 0.12, log=True),\n",
    "        \n",
    "        # Depth: range completo (200+ features)\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 6),\n",
    "        \"min_child_weight\": trial.suggest_int(\"min_child_weight\", 3, 12),\n",
    "        \n",
    "        # Regularization: range ampio per controllare gap\n",
    "        \"gamma\": trial.suggest_float(\"gamma\", 0.0, 1.2),\n",
    "        \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 0.0, 2.5),\n",
    "        \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 1.0, 12.0),\n",
    "        \n",
    "        # Sampling: completo\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.65, 0.95),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.65, 0.95),\n",
    "        \"colsample_bylevel\": trial.suggest_float(\"colsample_bylevel\", 0.7, 0.95),\n",
    "    }\n",
    "\n",
    "    fold_accs = []\n",
    "    fold_gaps = []\n",
    "    \n",
    "    for tr_idx, va_idx in cv.split(X_train_val, y_train_val):\n",
    "        X_tr, X_va = X_train_val[tr_idx], X_train_val[va_idx]\n",
    "        y_tr, y_va = y_train_val[tr_idx], y_train_val[va_idx]\n",
    "\n",
    "        clf = XGBClassifier(**params, use_label_encoder=False, eval_metric=\"logloss\", \n",
    "                           random_state=42, n_jobs=1)\n",
    "        \n",
    "        # Fit con early stopping\n",
    "        try:\n",
    "            cb = getattr(xgb.callback, \"EarlyStopping\", None)\n",
    "            if cb is not None:\n",
    "                clf.fit(X_tr, y_tr, eval_set=[(X_va, y_va)], \n",
    "                       callbacks=[cb(rounds=EARLY_STOPPING_ROUNDS, save_best=True, maximize=False)], \n",
    "                       verbose=False)\n",
    "            else:\n",
    "                clf.fit(X_tr, y_tr, eval_set=[(X_va, y_va)], \n",
    "                       early_stopping_rounds=EARLY_STOPPING_ROUNDS, verbose=False)\n",
    "        except Exception:\n",
    "            clf.fit(X_tr, y_tr, eval_set=[(X_va, y_va)], verbose=False)\n",
    "\n",
    "        # ✅ ACCURACY (coerente con CV finale)\n",
    "        proba_val = _predict_proba_best(clf, X_va)\n",
    "        pred_val = (proba_val >= 0.5).astype(int)\n",
    "        val_acc = accuracy_score(y_va, pred_val)\n",
    "        \n",
    "        proba_tr = _predict_proba_best(clf, X_tr)\n",
    "        pred_tr = (proba_tr >= 0.5).astype(int)\n",
    "        tr_acc = accuracy_score(y_tr, pred_tr)\n",
    "\n",
    "        gap = tr_acc - val_acc\n",
    "        fold_accs.append(val_acc)\n",
    "        fold_gaps.append(gap)\n",
    "\n",
    "        # Pruning: interrompi se accuracy troppo bassa\n",
    "        trial.report(val_acc, len(fold_accs))\n",
    "        if trial.should_prune():\n",
    "            raise optuna.TrialPruned()\n",
    "\n",
    "    mean_val_acc = float(np.mean(fold_accs))\n",
    "    mean_gap = float(np.mean(fold_gaps))\n",
    "    std_val = float(np.std(fold_accs))\n",
    "\n",
    "    # ✅ PENALTY PER GAP FUORI TARGET 3-5%\n",
    "    if mean_gap < GAP_TARGET_MIN:\n",
    "        # Gap troppo basso → underfit, penalizza moderatamente\n",
    "        gap_penalty = (GAP_TARGET_MIN - mean_gap) * 1.0\n",
    "    elif mean_gap > GAP_TARGET_MAX:\n",
    "        # Gap troppo alto → overfit, penalizza fortemente\n",
    "        gap_penalty = (mean_gap - GAP_TARGET_MAX) * 2.0\n",
    "    else:\n",
    "        # Gap nel range 3-5% → bonus se vicino a 4%\n",
    "        distance_from_ideal = abs(mean_gap - GAP_TARGET_IDEAL)\n",
    "        gap_penalty = -0.002 if distance_from_ideal < 0.005 else 0.0  # bonus se gap ~4%\n",
    "    \n",
    "    # ✅ PENALTY per variance alta (instabilità tra fold)\n",
    "    stability_penalty = std_val * 0.4\n",
    "    \n",
    "    # Objective: massimizza accuracy - penalties\n",
    "    objective_score = mean_val_acc - gap_penalty - stability_penalty\n",
    "\n",
    "    # Logga metriche per debug\n",
    "    trial.set_user_attr(\"mean_val_acc\", mean_val_acc)\n",
    "    trial.set_user_attr(\"mean_gap\", mean_gap)\n",
    "    trial.set_user_attr(\"std_val\", std_val)\n",
    "    trial.set_user_attr(\"gap_penalty\", gap_penalty)\n",
    "\n",
    "    return float(objective_score)\n",
    "\n",
    "# ✅ Esegui ottimizzazione\n",
    "print(f\"Avvio ottimizzazione Optuna...\")\n",
    "print(f\"Trials: {N_TRIALS} | CV folds: 5 | Gap target: {GAP_TARGET_IDEAL*100:.1f}% ± 1%\\n\")\n",
    "\n",
    "study = optuna.create_study(\n",
    "    direction=\"maximize\", \n",
    "    sampler=TPESampler(seed=42, n_startup_trials=15),  # 15 trial random warmup\n",
    "    pruner=MedianPruner(n_startup_trials=10, n_warmup_steps=2)  # pruning aggressivo\n",
    ")\n",
    "\n",
    "study.optimize(objective, n_trials=N_TRIALS, timeout=5400, gc_after_trial=True)\n",
    "\n",
    "# ✅ Report dettagliato\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"OPTUNA OPTIMIZATION RESULTS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Best objective score: {study.best_value:.4f}\")\n",
    "print(f\"Best trial: #{study.best_trial.number}\")\n",
    "\n",
    "best_attrs = study.best_trial.user_attrs\n",
    "best_val_acc = best_attrs.get('mean_val_acc', 0.0)\n",
    "best_gap = best_attrs.get('mean_gap', 0.0)\n",
    "best_std = best_attrs.get('std_val', 0.0)\n",
    "\n",
    "print(f\"\\n📊 Best trial metrics:\")\n",
    "print(f\"  Mean CV accuracy:    {best_val_acc*100:.2f}%\")\n",
    "print(f\"  Mean train-val gap:  {best_gap*100:.2f}%  (target: {GAP_TARGET_IDEAL*100:.1f}%)\")\n",
    "print(f\"  Std CV accuracy:     {best_std*100:.2f}%\")\n",
    "\n",
    "print(f\"\\n⚙️ Best hyperparameters:\")\n",
    "for k, v in study.best_params.items():\n",
    "    if isinstance(v, float):\n",
    "        print(f\"  {k:20s}: {v:.4f}\")\n",
    "    else:\n",
    "        print(f\"  {k:20s}: {v}\")\n",
    "\n",
    "# Salva risultati\n",
    "import pandas as pd\n",
    "trials_df = study.trials_dataframe()\n",
    "\n",
    "# Aggiungi colonne user_attrs per analisi\n",
    "for attr in ['mean_val_acc', 'mean_gap', 'std_val']:\n",
    "    trials_df[attr] = trials_df['user_attrs'].apply(lambda x: x.get(attr, np.nan) if isinstance(x, dict) else np.nan)\n",
    "\n",
    "trials_df = trials_df.sort_values('value', ascending=False)\n",
    "trials_df.to_csv('optuna_trials_gap4_target.csv', index=False)\n",
    "print(f\"\\n✅ Salvati {len(trials_df)} trial in 'optuna_trials_gap4_target.csv'\")\n",
    "\n",
    "# ✅ Analisi distribuzione gap nei trial\n",
    "valid_trials = trials_df[trials_df['state'] == 'COMPLETE'].copy()\n",
    "if len(valid_trials) > 0:\n",
    "    gaps = valid_trials['mean_gap'].dropna()\n",
    "    print(f\"\\n📈 DISTRIBUZIONE GAP (su {len(gaps)} trial completi):\")\n",
    "    print(f\"  Mean: {gaps.mean()*100:.2f}%\")\n",
    "    print(f\"  Std:  {gaps.std()*100:.2f}%\")\n",
    "    print(f\"  Min:  {gaps.min()*100:.2f}%\")\n",
    "    print(f\"  Max:  {gaps.max()*100:.2f}%\")\n",
    "    print(f\"  Trials con gap 3-5%: {((gaps >= 0.03) & (gaps <= 0.05)).sum()}/{len(gaps)} ({((gaps >= 0.03) & (gaps <= 0.05)).mean()*100:.1f}%)\")\n",
    "\n",
    "# ✅ Confronto con baseline\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"COMPARISON WITH BASELINE\")\n",
    "print(f\"{'='*70}\")\n",
    "baseline_cv = 81.38\n",
    "baseline_gap = 4.19\n",
    "improvement_acc = (best_val_acc * 100) - baseline_cv\n",
    "improvement_gap = abs(best_gap*100 - baseline_gap)\n",
    "\n",
    "print(f\"Baseline:\")\n",
    "print(f\"  CV accuracy:  {baseline_cv:.2f}%\")\n",
    "print(f\"  Gap:          {baseline_gap:.2f}%\")\n",
    "print(f\"\\nOptuna best:\")\n",
    "print(f\"  CV accuracy:  {best_val_acc*100:.2f}%  ({improvement_acc:+.2f}%)\")\n",
    "print(f\"  Gap:          {best_gap*100:.2f}%  (Δ={improvement_gap:+.2f}%)\")\n",
    "\n",
    "# Decision logic\n",
    "if improvement_acc > 0.3 and abs(best_gap*100 - GAP_TARGET_IDEAL*100) < 1.5:\n",
    "    print(f\"\\n🎉 OTTIMO! Accuracy migliorata E gap ottimale (~4%)\")\n",
    "    print(f\"   ✅ USA questi HP per CV/holdout\")\n",
    "    USE_OPTUNA_HP = True\n",
    "elif improvement_acc > 0.5:\n",
    "    print(f\"\\n✅ Accuracy significativamente migliorata (>{improvement_acc:.2f}%)\")\n",
    "    print(f\"   Gap: {best_gap*100:.2f}% (target 4%, attuale baseline 4.19%)\")\n",
    "    if best_gap*100 <= 6.0:\n",
    "        print(f\"   ✅ Gap accettabile, USA questi HP\")\n",
    "        USE_OPTUNA_HP = True\n",
    "    else:\n",
    "        print(f\"   ⚠️ Gap alto, valuta holdout prima di usare\")\n",
    "        USE_OPTUNA_HP = False\n",
    "elif best_gap*100 >= GAP_TARGET_MIN*100 and best_gap*100 <= GAP_TARGET_MAX*100:\n",
    "    print(f\"\\n✅ Gap ottimale ({best_gap*100:.2f}% nel range 3-5%)\")\n",
    "    if improvement_acc > 0:\n",
    "        print(f\"   Accuracy: {improvement_acc:+.2f}% → USA questi HP\")\n",
    "        USE_OPTUNA_HP = True\n",
    "    else:\n",
    "        print(f\"   Accuracy non migliorata, mantieni baseline\")\n",
    "        USE_OPTUNA_HP = False\n",
    "else:\n",
    "    print(f\"\\n⚠️ Nessun miglioramento significativo\")\n",
    "    print(f\"   Baseline HP già ottimi, SKIP Optuna\")\n",
    "    USE_OPTUNA_HP = False\n",
    "\n",
    "# ✅ Assegna best_params per le celle successive\n",
    "if USE_OPTUNA_HP:\n",
    "    best_params = study.best_params.copy()\n",
    "    best_params.update({\n",
    "        'booster': 'gbtree',\n",
    "        'tree_method': 'hist',\n",
    "        'max_bin': 256,\n",
    "        'use_label_encoder': False,\n",
    "        'eval_metric': 'logloss',\n",
    "        'random_state': 42\n",
    "    })\n",
    "    print(f\"\\n✅ Variabile 'best_params' aggiornata con HP Optuna\")\n",
    "    print(\"   Riesegui cella 'Cross validation' per validare su 10-fold\")\n",
    "else:\n",
    "    print(f\"\\n⚠️ Mantieni best_params baseline (non sovrascritto)\")\n",
    "    print(\"   Puoi comunque testare manualmente copiando HP da best_trial\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c663bc73",
   "metadata": {},
   "source": [
    "# Ottimizzazione Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "efdc9272",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "THRESHOLD OPTIMIZATION - CALIBRATION SET APPROACH\n",
      "======================================================================\n",
      "Train size: 5600 | Calib size: 2400 | Holdout size: 2000\n",
      "\n",
      "CALIBRATION RESULTS:\n",
      "  Threshold 0.5:  accuracy = 81.96%\n",
      "  Threshold opt:  accuracy = 82.33% @thr=0.445\n",
      "  Gain:           +0.38%\n",
      "\n",
      "HOLDOUT VALIDATION (final unbiased estimate):\n",
      "  Threshold 0.5:  accuracy = 82.15%\n",
      "  Threshold opt:  accuracy = 81.70% @thr=0.445\n",
      "  Gain:           +-0.45%\n",
      "\n",
      "STABILITY CHECK:\n",
      "  CV thresholds:     mean=0.498, std=0.090, range=[0.326, 0.623]\n",
      "  Calib threshold:   0.445\n",
      "  Distance from CV:  0.053 (< 0.1 → stable)\n",
      "\n",
      "⚠️ Guadagno marginale (<0.5%), meglio threshold=0.5 per robustezza\n",
      "\n",
      "======================================================================\n",
      "RE-TRAINING SU FULL TRAIN_VAL CON THRESHOLD OTTIMALE\n",
      "======================================================================\n",
      "Final holdout accuracy @thr=0.500: 82.70%\n",
      "CV accuracy (riferimento):                      81.38%\n",
      "Gain vs baseline CV:                            +1.32%\n",
      "\n",
      "✅ Variabile OPTIMIZED_THRESHOLD = 0.500 salvata per submission\n"
     ]
    }
   ],
   "source": [
    "# === NEW CELL: Threshold Optimization su Calibration Set ===\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"THRESHOLD OPTIMIZATION - CALIBRATION SET APPROACH\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Split train_val in 70% train + 30% calib (NO LEAK!)\n",
    "X_train, X_calib, y_train, y_calib = train_test_split(\n",
    "    X_train_val, y_train_val, test_size=0.3, random_state=42, stratify=y_train_val\n",
    ")\n",
    "\n",
    "print(f\"Train size: {len(X_train)} | Calib size: {len(X_calib)} | Holdout size: {len(X_holdout)}\")\n",
    "\n",
    "# 1. Fit model su train (70%)\n",
    "clf_thr = XGBClassifier(**best_params, use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
    "clf_thr.fit(X_train, y_train)\n",
    "\n",
    "# 2. Predict proba su calib set (30% unseen)\n",
    "proba_calib = clf_thr.predict_proba(X_calib)[:, 1]\n",
    "\n",
    "# 3. Grid search threshold su calib (NO LEAK: non usiamo y_calib per nient'altro)\n",
    "def find_best_threshold(y_true, proba, metric='accuracy', n_grid=301):\n",
    "    \"\"\"Grid search threshold ottimizzando metric specificata.\"\"\"\n",
    "    grid = np.unique(np.quantile(proba, np.linspace(0, 1, n_grid)))\n",
    "    best_thr, best_score = 0.5, 0.0\n",
    "    \n",
    "    for t in grid:\n",
    "        pred = (proba >= t).astype(int)\n",
    "        \n",
    "        if metric == 'accuracy':\n",
    "            score = accuracy_score(y_true, pred)\n",
    "        elif metric == 'balanced_accuracy':\n",
    "            from sklearn.metrics import balanced_accuracy_score\n",
    "            score = balanced_accuracy_score(y_true, pred)\n",
    "        else:\n",
    "            raise ValueError(f\"metric={metric} non supportato\")\n",
    "        \n",
    "        # Tie-break: preferisci threshold più vicino a 0.5 (più stabile)\n",
    "        if (score > best_score) or (abs(score - best_score) < 1e-12 and abs(t - 0.5) < abs(best_thr - 0.5)):\n",
    "            best_score, best_thr = float(score), float(t)\n",
    "    \n",
    "    return best_thr, best_score\n",
    "\n",
    "thr_opt, acc_calib_opt = find_best_threshold(y_calib, proba_calib, metric='accuracy', n_grid=501)\n",
    "\n",
    "# Confronta con threshold fisso 0.5\n",
    "pred_calib_05 = (proba_calib >= 0.5).astype(int)\n",
    "acc_calib_05 = accuracy_score(y_calib, pred_calib_05)\n",
    "\n",
    "print(f\"\\nCALIBRATION RESULTS:\")\n",
    "print(f\"  Threshold 0.5:  accuracy = {acc_calib_05*100:.2f}%\")\n",
    "print(f\"  Threshold opt:  accuracy = {acc_calib_opt*100:.2f}% @thr={thr_opt:.3f}\")\n",
    "print(f\"  Gain:           +{(acc_calib_opt - acc_calib_05)*100:.2f}%\")\n",
    "\n",
    "# 4. Valida su HOLDOUT (unbiased final test)\n",
    "proba_holdout = clf_thr.predict_proba(X_holdout)[:, 1]\n",
    "\n",
    "pred_holdout_05 = (proba_holdout >= 0.5).astype(int)\n",
    "acc_holdout_05 = accuracy_score(y_holdout, pred_holdout_05)\n",
    "\n",
    "pred_holdout_opt = (proba_holdout >= thr_opt).astype(int)\n",
    "acc_holdout_opt = accuracy_score(y_holdout, pred_holdout_opt)\n",
    "\n",
    "print(f\"\\nHOLDOUT VALIDATION (final unbiased estimate):\")\n",
    "print(f\"  Threshold 0.5:  accuracy = {acc_holdout_05*100:.2f}%\")\n",
    "print(f\"  Threshold opt:  accuracy = {acc_holdout_opt*100:.2f}% @thr={thr_opt:.3f}\")\n",
    "print(f\"  Gain:           +{(acc_holdout_opt - acc_holdout_05)*100:.2f}%\")\n",
    "\n",
    "# 5. Check se threshold è stabile (confronta con CV folds)\n",
    "cv_thresholds = [folds_info[i]['thr_acc'] for i in range(10)]\n",
    "thr_mean_cv = np.mean(cv_thresholds)\n",
    "thr_std_cv = np.std(cv_thresholds)\n",
    "\n",
    "print(f\"\\nSTABILITY CHECK:\")\n",
    "print(f\"  CV thresholds:     mean={thr_mean_cv:.3f}, std={thr_std_cv:.3f}, range=[{min(cv_thresholds):.3f}, {max(cv_thresholds):.3f}]\")\n",
    "print(f\"  Calib threshold:   {thr_opt:.3f}\")\n",
    "print(f\"  Distance from CV:  {abs(thr_opt - thr_mean_cv):.3f} (< 0.1 → stable)\")\n",
    "\n",
    "if thr_std_cv > 0.15:\n",
    "    print(\"\\n⚠️ WARNING: CV thresholds highly variable → probabilità mal calibrate\")\n",
    "    print(\"   Considera Platt scaling o isotonic regression per calibration\")\n",
    "elif acc_holdout_opt > acc_holdout_05 + 0.005:\n",
    "    print(f\"\\n✅ RACCOMANDAZIONE: Usa threshold={thr_opt:.3f} in submission\")\n",
    "    print(f\"   Guadagno stimato su test: +{(acc_holdout_opt - acc_holdout_05)*100:.2f}%\")\n",
    "    BEST_THRESHOLD = thr_opt\n",
    "else:\n",
    "    print(f\"\\n⚠️ Guadagno marginale (<0.5%), meglio threshold=0.5 per robustezza\")\n",
    "    BEST_THRESHOLD = 0.5\n",
    "\n",
    "# 6. Re-train su FULL train_val con threshold ottimale\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"RE-TRAINING SU FULL TRAIN_VAL CON THRESHOLD OTTIMALE\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "clf_final = XGBClassifier(**best_params, use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
    "clf_final.fit(X_train_val, y_train_val)\n",
    "\n",
    "# Final holdout eval\n",
    "proba_holdout_final = clf_final.predict_proba(X_holdout)[:, 1]\n",
    "pred_holdout_final = (proba_holdout_final >= BEST_THRESHOLD).astype(int)\n",
    "acc_holdout_final = accuracy_score(y_holdout, pred_holdout_final)\n",
    "\n",
    "print(f\"Final holdout accuracy @thr={BEST_THRESHOLD:.3f}: {acc_holdout_final*100:.2f}%\")\n",
    "print(f\"CV accuracy (riferimento):                      {81.38:.2f}%\")\n",
    "print(f\"Gain vs baseline CV:                            +{(acc_holdout_final - 81.38/100)*100:.2f}%\")\n",
    "\n",
    "# Save per submission\n",
    "OPTIMIZED_THRESHOLD = BEST_THRESHOLD\n",
    "print(f\"\\n✅ Variabile OPTIMIZED_THRESHOLD = {OPTIMIZED_THRESHOLD:.3f} salvata per submission\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4ff77b",
   "metadata": {},
   "source": [
    "# Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cab0d514",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 10-Fold Cross-Validation (9 train + 1 validation) ===\n",
      "Parametri utilizzati: {'booster': 'gbtree', 'tree_method': 'hist', 'max_bin': 256, 'learning_rate': 0.035, 'n_estimators': 900, 'max_depth': 3, 'min_child_weight': 9, 'gamma': 0.5, 'subsample': 0.7, 'colsample_bytree': 0.7, 'colsample_bynode': 0.7, 'colsample_bylevel': 0.8, 'reg_alpha': 0.4, 'reg_lambda': 10.0, 'max_delta_step': 1}\n",
      "\n",
      "Fold 1: no ES, train=7200, val=800, acc_val=80.88%, acc_val_opt=81.12% @thr=0.490, acc_train=86.99%, gap=6.11%\n",
      "Fold 2: no ES, train=7200, val=800, acc_val=81.25%, acc_val_opt=81.75% @thr=0.439, acc_train=86.93%, gap=5.68%\n",
      "Fold 3: no ES, train=7200, val=800, acc_val=82.25%, acc_val_opt=82.38% @thr=0.400, acc_train=86.82%, gap=4.57%\n",
      "Fold 4: no ES, train=7200, val=800, acc_val=83.88%, acc_val_opt=84.88% @thr=0.429, acc_train=87.00%, gap=3.12%\n",
      "Fold 5: no ES, train=7200, val=800, acc_val=85.25%, acc_val_opt=85.38% @thr=0.504, acc_train=86.71%, gap=1.46%\n",
      "Fold 6: no ES, train=7200, val=800, acc_val=81.38%, acc_val_opt=81.75% @thr=0.522, acc_train=87.10%, gap=5.72%\n",
      "Fold 7: no ES, train=7200, val=800, acc_val=83.50%, acc_val_opt=84.12% @thr=0.514, acc_train=86.79%, gap=3.29%\n",
      "Fold 8: no ES, train=7200, val=800, acc_val=83.50%, acc_val_opt=83.75% @thr=0.506, acc_train=86.76%, gap=3.26%\n",
      "Fold 9: no ES, train=7200, val=800, acc_val=82.75%, acc_val_opt=82.88% @thr=0.490, acc_train=87.03%, gap=4.28%\n",
      "Fold 10: no ES, train=7200, val=800, acc_val=81.38%, acc_val_opt=81.88% @thr=0.489, acc_train=87.25%, gap=5.88%\n",
      "\n",
      "============================================================\n",
      "Risultati Cross-Validation\n",
      "============================================================\n",
      "  Fold 1: val_acc=80.88%, val_acc_opt=81.12% @thr=0.490, train_acc=86.99%, gap=6.11%\n",
      "  Fold 2: val_acc=81.25%, val_acc_opt=81.75% @thr=0.439, train_acc=86.93%, gap=5.68%\n",
      "  Fold 3: val_acc=82.25%, val_acc_opt=82.38% @thr=0.400, train_acc=86.82%, gap=4.57%\n",
      "  Fold 4: val_acc=83.88%, val_acc_opt=84.88% @thr=0.429, train_acc=87.00%, gap=3.12%\n",
      "  Fold 5: val_acc=85.25%, val_acc_opt=85.38% @thr=0.504, train_acc=86.71%, gap=1.46%\n",
      "  Fold 6: val_acc=81.38%, val_acc_opt=81.75% @thr=0.522, train_acc=87.10%, gap=5.72%\n",
      "  Fold 7: val_acc=83.50%, val_acc_opt=84.12% @thr=0.514, train_acc=86.79%, gap=3.29%\n",
      "  Fold 8: val_acc=83.50%, val_acc_opt=83.75% @thr=0.506, train_acc=86.76%, gap=3.26%\n",
      "  Fold 9: val_acc=82.75%, val_acc_opt=82.88% @thr=0.490, train_acc=87.03%, gap=4.28%\n",
      "  Fold 10: val_acc=81.38%, val_acc_opt=81.88% @thr=0.489, train_acc=87.25%, gap=5.88%\n",
      "\n",
      "Mean CV accuracy (0.5): 82.60%\n",
      "Mean CV accuracy (opt thr): 82.99%\n",
      "Mean train accuracy: 86.94%\n",
      "Mean gap (train - val): 4.34%\n",
      "Std CV accuracy:  1.35%\n",
      "Min/Max val acc:  80.88% / 85.25%\n",
      "\n",
      "Peggiore fold: #1 con acc_val=80.88% | acc_val_opt=81.12% | acc_train=86.99% | gap=6.11%\n"
     ]
    }
   ],
   "source": [
    "# === 10-Fold Cross-Validation con iperparametri FISSI ===\n",
    "# IMPORTANTE: Assegna qui i migliori iperparametri trovati dalla cella precedente\n",
    "# Oppure lascia questi di default (conservativi per ridurre overfitting)\n",
    "\n",
    "best_params = {\n",
    "    'booster': 'gbtree',\n",
    "    'tree_method': 'hist',\n",
    "    'max_bin': 256,          # istogrammi più grossolani = meno varianza e più veloce\n",
    "    'learning_rate': 0.035,  # leggermente più alto con meno alberi\n",
    "    'n_estimators': 900,     # meno alberi per ridurre overfitting\n",
    "    'max_depth': 3,\n",
    "    'min_child_weight': 9,   # nodi con più peso => meno overfit\n",
    "    'gamma': 0.5,            # penalizza split deboli\n",
    "    'subsample': 0.7,\n",
    "    'colsample_bytree': 0.7,\n",
    "    'colsample_bynode': 0.7,\n",
    "    'colsample_bylevel': 0.8,\n",
    "    'reg_alpha': 0.4,        # L1\n",
    "    'reg_lambda': 10.0,      # L2 più alta\n",
    "    'max_delta_step': 1\n",
    "}\n",
    "\n",
    "print(\"=== 10-Fold Cross-Validation (9 train + 1 validation) ===\")\n",
    "print(f\"Parametri utilizzati: {best_params}\\n\")\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import xgboost as xgb  # per callback EarlyStopping se disponibile\n",
    "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "outer_accuracies = []\n",
    "folds_info = []\n",
    "train_accuracies = []\n",
    "train_val_gaps = []\n",
    "outer_accuracies_opt = []\n",
    "\n",
    "EARLY_STOPPING_ROUNDS = 50\n",
    "\n",
    "def best_threshold_for_accuracy(y_true, proba, n_grid=201):\n",
    "    y_true = np.asarray(y_true).astype(int)\n",
    "    proba = np.asarray(proba).astype(float)\n",
    "    grid = np.unique(np.quantile(proba, np.linspace(0, 1, n_grid)))\n",
    "    best_thr, best_acc = 0.5, 0.0\n",
    "    for t in grid:\n",
    "        acc = ( ((proba >= t).astype(int) == y_true).mean() )\n",
    "        if (acc > best_acc) or (abs(acc - best_acc) < 1e-12 and abs(t - 0.5) < abs(best_thr - 0.5)):\n",
    "            best_acc, best_thr = float(acc), float(t)\n",
    "    return best_thr, best_acc\n",
    "\n",
    "def _fit_with_es(clf, X_tr, y_tr, X_val, y_val):\n",
    "    \"\"\"Fit con EarlyStopping via callback se supportato; fallback senza ES.\"\"\"\n",
    "    try:\n",
    "        cb = getattr(xgb.callback, 'EarlyStopping', None)\n",
    "        if cb is not None:\n",
    "            clf.fit(X_tr, y_tr, eval_set=[(X_val, y_val)], callbacks=[cb(rounds=EARLY_STOPPING_ROUNDS, save_best=True, maximize=False)], verbose=False)\n",
    "            return True\n",
    "    except TypeError:\n",
    "        pass\n",
    "    clf.fit(X_tr, y_tr, eval_set=[(X_val, y_val)], verbose=False)\n",
    "    return False\n",
    "\n",
    "def _predict_proba_best(clf, X, best_iter=None, best_ntree_limit=None):\n",
    "    \"\"\"Version-safe predict_proba using either iteration_range (new) or ntree_limit (old).\"\"\"\n",
    "    try:\n",
    "        if best_iter is not None:\n",
    "            return clf.predict_proba(X, iteration_range=(0, int(best_iter)+1))[:, 1]\n",
    "    except TypeError:\n",
    "        pass\n",
    "    try:\n",
    "        if best_ntree_limit is not None:\n",
    "            return clf.predict_proba(X, ntree_limit=int(best_ntree_limit))[:, 1]\n",
    "    except TypeError:\n",
    "        pass\n",
    "    return clf.predict_proba(X)[:, 1]\n",
    "\n",
    "fold_idx = 0\n",
    "for train_idx, val_idx in skf.split(X_train_val, y_train_val):\n",
    "    fold_idx += 1\n",
    "    X_tr, X_val = X_train_val[train_idx], X_train_val[val_idx]\n",
    "    y_tr, y_val = y_train_val[train_idx], y_train_val[val_idx]\n",
    "\n",
    "    clf = XGBClassifier(**best_params, use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
    "    used_es = _fit_with_es(clf, X_tr, y_tr, X_val, y_val)\n",
    "\n",
    "    best_iter = getattr(clf, 'best_iteration', None)\n",
    "    try:\n",
    "        booster = clf.get_booster()\n",
    "    except Exception:\n",
    "        booster = None\n",
    "    best_ntree_limit = getattr(booster, 'best_ntree_limit', None) if booster is not None else None\n",
    "\n",
    "    y_val_proba = _predict_proba_best(clf, X_val, best_iter, best_ntree_limit)\n",
    "    y_pred = (y_val_proba >= 0.5).astype(int)\n",
    "    acc = accuracy_score(y_val, y_pred)\n",
    "    outer_accuracies.append(acc)\n",
    "\n",
    "    y_tr_proba = _predict_proba_best(clf, X_tr, best_iter, best_ntree_limit)\n",
    "    y_tr_pred = (y_tr_proba >= 0.5).astype(int)\n",
    "    tr_acc = accuracy_score(y_tr, y_tr_pred)\n",
    "    gap = float(tr_acc - acc)\n",
    "    train_accuracies.append(tr_acc)\n",
    "    train_val_gaps.append(gap)\n",
    "\n",
    "    thr_acc, acc_opt = best_threshold_for_accuracy(y_val, y_val_proba, n_grid=301)\n",
    "    outer_accuracies_opt.append(acc_opt)\n",
    "\n",
    "    val_index_global = idx_train_val[val_idx]\n",
    "    train_index_global = idx_train_val[train_idx]\n",
    "\n",
    "    folds_info.append({\n",
    "        'fold': fold_idx,\n",
    "        'acc': float(acc),\n",
    "        'train_acc': float(tr_acc),\n",
    "        'gap_train_minus_val': float(gap),\n",
    "        'acc_opt': float(acc_opt),\n",
    "        'thr_acc': float(thr_acc),\n",
    "        'best_iteration': int(best_iter) if best_iter is not None else None,\n",
    "        'train_idx': train_idx,\n",
    "        'val_idx': val_idx,\n",
    "        'train_index_global': train_index_global,\n",
    "        'val_index_global': val_index_global,\n",
    "        'y_true': y_val.astype(int),\n",
    "        'y_pred': y_pred.astype(int),\n",
    "        'y_proba': y_val_proba.astype(float)\n",
    "    })\n",
    "\n",
    "    es_tag = 'with ES' if used_es else 'no ES'\n",
    "    print(f'Fold {fold_idx}: {es_tag}, train={len(y_tr)}, val={len(y_val)}, acc_val={acc*100:.2f}%, acc_val_opt={acc_opt*100:.2f}% @thr={thr_acc:.3f}, acc_train={tr_acc*100:.2f}%, gap={(gap)*100:.2f}%')\n",
    "\n",
    "print('\\n' + '='*60)\n",
    "print('Risultati Cross-Validation')\n",
    "print('='*60)\n",
    "for i, a in enumerate(outer_accuracies, 1):\n",
    "    print(f'  Fold {i}: val_acc={a*100:.2f}%, val_acc_opt={outer_accuracies_opt[i-1]*100:.2f}% @thr={folds_info[i-1][\"thr_acc\"]:.3f}, train_acc={train_accuracies[i-1]*100:.2f}%, gap={train_val_gaps[i-1]*100:.2f}%')\n",
    "print(f'\\nMean CV accuracy (0.5): {np.mean(outer_accuracies)*100:.2f}%')\n",
    "print(f'Mean CV accuracy (opt thr): {np.mean(outer_accuracies_opt)*100:.2f}%')\n",
    "print(f'Mean train accuracy: {np.mean(train_accuracies)*100:.2f}%')\n",
    "print(f'Mean gap (train - val): {np.mean(train_val_gaps)*100:.2f}%')\n",
    "print(f'Std CV accuracy:  {np.std(outer_accuracies)*100:.2f}%')\n",
    "print(f'Min/Max val acc:  {np.min(outer_accuracies)*100:.2f}% / {np.max(outer_accuracies)*100:.2f}%')\n",
    "\n",
    "WORST_FOLD_IDX = int(np.argmin(outer_accuracies))\n",
    "WORST_FOLD_NUM = int(folds_info[WORST_FOLD_IDX]['fold'])\n",
    "print(f\"\\nPeggiore fold: #{WORST_FOLD_NUM} con acc_val={outer_accuracies[WORST_FOLD_IDX]*100:.2f}% | acc_val_opt={outer_accuracies_opt[WORST_FOLD_IDX]*100:.2f}% | acc_train={train_accuracies[WORST_FOLD_IDX]*100:.2f}% | gap={train_val_gaps[WORST_FOLD_IDX]*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aef63ee",
   "metadata": {},
   "source": [
    "# Holdout validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c175e44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start holdout evaluation\n",
      "Using best_params from notebook\n",
      "Holdout size: 2000\n",
      "Accuracy: 0.8270, Balanced Accuracy: 0.8270, ROC AUC: 0.8936\n",
      "Confusion matrix:\n",
      " [[833 167]\n",
      " [179 821]]\n",
      "\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8231    0.8330    0.8280      1000\n",
      "           1     0.8310    0.8210    0.8260      1000\n",
      "\n",
      "    accuracy                         0.8270      2000\n",
      "   macro avg     0.8270    0.8270    0.8270      2000\n",
      "weighted avg     0.8270    0.8270    0.8270      2000\n",
      "\n",
      "Holdout predictions saved to holdout_predictions.csv (rows=2000)\n"
     ]
    }
   ],
   "source": [
    "# === Valutazione su holdout ===\n",
    "from sklearn.metrics import balanced_accuracy_score, roc_auc_score, confusion_matrix, classification_report\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "print('Start holdout evaluation')\n",
    "\n",
    "# Usa best_params se esiste, fallback a parametri di base\n",
    "try:\n",
    "    params = dict(best_params)\n",
    "    print('Using best_params from notebook')\n",
    "except Exception:\n",
    "    params = {}\n",
    "    print('best_params non trovato: uso parametri di default')\n",
    "\n",
    "clf = XGBClassifier(**params, use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
    "clf.fit(X_train_val, y_train_val)\n",
    "\n",
    "# predizioni e probabilità su holdout\n",
    "try:\n",
    "    proba = clf.predict_proba(X_holdout)[:, 1]\n",
    "except Exception:\n",
    "    # fallback: predict then map to 0/1 probs\n",
    "    preds_tmp = clf.predict(X_holdout)\n",
    "    proba = preds_tmp.astype(float)\n",
    "\n",
    "pred = (proba >= 0.5).astype(int)\n",
    "\n",
    "# metriche\n",
    "acc = (pred == y_holdout).mean()\n",
    "bacc = balanced_accuracy_score(y_holdout, pred)\n",
    "try:\n",
    "    roc = roc_auc_score(y_holdout, proba)\n",
    "except Exception:\n",
    "    roc = float('nan')\n",
    "\n",
    "cm = confusion_matrix(y_holdout, pred)\n",
    "cr = classification_report(y_holdout, pred, digits=4)\n",
    "\n",
    "print(f'Holdout size: {len(y_holdout)}')\n",
    "print(f'Accuracy: {acc:.4f}, Balanced Accuracy: {bacc:.4f}, ROC AUC: {roc:.4f}')\n",
    "print('Confusion matrix:\\n', cm)\n",
    "print('\\nClassification report:\\n', cr)\n",
    "\n",
    "# Salva predizioni holdout per ispezione (include battle_id se disponibili)\n",
    "try:\n",
    "    holdout_idx = idx_holdout\n",
    "    holdout_ids = train_df.loc[holdout_idx, 'battle_id'] if 'battle_id' in train_df.columns else pd.Series(holdout_idx, index=holdout_idx)\n",
    "    out_df = pd.DataFrame({'battle_id': holdout_ids.values, 'y_true': y_holdout, 'y_pred': pred, 'y_proba': proba})\n",
    "except Exception:\n",
    "    out_df = pd.DataFrame({'y_true': y_holdout, 'y_pred': pred, 'y_proba': proba})\n",
    "\n",
    "out_path = 'holdout_predictions.csv'\n",
    "out_df.to_csv(out_path, index=False)\n",
    "print(f'Holdout predictions saved to {out_path} (rows={len(out_df)})')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8873db82",
   "metadata": {},
   "source": [
    "# Make submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f3b87173",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Submission rapida post-CV ===\n",
      "✅ File di submission salvato in submission.csv\n",
      "   battle_id  player_won\n",
      "0          0           0\n",
      "1          1           1\n",
      "2          2           1\n",
      "3          3           1\n",
      "4          4           1\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Submission rapida post-CV ===\")\n",
    "cv_submission_model = XGBClassifier(**best_params, use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
    "cv_submission_model.fit(X_train_val, y_train_val)\n",
    "\n",
    "test_aligned = test_df.reindex(columns=FEATURES, fill_value=0)\n",
    "X_test_matrix = test_aligned.astype(float).to_numpy()\n",
    "test_predictions = cv_submission_model.predict(X_test_matrix).astype(int)\n",
    "\n",
    "submission_df = pd.DataFrame({\n",
    "    'battle_id': test_df['battle_id'].astype(np.int64),\n",
    "    'player_won': test_predictions.astype(np.int64)\n",
    "})\n",
    "\n",
    "submission_path = 'submission.csv'\n",
    "submission_df.to_csv(submission_path, index=False)\n",
    "print(f\"✅ File di submission salvato in {submission_path}\")\n",
    "print(submission_df.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
